2025-06-12T09:05:19.201+05:30 host=ip-131-32-57-61 service=job_executor app=data_pipeline environment=prod job_id=78487a01 source=Airflow status=PENDING message="DAG 'dag_pipeline_15' task 'cleanup_temp' is pending execution."
2025-06-12T09:05:34.201+05:30 host=ip-138-223-215-159 service=job_executor app=data_pipeline environment=prod job_id=8380593d source=Glue status=PENDING message="Glue ETL job 'job_name_78' is queued for execution."
2025-06-12T09:05:45.201+05:30 host=ip-108-185-131-5 service=job_executor app=data_pipeline environment=prod job_id=163ef33c source=Airflow status=PENDING message="DAG 'dag_pipeline_19' task 'transform_data' is pending execution."
2025-06-12T09:08:36.201+05:30 host=ip-131-205-233-83 service=job_executor app=data_pipeline environment=prod job_id=b04fba81 source=Airflow status=PENDING message="DAG 'dag_pipeline_37' task 'generate_report' is pending execution."
2025-06-12T09:09:45.201+05:30 host=ip-44-212-48-105 service=job_executor app=data_pipeline environment=prod job_id=2ec87c8e source=Glue status=PENDING message="Glue ETL job 'job_name_185' is queued for execution."
2025-06-12T09:10:27.201+05:30 host=ip-97-14-218-7 service=job_executor app=data_pipeline environment=prod job_id=0fbc2ffa source=Glue status=PENDING message="Glue ETL job 'job_name_131' is queued for execution."
2025-06-12T09:11:06.201+05:30 host=ip-35-244-188-47 service=job_executor app=data_pipeline environment=prod job_id=8e75b293 source=Airflow status=PENDING message="DAG 'dag_pipeline_44' task 'extract_data' is pending execution."
2025-06-12T09:11:14.201+05:30 host=ip-85-16-197-253 service=job_executor app=data_pipeline environment=prod job_id=3cf11aac source=Hadoop status=PENDING message="MapReduce job 'job_name_23' submitted to YARN queue."
2025-06-12T09:11:14.201+05:30 host=ip-47-75-229-115 service=job_executor app=data_pipeline environment=prod job_id=2ec87c8e source=Glue status=RUNNING message="Glue ETL job 'job_name_185' started execution."
2025-06-12T09:11:41.201+05:30 host=ip-29-120-140-51 service=job_executor app=data_pipeline environment=prod job_id=1d41010a source=Airflow status=PENDING message="DAG 'dag_pipeline_20' task 'extract_data' is pending execution."
2025-06-12T09:12:54.201+05:30 host=ip-140-225-121-197 service=job_executor app=data_pipeline environment=prod job_id=78487a01 source=Airflow status=RUNNING message="DAG 'dag_pipeline_15' task 'transform_data' is now running."
2025-06-12T09:13:34.201+05:30 host=ip-183-16-194-200 service=job_executor app=data_pipeline environment=prod job_id=0fbc2ffa source=Glue status=RUNNING message="Glue ETL job 'job_name_131' started execution."
2025-06-12T09:14:05.201+05:30 host=ip-116-211-30-161 service=job_executor app=data_pipeline environment=prod job_id=e68b6ef8 source=Airflow status=PENDING message="DAG 'dag_pipeline_31' task 'transform_data' is pending execution."
2025-06-12T09:14:52.201+05:30 host=ip-41-73-94-73 service=job_executor app=data_pipeline environment=prod job_id=b04fba81 source=Airflow status=RUNNING message="DAG 'dag_pipeline_37' task 'transform_data' is now running."
2025-06-12T09:15:11.201+05:30 host=ip-131-183-80-142 service=job_executor app=data_pipeline environment=prod job_id=16163a35 source=Spark status=PENDING message="Spark job 'job_name_4' submitted to cluster, waiting for resources."
2025-06-12T09:15:37.201+05:30 host=ip-137-40-169-248 service=job_executor app=data_pipeline environment=prod job_id=2ec87c8e source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_185' completed successfully. Output written to S3."
2025-06-12T09:16:52.201+05:30 host=ip-24-13-14-178 service=job_executor app=data_pipeline environment=prod job_id=16163a35 source=Spark status=RUNNING message="Spark job 'job_name_4' started. Processing partition 60 of 147."
2025-06-12T09:16:53.201+05:30 host=ip-62-38-14-165 service=job_executor app=data_pipeline environment=prod job_id=2f251691 source=Spark status=PENDING message="Spark job 'job_name_219' submitted to cluster, waiting for resources."
2025-06-12T09:17:29.201+05:30 host=ip-41-244-234-202 service=job_executor app=data_pipeline environment=prod job_id=8380593d source=Glue status=RUNNING message="Glue ETL job 'job_name_78' started execution."
2025-06-12T09:17:52.201+05:30 host=ip-50-98-76-233 service=job_executor app=data_pipeline environment=prod job_id=89c7b090 source=Airflow status=PENDING message="DAG 'dag_pipeline_17' task 'transform_data' is pending execution."
2025-06-12T09:18:17.201+05:30 host=ip-93-110-189-238 service=job_executor app=data_pipeline environment=prod job_id=1d41010a source=Airflow status=RUNNING message="DAG 'dag_pipeline_20' task 'extract_data' is now running."
2025-06-12T09:19:05.201+05:30 host=ip-105-14-80-125 service=job_executor app=data_pipeline environment=prod job_id=163ef33c source=Airflow status=RUNNING message="DAG 'dag_pipeline_19' task 'load_to_dw' is now running."
2025-06-12T09:19:57.201+05:30 host=ip-190-33-237-56 service=job_executor app=data_pipeline environment=prod job_id=383293e0 source=Glue status=PENDING message="Glue ETL job 'job_name_196' is queued for execution."
2025-06-12T09:21:57.201+05:30 host=ip-125-73-227-236 service=job_executor app=data_pipeline environment=prod job_id=e68b6ef8 source=Airflow status=RUNNING message="DAG 'dag_pipeline_31' task 'transform_data' is now running."
2025-06-12T09:23:08.201+05:30 host=ip-188-62-147-54 service=job_executor app=data_pipeline environment=prod job_id=3cf11aac source=Hadoop status=RUNNING message="MapReduce job 'job_name_23' running. Map phase 66% complete."
2025-06-12T09:23:11.201+05:30 host=ip-129-204-48-247 service=job_executor app=data_pipeline environment=prod job_id=2f251691 source=Spark status=RUNNING message="Spark job 'job_name_219' started. Processing partition 10 of 131."
2025-06-12T09:23:35.201+05:30 host=ip-187-175-89-124 service=job_executor app=data_pipeline environment=prod job_id=8e75b293 source=Airflow status=RUNNING message="DAG 'dag_pipeline_44' task 'extract_data' is now running."
2025-06-12T09:25:52.201+05:30 host=ip-189-54-13-6 service=job_executor app=data_pipeline environment=prod job_id=5a374285 source=Airflow status=PENDING message="DAG 'dag_pipeline_7' task 'generate_report' is pending execution."
2025-06-12T09:26:02.201+05:30 host=ip-141-203-61-96 service=job_executor app=data_pipeline environment=prod job_id=1d41010a source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_20' task 'transform_data' completed successfully."
2025-06-12T09:26:49.201+05:30 host=ip-114-252-196-222 service=job_executor app=data_pipeline environment=prod job_id=8e75b293 source=Airflow status=FAILED error_type="ZeroDivisionError" message="Job 'dag_pipeline_44' failed: ZeroDivisionError: division by zero"
ZeroDivisionError: division by zero
	at calculate.py:87
	at main.py:54
	... 2 more
Caused by: ZeroDivisionError (for JobID: 8e75b293)
2025-06-12T09:26:50.201+05:30 host=ip-110-91-138-165 service=job_executor app=data_pipeline environment=prod job_id=0fbc2ffa source=Glue status=FAILED error_type="java.lang.OutOfMemoryError" message="Job 'job_name_131' failed: java.lang.OutOfMemoryError: Java heap space"
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
	... 15 more
Caused by: java.lang.OutOfMemoryError (for JobID: 0fbc2ffa)
2025-06-12T09:28:45.201+05:30 host=ip-198-169-37-102 service=job_executor app=data_pipeline environment=prod job_id=6d57f63f source=Spark status=PENDING message="Spark job 'job_name_85' submitted to cluster, waiting for resources."
2025-06-12T09:28:54.201+05:30 host=ip-147-199-3-211 service=job_executor app=data_pipeline environment=prod job_id=3cf11aac source=Hadoop status=FAILED error_type="ValueError" message="Job 'job_name_23' failed: ValueError: invalid literal"
ValueError: invalid literal for int() with base 10: 'abc'
	at pandas_read.py:127
	at data_processing.py:42
	... 3 more
Caused by: ValueError (for JobID: 3cf11aac)
2025-06-12T09:29:10.201+05:30 host=ip-157-129-104-180 service=job_executor app=data_pipeline environment=prod job_id=5a374285 source=Airflow status=RUNNING message="DAG 'dag_pipeline_7' task 'transform_data' is now running."
2025-06-12T09:29:21.201+05:30 host=ip-136-13-61-202 service=job_executor app=data_pipeline environment=prod job_id=96a0b378 source=Spark status=PENDING message="Spark job 'job_name_101' submitted to cluster, waiting for resources."
2025-06-12T09:30:05.201+05:30 host=ip-62-101-187-71 service=job_executor app=data_pipeline environment=prod job_id=383293e0 source=Glue status=RUNNING message="Glue ETL job 'job_name_196' started execution."
2025-06-12T09:31:25.201+05:30 host=ip-101-2-26-110 service=job_executor app=data_pipeline environment=prod job_id=6c2ad901 source=Hadoop status=PENDING message="MapReduce job 'job_name_90' submitted to YARN queue."
2025-06-12T09:31:35.201+05:30 host=ip-177-212-237-53 service=job_executor app=data_pipeline environment=prod job_id=8c08db48 source=Glue status=PENDING message="Glue ETL job 'job_name_213' is queued for execution."
2025-06-12T09:31:58.201+05:30 host=ip-16-60-129-47 service=job_executor app=data_pipeline environment=prod job_id=fde601cd source=Spark status=PENDING message="Spark job 'job_name_253' submitted to cluster, waiting for resources."
2025-06-12T09:32:10.201+05:30 host=ip-130-47-251-223 service=job_executor app=data_pipeline environment=prod job_id=89c7b090 source=Airflow status=RUNNING message="DAG 'dag_pipeline_17' task 'extract_data' is now running."
2025-06-12T09:32:29.201+05:30 host=ip-116-182-237-230 service=job_executor app=data_pipeline environment=prod job_id=89c7b090 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_17' task 'load_to_dw' completed successfully."
2025-06-12T09:33:07.201+05:30 host=ip-126-249-181-168 service=job_executor app=data_pipeline environment=prod job_id=78487a01 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_15' task 'cleanup_temp' completed successfully."
2025-06-12T09:33:17.201+05:30 host=ip-73-63-116-25 service=job_executor app=data_pipeline environment=prod job_id=f37b0aac source=Glue status=PENDING message="Glue ETL job 'job_name_15' is queued for execution."
2025-06-12T09:33:29.201+05:30 host=ip-13-61-68-132 service=job_executor app=data_pipeline environment=prod job_id=f2d54da2 source=Glue status=PENDING message="Glue ETL job 'job_name_176' is queued for execution."
2025-06-12T09:33:36.201+05:30 host=ip-125-183-198-158 service=job_executor app=data_pipeline environment=prod job_id=147396f8 source=Spark status=PENDING message="Spark job 'job_name_279' submitted to cluster, waiting for resources."
2025-06-12T09:33:49.201+05:30 host=ip-183-83-171-89 service=job_executor app=data_pipeline environment=prod job_id=2064f74c source=Airflow status=PENDING message="DAG 'dag_pipeline_24' task 'generate_report' is pending execution."
2025-06-12T09:33:58.201+05:30 host=ip-50-60-248-34 service=job_executor app=data_pipeline environment=prod job_id=6abb1090 source=Spark status=PENDING message="Spark job 'job_name_215' submitted to cluster, waiting for resources."
2025-06-12T09:34:03.201+05:30 host=ip-161-136-151-86 service=job_executor app=data_pipeline environment=prod job_id=20f2fdd5 source=Hadoop status=PENDING message="MapReduce job 'job_name_106' submitted to YARN queue."
2025-06-12T09:35:18.201+05:30 host=ip-148-18-63-148 service=job_executor app=data_pipeline environment=prod job_id=16163a35 source=Spark status=FAILED error_type="TypeError" message="Job 'job_name_4' failed: TypeError: cannot concatenate"
TypeError: cannot concatenate 'str' and 'int' objects
	at transform_data.py:56
	at main_workflow.py:112
	... 4 more
Caused by: TypeError (for JobID: 16163a35)
2025-06-12T09:35:51.201+05:30 host=ip-59-9-174-112 service=job_executor app=data_pipeline environment=prod job_id=20f2fdd5 source=Hadoop status=RUNNING message="MapReduce job 'job_name_106' running. Map phase 76% complete."
2025-06-12T09:36:19.201+05:30 host=ip-123-163-37-174 service=job_executor app=data_pipeline environment=prod job_id=96a0b378 source=Spark status=RUNNING message="Spark job 'job_name_101' started. Processing partition 16 of 139."
2025-06-12T09:36:31.201+05:30 host=ip-94-164-96-162 service=job_executor app=data_pipeline environment=prod job_id=fde601cd source=Spark status=RUNNING message="Spark job 'job_name_253' started. Processing partition 37 of 116."
2025-06-12T09:37:20.201+05:30 host=ip-59-29-62-5 service=job_executor app=data_pipeline environment=prod job_id=6d57f63f source=Spark status=RUNNING message="Spark job 'job_name_85' started. Processing partition 95 of 100."
2025-06-12T09:37:23.201+05:30 host=ip-17-47-113-164 service=job_executor app=data_pipeline environment=prod job_id=fde601cd source=Spark status=SUCCEEDED message="Spark job 'job_name_253' completed successfully. Results saved."
2025-06-12T09:37:54.201+05:30 host=ip-153-91-17-214 service=job_executor app=data_pipeline environment=prod job_id=5a374285 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_7' task 'generate_report' completed successfully."
2025-06-12T09:38:13.201+05:30 host=ip-37-232-244-246 service=job_executor app=data_pipeline environment=prod job_id=c5caa8da source=Hadoop status=PENDING message="MapReduce job 'job_name_143' submitted to YARN queue."
2025-06-12T09:38:15.201+05:30 host=ip-33-141-121-140 service=job_executor app=data_pipeline environment=prod job_id=75fae8e1 source=Glue status=PENDING message="Glue ETL job 'job_name_199' is queued for execution."
2025-06-12T09:38:26.201+05:30 host=ip-194-99-182-3 service=job_executor app=data_pipeline environment=prod job_id=2064f74c source=Airflow status=RUNNING message="DAG 'dag_pipeline_24' task 'transform_data' is now running."
2025-06-12T09:38:52.201+05:30 host=ip-170-22-216-38 service=job_executor app=data_pipeline environment=prod job_id=f5d28766 source=Spark status=PENDING message="Spark job 'job_name_104' submitted to cluster, waiting for resources."
2025-06-12T09:38:56.201+05:30 host=ip-125-49-27-135 service=job_executor app=data_pipeline environment=prod job_id=8380593d source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_78' completed successfully. Output written to S3."
2025-06-12T09:39:35.201+05:30 host=ip-135-77-171-181 service=job_executor app=data_pipeline environment=prod job_id=f37b0aac source=Glue status=RUNNING message="Glue ETL job 'job_name_15' started execution."
2025-06-12T09:39:51.201+05:30 host=ip-164-70-93-169 service=job_executor app=data_pipeline environment=prod job_id=baece963 source=Spark status=PENDING message="Spark job 'job_name_34' submitted to cluster, waiting for resources."
2025-06-12T09:40:31.201+05:30 host=ip-100-57-128-230 service=job_executor app=data_pipeline environment=prod job_id=96924d62 source=Spark status=PENDING message="Spark job 'job_name_109' submitted to cluster, waiting for resources."
2025-06-12T09:40:48.201+05:30 host=ip-44-100-15-180 service=job_executor app=data_pipeline environment=prod job_id=ff897482 source=Spark status=PENDING message="Spark job 'job_name_189' submitted to cluster, waiting for resources."
2025-06-12T09:41:16.201+05:30 host=ip-97-117-121-109 service=job_executor app=data_pipeline environment=prod job_id=b04fba81 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_37' task 'load_to_dw' completed successfully."
2025-06-12T09:41:20.201+05:30 host=ip-45-148-224-226 service=job_executor app=data_pipeline environment=prod job_id=089f2061 source=Glue status=PENDING message="Glue ETL job 'job_name_273' is queued for execution."
2025-06-12T09:41:41.201+05:30 host=ip-79-24-115-211 service=job_executor app=data_pipeline environment=prod job_id=3fbb3ab4 source=Airflow status=PENDING message="DAG 'dag_pipeline_3' task 'load_to_dw' is pending execution."
2025-06-12T09:42:02.201+05:30 host=ip-155-208-82-230 service=job_executor app=data_pipeline environment=prod job_id=8c08db48 source=Glue status=RUNNING message="Glue ETL job 'job_name_213' started execution."
2025-06-12T09:42:06.201+05:30 host=ip-106-239-223-104 service=job_executor app=data_pipeline environment=prod job_id=e685aac2 source=Spark status=PENDING message="Spark job 'job_name_232' submitted to cluster, waiting for resources."
2025-06-12T09:42:10.201+05:30 host=ip-125-157-205-110 service=job_executor app=data_pipeline environment=prod job_id=f5d28766 source=Spark status=RUNNING message="Spark job 'job_name_104' started. Processing partition 22 of 149."
2025-06-12T09:42:14.201+05:30 host=ip-103-218-165-25 service=job_executor app=data_pipeline environment=prod job_id=6abb1090 source=Spark status=RUNNING message="Spark job 'job_name_215' started. Processing partition 83 of 197."
2025-06-12T09:42:29.201+05:30 host=ip-132-188-221-247 service=job_executor app=data_pipeline environment=prod job_id=a14176d7 source=Airflow status=PENDING message="DAG 'dag_pipeline_29' task 'generate_report' is pending execution."
2025-06-12T09:42:43.201+05:30 host=ip-188-229-34-145 service=job_executor app=data_pipeline environment=prod job_id=b16f086d source=Hadoop status=PENDING message="MapReduce job 'job_name_2' submitted to YARN queue."
2025-06-12T09:43:09.201+05:30 host=ip-120-232-139-145 service=job_executor app=data_pipeline environment=prod job_id=1c426202 source=Airflow status=PENDING message="DAG 'dag_pipeline_13' task 'extract_data' is pending execution."
2025-06-12T09:43:13.201+05:30 host=ip-173-97-214-108 service=job_executor app=data_pipeline environment=prod job_id=20f2fdd5 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_106' completed successfully. Output written to HDFS."
2025-06-12T09:43:21.201+05:30 host=ip-104-20-21-158 service=job_executor app=data_pipeline environment=prod job_id=5ad8282b source=Airflow status=PENDING message="DAG 'dag_pipeline_6' task 'transform_data' is pending execution."
2025-06-12T09:43:48.201+05:30 host=ip-111-105-178-196 service=job_executor app=data_pipeline environment=prod job_id=baece963 source=Spark status=RUNNING message="Spark job 'job_name_34' started. Processing partition 58 of 193."
2025-06-12T09:44:07.201+05:30 host=ip-116-91-158-226 service=job_executor app=data_pipeline environment=prod job_id=6c2ad901 source=Hadoop status=RUNNING message="MapReduce job 'job_name_90' running. Map phase 57% complete."
2025-06-12T09:45:48.201+05:30 host=ip-80-226-27-112 service=job_executor app=data_pipeline environment=prod job_id=e685aac2 source=Spark status=RUNNING message="Spark job 'job_name_232' started. Processing partition 95 of 198."
2025-06-12T09:45:49.201+05:30 host=ip-103-148-207-58 service=job_executor app=data_pipeline environment=prod job_id=f2d54da2 source=Glue status=RUNNING message="Glue ETL job 'job_name_176' started execution."
2025-06-12T09:45:51.201+05:30 host=ip-189-72-141-226 service=job_executor app=data_pipeline environment=prod job_id=a14176d7 source=Airflow status=RUNNING message="DAG 'dag_pipeline_29' task 'transform_data' is now running."
2025-06-12T09:45:55.201+05:30 host=ip-32-2-247-28 service=job_executor app=data_pipeline environment=prod job_id=d77f25d9 source=Airflow status=PENDING message="DAG 'dag_pipeline_3' task 'transform_data' is pending execution."
2025-06-12T09:46:27.201+05:30 host=ip-48-105-101-244 service=job_executor app=data_pipeline environment=prod job_id=a4db38c3 source=Hadoop status=PENDING message="MapReduce job 'job_name_264' submitted to YARN queue."
2025-06-12T09:46:28.201+05:30 host=ip-15-208-39-228 service=job_executor app=data_pipeline environment=prod job_id=e68b6ef8 source=Airflow status=FAILED error_type="TypeError" message="Job 'dag_pipeline_31' failed: TypeError: cannot concatenate"
TypeError: cannot concatenate 'str' and 'int' objects
	at transform_data.py:56
	at main_workflow.py:112
	... 4 more
Caused by: TypeError (for JobID: e68b6ef8)
2025-06-12T09:47:07.201+05:30 host=ip-177-176-104-85 service=job_executor app=data_pipeline environment=prod job_id=1c426202 source=Airflow status=RUNNING message="DAG 'dag_pipeline_13' task 'generate_report' is now running."
2025-06-12T09:47:16.201+05:30 host=ip-31-46-38-168 service=job_executor app=data_pipeline environment=prod job_id=ff897482 source=Spark status=RUNNING message="Spark job 'job_name_189' started. Processing partition 55 of 111."
2025-06-12T09:47:41.201+05:30 host=ip-192-233-203-134 service=job_executor app=data_pipeline environment=prod job_id=147396f8 source=Spark status=RUNNING message="Spark job 'job_name_279' started. Processing partition 66 of 145."
2025-06-12T09:47:50.201+05:30 host=ip-165-32-152-25 service=job_executor app=data_pipeline environment=prod job_id=ae379b2f source=Glue status=PENDING message="Glue ETL job 'job_name_142' is queued for execution."
2025-06-12T09:47:53.201+05:30 host=ip-115-142-95-185 service=job_executor app=data_pipeline environment=prod job_id=27446a20 source=Hadoop status=PENDING message="MapReduce job 'job_name_126' submitted to YARN queue."
2025-06-12T09:47:58.201+05:30 host=ip-140-74-180-181 service=job_executor app=data_pipeline environment=prod job_id=ca33381a source=Glue status=PENDING message="Glue ETL job 'job_name_19' is queued for execution."
2025-06-12T09:48:14.201+05:30 host=ip-83-147-174-247 service=job_executor app=data_pipeline environment=prod job_id=eb628a8e source=Glue status=PENDING message="Glue ETL job 'job_name_41' is queued for execution."
2025-06-12T09:48:28.201+05:30 host=ip-170-181-160-104 service=job_executor app=data_pipeline environment=prod job_id=1ebc2f94 source=Hadoop status=PENDING message="MapReduce job 'job_name_231' submitted to YARN queue."
2025-06-12T09:48:38.201+05:30 host=ip-70-241-75-199 service=job_executor app=data_pipeline environment=prod job_id=a4db38c3 source=Hadoop status=RUNNING message="MapReduce job 'job_name_264' running. Map phase 67% complete."
2025-06-12T09:48:39.201+05:30 host=ip-27-192-63-182 service=job_executor app=data_pipeline environment=prod job_id=163ef33c source=Airflow status=FAILED error_type="java.lang.OutOfMemoryError" message="Job 'dag_pipeline_19' failed: java.lang.OutOfMemoryError: Java heap space"
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
	... 15 more
Caused by: java.lang.OutOfMemoryError (for JobID: 163ef33c)
2025-06-12T09:48:43.201+05:30 host=ip-56-176-66-93 service=job_executor app=data_pipeline environment=prod job_id=2f251691 source=Spark status=FAILED error_type="org.apache.spark.SparkException" message="Job 'job_name_219' failed: org.apache.spark.SparkException"
org.apache.spark.SparkException: Job aborted due to stage failure
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.Task.run(Task.scala:120)
	at org.apache.spark.executor.Executor.$anonfun$runJob$4(Executor.scala:180)
	... 5 more
Caused by: org.apache.spark.SparkException (for JobID: 2f251691)
2025-06-12T09:48:56.201+05:30 host=ip-82-103-195-48 service=job_executor app=data_pipeline environment=prod job_id=a14176d7 source=Airflow status=FAILED error_type="org.apache.hadoop.fs.FileNotFoundException" message="Job 'dag_pipeline_29' failed: org.apache.hadoop.fs.FileNotFoundException"
org.apache.hadoop.fs.FileNotFoundException: File /user/hadoop/input.txt does not exist
	at org.apache.hadoop.fs.LocalFileSystem.open(LocalFileSystem.java:123)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:67)
	... 8 more
Caused by: org.apache.hadoop.fs.FileNotFoundException (for JobID: a14176d7)
2025-06-12T09:49:47.201+05:30 host=ip-37-249-205-164 service=job_executor app=data_pipeline environment=prod job_id=aec1e288 source=Glue status=PENDING message="Glue ETL job 'job_name_268' is queued for execution."
2025-06-12T09:49:49.201+05:30 host=ip-58-197-70-199 service=job_executor app=data_pipeline environment=prod job_id=c5caa8da source=Hadoop status=RUNNING message="MapReduce job 'job_name_143' running. Map phase 24% complete."
2025-06-12T09:50:12.201+05:30 host=ip-134-57-20-147 service=job_executor app=data_pipeline environment=prod job_id=b16f086d source=Hadoop status=RUNNING message="MapReduce job 'job_name_2' running. Map phase 10% complete."
2025-06-12T09:50:12.201+05:30 host=ip-158-142-17-32 service=job_executor app=data_pipeline environment=prod job_id=75fae8e1 source=Glue status=RUNNING message="Glue ETL job 'job_name_199' started execution."
2025-06-12T09:50:13.201+05:30 host=ip-126-81-39-5 service=job_executor app=data_pipeline environment=prod job_id=2adf3c83 source=Hadoop status=PENDING message="MapReduce job 'job_name_116' submitted to YARN queue."
2025-06-12T09:50:47.201+05:30 host=ip-19-24-223-17 service=job_executor app=data_pipeline environment=prod job_id=383293e0 source=Glue status=FAILED error_type="org.apache.hadoop.fs.FileNotFoundException" message="Job 'job_name_196' failed: org.apache.hadoop.fs.FileNotFoundException"
org.apache.hadoop.fs.FileNotFoundException: File /user/hadoop/input.txt does not exist
	at org.apache.hadoop.fs.LocalFileSystem.open(LocalFileSystem.java:123)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:67)
	... 8 more
Caused by: org.apache.hadoop.fs.FileNotFoundException (for JobID: 383293e0)
2025-06-12T09:50:57.201+05:30 host=ip-63-151-191-146 service=job_executor app=data_pipeline environment=prod job_id=0a458474 source=Glue status=PENDING message="Glue ETL job 'job_name_128' is queued for execution."
2025-06-12T09:51:04.201+05:30 host=ip-122-182-99-6 service=job_executor app=data_pipeline environment=prod job_id=089f2061 source=Glue status=RUNNING message="Glue ETL job 'job_name_273' started execution."
2025-06-12T09:51:41.201+05:30 host=ip-22-93-77-219 service=job_executor app=data_pipeline environment=prod job_id=75fae8e1 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_199' completed successfully. Output written to S3."
2025-06-12T09:52:21.201+05:30 host=ip-158-247-132-209 service=job_executor app=data_pipeline environment=prod job_id=b16f086d source=Hadoop status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_2' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: b16f086d)
2025-06-12T09:52:32.201+05:30 host=ip-122-246-150-134 service=job_executor app=data_pipeline environment=prod job_id=2adf3c83 source=Hadoop status=RUNNING message="MapReduce job 'job_name_116' running. Map phase 14% complete."
2025-06-12T09:52:48.201+05:30 host=ip-135-63-112-51 service=job_executor app=data_pipeline environment=prod job_id=6b7ae658 source=Hadoop status=PENDING message="MapReduce job 'job_name_284' submitted to YARN queue."
2025-06-12T09:52:58.201+05:30 host=ip-195-192-153-203 service=job_executor app=data_pipeline environment=prod job_id=a4db38c3 source=Hadoop status=FAILED error_type="TypeError" message="Job 'job_name_264' failed: TypeError: cannot concatenate"
TypeError: cannot concatenate 'str' and 'int' objects
	at transform_data.py:56
	at main_workflow.py:112
	... 4 more
Caused by: TypeError (for JobID: a4db38c3)
2025-06-12T09:53:01.201+05:30 host=ip-167-162-173-14 service=job_executor app=data_pipeline environment=prod job_id=3fbb3ab4 source=Airflow status=RUNNING message="DAG 'dag_pipeline_3' task 'generate_report' is now running."
2025-06-12T09:53:15.201+05:30 host=ip-96-167-56-67 service=job_executor app=data_pipeline environment=prod job_id=30d83d0d source=Glue status=PENDING message="Glue ETL job 'job_name_108' is queued for execution."
2025-06-12T09:53:21.201+05:30 host=ip-112-117-223-116 service=job_executor app=data_pipeline environment=prod job_id=96924d62 source=Spark status=RUNNING message="Spark job 'job_name_109' started. Processing partition 23 of 146."
2025-06-12T09:53:50.201+05:30 host=ip-169-93-93-43 service=job_executor app=data_pipeline environment=prod job_id=f37b0aac source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_15' completed successfully. Output written to S3."
2025-06-12T09:53:56.201+05:30 host=ip-118-249-31-23 service=job_executor app=data_pipeline environment=prod job_id=96a0b378 source=Spark status=FAILED error_type="FileNotFoundError" message="Job 'job_name_101' failed: FileNotFoundError: [Errno 2] No such file or directory"
FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'
	at open_config.py:45
	at initialize.py:12
	... 3 more
Caused by: FileNotFoundError (for JobID: 96a0b378)
2025-06-12T09:54:33.201+05:30 host=ip-59-8-72-132 service=job_executor app=data_pipeline environment=prod job_id=6d57f63f source=Spark status=SUCCEEDED message="Spark job 'job_name_85' completed successfully. Results saved."
2025-06-12T09:54:34.201+05:30 host=ip-49-53-113-226 service=job_executor app=data_pipeline environment=prod job_id=0a458474 source=Glue status=RUNNING message="Glue ETL job 'job_name_128' started execution."
2025-06-12T09:55:08.201+05:30 host=ip-70-122-9-215 service=job_executor app=data_pipeline environment=prod job_id=1ebc2f94 source=Hadoop status=RUNNING message="MapReduce job 'job_name_231' running. Map phase 81% complete."
2025-06-12T09:55:29.201+05:30 host=ip-126-46-97-186 service=job_executor app=data_pipeline environment=prod job_id=30d83d0d source=Glue status=RUNNING message="Glue ETL job 'job_name_108' started execution."
2025-06-12T09:55:51.201+05:30 host=ip-103-176-137-197 service=job_executor app=data_pipeline environment=prod job_id=baece963 source=Spark status=FAILED error_type="com.microsoft.sqlserver.jdbc.SQLServerException" message="Job 'job_name_34' failed: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed"
com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user 'etl_user'
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:500)
	at com.yourcompany.db.DBConnector.getConnection(DBConnector.java:80)
	... 9 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException (for JobID: baece963)
2025-06-12T09:56:26.201+05:30 host=ip-39-26-188-102 service=job_executor app=data_pipeline environment=prod job_id=5ad8282b source=Airflow status=RUNNING message="DAG 'dag_pipeline_6' task 'cleanup_temp' is now running."
2025-06-12T09:56:47.201+05:30 host=ip-38-87-140-70 service=job_executor app=data_pipeline environment=prod job_id=d77f25d9 source=Airflow status=RUNNING message="DAG 'dag_pipeline_3' task 'transform_data' is now running."
2025-06-12T09:57:20.201+05:30 host=ip-86-132-116-48 service=job_executor app=data_pipeline environment=prod job_id=ae379b2f source=Glue status=RUNNING message="Glue ETL job 'job_name_142' started execution."
2025-06-12T09:57:21.201+05:30 host=ip-120-220-113-234 service=job_executor app=data_pipeline environment=prod job_id=ad7749cc source=Spark status=PENDING message="Spark job 'job_name_75' submitted to cluster, waiting for resources."
2025-06-12T09:57:36.201+05:30 host=ip-38-210-134-211 service=job_executor app=data_pipeline environment=prod job_id=ca33381a source=Glue status=RUNNING message="Glue ETL job 'job_name_19' started execution."
2025-06-12T09:57:50.201+05:30 host=ip-26-181-52-136 service=job_executor app=data_pipeline environment=prod job_id=aec1e288 source=Glue status=RUNNING message="Glue ETL job 'job_name_268' started execution."
2025-06-12T09:58:04.201+05:30 host=ip-27-116-27-16 service=job_executor app=data_pipeline environment=prod job_id=6c2ad901 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_90' completed successfully. Output written to HDFS."
2025-06-12T09:58:14.201+05:30 host=ip-30-23-127-170 service=job_executor app=data_pipeline environment=prod job_id=2adf3c83 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_116' completed successfully. Output written to HDFS."
2025-06-12T09:58:44.201+05:30 host=ip-162-33-222-202 service=job_executor app=data_pipeline environment=prod job_id=21c40e16 source=Glue status=PENDING message="Glue ETL job 'job_name_98' is queued for execution."
2025-06-12T09:59:02.201+05:30 host=ip-175-52-135-163 service=job_executor app=data_pipeline environment=prod job_id=2064f74c source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_24' task 'transform_data' completed successfully."
2025-06-12T10:00:07.201+05:30 host=ip-15-68-10-139 service=job_executor app=data_pipeline environment=prod job_id=c34a04de source=Hadoop status=PENDING message="MapReduce job 'job_name_5' submitted to YARN queue."
2025-06-12T10:00:12.201+05:30 host=ip-38-91-223-133 service=job_executor app=data_pipeline environment=prod job_id=c5caa8da source=Hadoop status=FAILED error_type="org.apache.spark.SparkException" message="Job 'job_name_143' failed: org.apache.spark.SparkException"
org.apache.spark.SparkException: Job aborted due to stage failure
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.Task.run(Task.scala:120)
	at org.apache.spark.executor.Executor.$anonfun$runJob$4(Executor.scala:180)
	... 5 more
Caused by: org.apache.spark.SparkException (for JobID: c5caa8da)
2025-06-12T10:01:50.201+05:30 host=ip-26-108-186-37 service=job_executor app=data_pipeline environment=prod job_id=6b7ae658 source=Hadoop status=RUNNING message="MapReduce job 'job_name_284' running. Map phase 13% complete."
2025-06-12T10:01:53.201+05:30 host=ip-86-59-26-204 service=job_executor app=data_pipeline environment=prod job_id=01cbfc5b source=Spark status=PENDING message="Spark job 'job_name_250' submitted to cluster, waiting for resources."
2025-06-12T10:02:15.201+05:30 host=ip-44-47-160-22 service=job_executor app=data_pipeline environment=prod job_id=e0a400d5 source=Airflow status=PENDING message="DAG 'dag_pipeline_18' task 'transform_data' is pending execution."
2025-06-12T10:02:39.201+05:30 host=ip-126-226-185-123 service=job_executor app=data_pipeline environment=prod job_id=27446a20 source=Hadoop status=RUNNING message="MapReduce job 'job_name_126' running. Map phase 89% complete."
2025-06-12T10:02:39.201+05:30 host=ip-195-90-133-68 service=job_executor app=data_pipeline environment=prod job_id=147396f8 source=Spark status=SUCCEEDED message="Spark job 'job_name_279' completed successfully. Results saved."
2025-06-12T10:02:41.201+05:30 host=ip-183-254-83-245 service=job_executor app=data_pipeline environment=prod job_id=eb628a8e source=Glue status=RUNNING message="Glue ETL job 'job_name_41' started execution."
2025-06-12T10:02:47.201+05:30 host=ip-52-20-124-185 service=job_executor app=data_pipeline environment=prod job_id=58edfb31 source=Spark status=PENDING message="Spark job 'job_name_84' submitted to cluster, waiting for resources."
2025-06-12T10:03:12.201+05:30 host=ip-88-65-17-152 service=job_executor app=data_pipeline environment=prod job_id=d3add30e source=Hadoop status=PENDING message="MapReduce job 'job_name_53' submitted to YARN queue."
2025-06-12T10:03:39.201+05:30 host=ip-181-194-125-49 service=job_executor app=data_pipeline environment=prod job_id=f5d28766 source=Spark status=SUCCEEDED message="Spark job 'job_name_104' completed successfully. Results saved."
2025-06-12T10:03:42.201+05:30 host=ip-15-113-5-72 service=job_executor app=data_pipeline environment=prod job_id=41dafa72 source=Hadoop status=PENDING message="MapReduce job 'job_name_171' submitted to YARN queue."
2025-06-12T10:04:01.201+05:30 host=ip-147-165-99-213 service=job_executor app=data_pipeline environment=prod job_id=f2d54da2 source=Glue status=FAILED error_type="ValueError" message="Job 'job_name_176' failed: ValueError: invalid literal"
ValueError: invalid literal for int() with base 10: 'abc'
	at pandas_read.py:127
	at data_processing.py:42
	... 3 more
Caused by: ValueError (for JobID: f2d54da2)
2025-06-12T10:04:38.201+05:30 host=ip-130-200-177-110 service=job_executor app=data_pipeline environment=prod job_id=21c40e16 source=Glue status=RUNNING message="Glue ETL job 'job_name_98' started execution."
2025-06-12T10:04:56.201+05:30 host=ip-118-191-84-107 service=job_executor app=data_pipeline environment=prod job_id=5ad8282b source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_6' task 'transform_data' completed successfully."
2025-06-12T10:05:11.201+05:30 host=ip-178-220-70-174 service=job_executor app=data_pipeline environment=prod job_id=e0b74e90 source=Glue status=PENDING message="Glue ETL job 'job_name_40' is queued for execution."
2025-06-12T10:05:12.201+05:30 host=ip-185-167-35-114 service=job_executor app=data_pipeline environment=prod job_id=d3add30e source=Hadoop status=RUNNING message="MapReduce job 'job_name_53' running. Map phase 38% complete."
2025-06-12T10:05:51.201+05:30 host=ip-161-182-54-210 service=job_executor app=data_pipeline environment=prod job_id=f71b393e source=Glue status=PENDING message="Glue ETL job 'job_name_9' is queued for execution."
2025-06-12T10:05:53.201+05:30 host=ip-116-115-245-4 service=job_executor app=data_pipeline environment=prod job_id=96924d62 source=Spark status=SUCCEEDED message="Spark job 'job_name_109' completed successfully. Results saved."
2025-06-12T10:06:02.201+05:30 host=ip-96-34-23-121 service=job_executor app=data_pipeline environment=prod job_id=ff897482 source=Spark status=FAILED error_type="org.apache.hadoop.mapreduce.lib.input.FileInputFormat" message="Job 'job_name_189' failed: org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist"
org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist: file:/data/missing.csv
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:275)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJob(JobSubmitter.java:145)
	... 10 more
Caused by: org.apache.hadoop.mapreduce.lib.input.FileInputFormat (for JobID: ff897482)
2025-06-12T10:06:15.201+05:30 host=ip-142-179-87-184 service=job_executor app=data_pipeline environment=prod job_id=58edfb31 source=Spark status=RUNNING message="Spark job 'job_name_84' started. Processing partition 98 of 120."
2025-06-12T10:06:15.201+05:30 host=ip-33-149-210-74 service=job_executor app=data_pipeline environment=prod job_id=00762ffb source=Airflow status=PENDING message="DAG 'dag_pipeline_47' task 'cleanup_temp' is pending execution."
2025-06-12T10:06:37.201+05:30 host=ip-77-81-150-12 service=job_executor app=data_pipeline environment=prod job_id=41dafa72 source=Hadoop status=RUNNING message="MapReduce job 'job_name_171' running. Map phase 75% complete."
2025-06-12T10:06:45.201+05:30 host=ip-92-138-190-169 service=job_executor app=data_pipeline environment=prod job_id=e0a400d5 source=Airflow status=RUNNING message="DAG 'dag_pipeline_18' task 'transform_data' is now running."
2025-06-12T10:07:42.201+05:30 host=ip-91-145-163-28 service=job_executor app=data_pipeline environment=prod job_id=6abb1090 source=Spark status=FAILED error_type="FileNotFoundError" message="Job 'job_name_215' failed: FileNotFoundError: [Errno 2] No such file or directory"
FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'
	at open_config.py:45
	at initialize.py:12
	... 3 more
Caused by: FileNotFoundError (for JobID: 6abb1090)
2025-06-12T10:08:07.201+05:30 host=ip-11-131-247-175 service=job_executor app=data_pipeline environment=prod job_id=5dd14a93 source=Spark status=PENDING message="Spark job 'job_name_224' submitted to cluster, waiting for resources."
2025-06-12T10:08:26.201+05:30 host=ip-108-163-46-51 service=job_executor app=data_pipeline environment=prod job_id=c3959950 source=Airflow status=PENDING message="DAG 'dag_pipeline_23' task 'cleanup_temp' is pending execution."
2025-06-12T10:08:32.201+05:30 host=ip-65-191-22-115 service=job_executor app=data_pipeline environment=prod job_id=8c08db48 source=Glue status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_213' failed: com.amazonaws.services.glue.GlueException: Entity not found"
com.amazonaws.services.glue.GlueException: Entity not found: database 'sales_db'
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:350)
	at com.amazonaws.services.glue.AWSGlueClient.getTable(AWSGlueClient.java:115)
	... 4 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 8c08db48)
2025-06-12T10:09:01.201+05:30 host=ip-78-24-117-24 service=job_executor app=data_pipeline environment=prod job_id=089f2061 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_273' completed successfully. Output written to S3."
2025-06-12T10:09:07.201+05:30 host=ip-42-89-153-87 service=job_executor app=data_pipeline environment=prod job_id=d3add30e source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_53' completed successfully. Output written to HDFS."
2025-06-12T10:09:23.201+05:30 host=ip-182-57-45-3 service=job_executor app=data_pipeline environment=prod job_id=1c426202 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_13' task 'cleanup_temp' completed successfully."
2025-06-12T10:09:33.201+05:30 host=ip-98-169-90-91 service=job_executor app=data_pipeline environment=prod job_id=ad7749cc source=Spark status=RUNNING message="Spark job 'job_name_75' started. Processing partition 38 of 181."
2025-06-12T10:09:33.201+05:30 host=ip-143-252-142-176 service=job_executor app=data_pipeline environment=prod job_id=41dafa72 source=Hadoop status=FAILED error_type="ZeroDivisionError" message="Job 'job_name_171' failed: ZeroDivisionError: division by zero"
ZeroDivisionError: division by zero
	at calculate.py:87
	at main.py:54
	... 2 more
Caused by: ZeroDivisionError (for JobID: 41dafa72)
2025-06-12T10:09:46.201+05:30 host=ip-15-155-247-244 service=job_executor app=data_pipeline environment=prod job_id=71a0d6c9 source=Airflow status=PENDING message="DAG 'dag_pipeline_34' task 'cleanup_temp' is pending execution."
2025-06-12T10:10:07.201+05:30 host=ip-149-132-237-137 service=job_executor app=data_pipeline environment=prod job_id=eb628a8e source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_41' completed successfully. Output written to S3."
2025-06-12T10:10:57.201+05:30 host=ip-53-67-107-185 service=job_executor app=data_pipeline environment=prod job_id=e0a400d5 source=Airflow status=FAILED error_type="TypeError" message="Job 'dag_pipeline_18' failed: TypeError: cannot concatenate"
TypeError: cannot concatenate 'str' and 'int' objects
	at transform_data.py:56
	at main_workflow.py:112
	... 4 more
Caused by: TypeError (for JobID: e0a400d5)
2025-06-12T10:11:22.201+05:30 host=ip-57-56-71-55 service=job_executor app=data_pipeline environment=prod job_id=52d935db source=Glue status=PENDING message="Glue ETL job 'job_name_216' is queued for execution."
2025-06-12T10:11:49.201+05:30 host=ip-63-11-11-118 service=job_executor app=data_pipeline environment=prod job_id=5dd14a93 source=Spark status=RUNNING message="Spark job 'job_name_224' started. Processing partition 37 of 117."
2025-06-12T10:11:54.201+05:30 host=ip-173-140-86-17 service=job_executor app=data_pipeline environment=prod job_id=4ed1ad89 source=Glue status=PENDING message="Glue ETL job 'job_name_226' is queued for execution."
2025-06-12T10:13:47.201+05:30 host=ip-50-13-28-2 service=job_executor app=data_pipeline environment=prod job_id=00762ffb source=Airflow status=RUNNING message="DAG 'dag_pipeline_47' task 'cleanup_temp' is now running."
2025-06-12T10:14:09.201+05:30 host=ip-153-183-60-166 service=job_executor app=data_pipeline environment=prod job_id=6b7ae658 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_284' completed successfully. Output written to HDFS."
2025-06-12T10:14:10.201+05:30 host=ip-79-152-255-206 service=job_executor app=data_pipeline environment=prod job_id=fa780461 source=Airflow status=PENDING message="DAG 'dag_pipeline_29' task 'cleanup_temp' is pending execution."
2025-06-12T10:14:25.201+05:30 host=ip-52-232-109-23 service=job_executor app=data_pipeline environment=prod job_id=c34a04de source=Hadoop status=RUNNING message="MapReduce job 'job_name_5' running. Map phase 42% complete."
2025-06-12T10:14:35.201+05:30 host=ip-70-64-216-125 service=job_executor app=data_pipeline environment=prod job_id=01cbfc5b source=Spark status=RUNNING message="Spark job 'job_name_250' started. Processing partition 99 of 144."
2025-06-12T10:14:41.201+05:30 host=ip-100-197-46-250 service=job_executor app=data_pipeline environment=prod job_id=56463e1c source=Hadoop status=PENDING message="MapReduce job 'job_name_275' submitted to YARN queue."
2025-06-12T10:15:16.201+05:30 host=ip-49-163-227-71 service=job_executor app=data_pipeline environment=prod job_id=e685aac2 source=Spark status=SUCCEEDED message="Spark job 'job_name_232' completed successfully. Results saved."
2025-06-12T10:15:57.201+05:30 host=ip-39-252-131-248 service=job_executor app=data_pipeline environment=prod job_id=8437b00c source=Spark status=PENDING message="Spark job 'job_name_76' submitted to cluster, waiting for resources."
2025-06-12T10:18:20.201+05:30 host=ip-155-54-221-176 service=job_executor app=data_pipeline environment=prod job_id=aec1e288 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_268' completed successfully. Output written to S3."
2025-06-12T10:18:29.201+05:30 host=ip-41-179-56-122 service=job_executor app=data_pipeline environment=prod job_id=4ed1ad89 source=Glue status=RUNNING message="Glue ETL job 'job_name_226' started execution."
2025-06-12T10:18:32.201+05:30 host=ip-82-101-199-119 service=job_executor app=data_pipeline environment=prod job_id=27446a20 source=Hadoop status=FAILED error_type="IOError" message="Job 'job_name_126' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: 27446a20)
2025-06-12T10:18:40.201+05:30 host=ip-77-200-224-70 service=job_executor app=data_pipeline environment=prod job_id=0a458474 source=Glue status=FAILED error_type="com.microsoft.sqlserver.jdbc.SQLServerException" message="Job 'job_name_128' failed: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed"
com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user 'etl_user'
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:500)
	at com.yourcompany.db.DBConnector.getConnection(DBConnector.java:80)
	... 9 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException (for JobID: 0a458474)
2025-06-12T10:18:56.201+05:30 host=ip-108-141-96-75 service=job_executor app=data_pipeline environment=prod job_id=e0b74e90 source=Glue status=RUNNING message="Glue ETL job 'job_name_40' started execution."
2025-06-12T10:19:00.201+05:30 host=ip-118-175-249-180 service=job_executor app=data_pipeline environment=prod job_id=ae379b2f source=Glue status=FAILED error_type="java.net.ConnectException" message="Job 'job_name_142' failed: java.net.ConnectException: Connection refused"
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	... 7 more
Caused by: java.net.ConnectException (for JobID: ae379b2f)
2025-06-12T10:19:00.201+05:30 host=ip-55-169-44-22 service=job_executor app=data_pipeline environment=prod job_id=6cf249b7 source=Hadoop status=PENDING message="MapReduce job 'job_name_181' submitted to YARN queue."
2025-06-12T10:20:27.201+05:30 host=ip-11-20-54-120 service=job_executor app=data_pipeline environment=prod job_id=ae63fef9 source=Spark status=PENDING message="Spark job 'job_name_11' submitted to cluster, waiting for resources."
2025-06-12T10:20:28.201+05:30 host=ip-31-192-87-28 service=job_executor app=data_pipeline environment=prod job_id=71a0d6c9 source=Airflow status=RUNNING message="DAG 'dag_pipeline_34' task 'load_to_dw' is now running."
2025-06-12T10:20:46.201+05:30 host=ip-169-206-54-32 service=job_executor app=data_pipeline environment=prod job_id=f71b393e source=Glue status=RUNNING message="Glue ETL job 'job_name_9' started execution."
2025-06-12T10:20:54.201+05:30 host=ip-46-166-130-199 service=job_executor app=data_pipeline environment=prod job_id=d77f25d9 source=Airflow status=FAILED error_type="java.lang.NullPointerException" message="Job 'dag_pipeline_3' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: d77f25d9)
2025-06-12T10:21:03.201+05:30 host=ip-100-51-250-192 service=job_executor app=data_pipeline environment=prod job_id=52d935db source=Glue status=RUNNING message="Glue ETL job 'job_name_216' started execution."
2025-06-12T10:21:06.201+05:30 host=ip-23-4-232-20 service=job_executor app=data_pipeline environment=prod job_id=8e2e0909 source=Airflow status=PENDING message="DAG 'dag_pipeline_37' task 'extract_data' is pending execution."
2025-06-12T10:21:12.201+05:30 host=ip-135-81-229-221 service=job_executor app=data_pipeline environment=prod job_id=e0b74e90 source=Glue status=FAILED error_type="java.lang.IllegalArgumentException" message="Job 'job_name_40' failed: java.lang.IllegalArgumentException: Port number"
java.lang.IllegalArgumentException: Port number out of range: 70000
	at java.net.ServerSocket.bind(ServerSocket.java:223)
	at com.example.NetworkService.start(NetworkService.java:89)
	... 2 more
Caused by: java.lang.IllegalArgumentException (for JobID: e0b74e90)
2025-06-12T10:21:38.201+05:30 host=ip-96-14-2-151 service=job_executor app=data_pipeline environment=prod job_id=1ebc2f94 source=Hadoop status=FAILED error_type="java.net.ConnectException" message="Job 'job_name_231' failed: java.net.ConnectException: Connection refused"
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	... 7 more
Caused by: java.net.ConnectException (for JobID: 1ebc2f94)
2025-06-12T10:21:49.201+05:30 host=ip-54-177-46-160 service=job_executor app=data_pipeline environment=prod job_id=94a432a0 source=Spark status=PENDING message="Spark job 'job_name_240' submitted to cluster, waiting for resources."
2025-06-12T10:22:07.201+05:30 host=ip-166-190-43-101 service=job_executor app=data_pipeline environment=prod job_id=c3959950 source=Airflow status=RUNNING message="DAG 'dag_pipeline_23' task 'load_to_dw' is now running."
2025-06-12T10:22:34.201+05:30 host=ip-83-1-250-14 service=job_executor app=data_pipeline environment=prod job_id=ae63fef9 source=Spark status=RUNNING message="Spark job 'job_name_11' started. Processing partition 11 of 101."
2025-06-12T10:22:41.201+05:30 host=ip-156-135-104-229 service=job_executor app=data_pipeline environment=prod job_id=3fbb3ab4 source=Airflow status=FAILED error_type="TypeError" message="Job 'dag_pipeline_3' failed: TypeError: cannot concatenate"
TypeError: cannot concatenate 'str' and 'int' objects
	at transform_data.py:56
	at main_workflow.py:112
	... 4 more
Caused by: TypeError (for JobID: 3fbb3ab4)
2025-06-12T10:23:39.201+05:30 host=ip-61-10-52-172 service=job_executor app=data_pipeline environment=prod job_id=30d83d0d source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_108' completed successfully. Output written to S3."
2025-06-12T10:24:07.201+05:30 host=ip-151-231-194-227 service=job_executor app=data_pipeline environment=prod job_id=21c40e16 source=Glue status=FAILED error_type="IOError" message="Job 'job_name_98' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: 21c40e16)
2025-06-12T10:24:30.201+05:30 host=ip-194-199-211-167 service=job_executor app=data_pipeline environment=prod job_id=01cbfc5b source=Spark status=SUCCEEDED message="Spark job 'job_name_250' completed successfully. Results saved."
2025-06-12T10:24:50.201+05:30 host=ip-188-115-229-12 service=job_executor app=data_pipeline environment=prod job_id=5dfbde1f source=Airflow status=PENDING message="DAG 'dag_pipeline_36' task 'generate_report' is pending execution."
2025-06-12T10:24:56.201+05:30 host=ip-160-3-243-203 service=job_executor app=data_pipeline environment=prod job_id=ec792d74 source=Glue status=PENDING message="Glue ETL job 'job_name_7' is queued for execution."
2025-06-12T10:25:03.201+05:30 host=ip-189-190-1-159 service=job_executor app=data_pipeline environment=prod job_id=ad7749cc source=Spark status=SUCCEEDED message="Spark job 'job_name_75' completed successfully. Results saved."
2025-06-12T10:25:21.201+05:30 host=ip-166-227-126-223 service=job_executor app=data_pipeline environment=prod job_id=ca33381a source=Glue status=FAILED error_type="org.apache.spark.SparkException" message="Job 'job_name_19' failed: org.apache.spark.SparkException"
org.apache.spark.SparkException: Job aborted due to stage failure
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.Task.run(Task.scala:120)
	at org.apache.spark.executor.Executor.$anonfun$runJob$4(Executor.scala:180)
	... 5 more
Caused by: org.apache.spark.SparkException (for JobID: ca33381a)
2025-06-12T10:25:23.201+05:30 host=ip-110-158-110-216 service=job_executor app=data_pipeline environment=prod job_id=b33b553d source=Spark status=PENDING message="Spark job 'job_name_92' submitted to cluster, waiting for resources."
2025-06-12T10:25:31.201+05:30 host=ip-167-158-98-20 service=job_executor app=data_pipeline environment=prod job_id=1fe0b6fa source=Airflow status=PENDING message="DAG 'dag_pipeline_32' task 'extract_data' is pending execution."
2025-06-12T10:26:11.201+05:30 host=ip-144-87-32-240 service=job_executor app=data_pipeline environment=prod job_id=56463e1c source=Hadoop status=RUNNING message="MapReduce job 'job_name_275' running. Map phase 58% complete."
2025-06-12T10:26:33.201+05:30 host=ip-97-142-18-111 service=job_executor app=data_pipeline environment=prod job_id=fa780461 source=Airflow status=RUNNING message="DAG 'dag_pipeline_29' task 'load_to_dw' is now running."
2025-06-12T10:27:07.201+05:30 host=ip-143-46-107-160 service=job_executor app=data_pipeline environment=prod job_id=c3959950 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_23' task 'generate_report' completed successfully."
2025-06-12T10:27:44.201+05:30 host=ip-92-46-189-160 service=job_executor app=data_pipeline environment=prod job_id=52d935db source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_216' completed successfully. Output written to S3."
2025-06-12T10:27:46.201+05:30 host=ip-141-11-14-178 service=job_executor app=data_pipeline environment=prod job_id=ae63fef9 source=Spark status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_11' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: ae63fef9)
2025-06-12T10:28:09.201+05:30 host=ip-114-124-92-240 service=job_executor app=data_pipeline environment=prod job_id=1fe0b6fa source=Airflow status=RUNNING message="DAG 'dag_pipeline_32' task 'cleanup_temp' is now running."
2025-06-12T10:29:24.201+05:30 host=ip-106-90-20-186 service=job_executor app=data_pipeline environment=prod job_id=8e2e0909 source=Airflow status=RUNNING message="DAG 'dag_pipeline_37' task 'cleanup_temp' is now running."
2025-06-12T10:30:01.201+05:30 host=ip-47-83-239-235 service=job_executor app=data_pipeline environment=prod job_id=6cf249b7 source=Hadoop status=RUNNING message="MapReduce job 'job_name_181' running. Map phase 35% complete."
2025-06-12T10:30:19.201+05:30 host=ip-200-7-46-120 service=job_executor app=data_pipeline environment=prod job_id=989309df source=Spark status=PENDING message="Spark job 'job_name_248' submitted to cluster, waiting for resources."
2025-06-12T10:30:44.201+05:30 host=ip-121-74-33-161 service=job_executor app=data_pipeline environment=prod job_id=8e2e0909 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_37' task 'generate_report' completed successfully."
2025-06-12T10:30:56.201+05:30 host=ip-154-194-216-24 service=job_executor app=data_pipeline environment=prod job_id=8437b00c source=Spark status=RUNNING message="Spark job 'job_name_76' started. Processing partition 76 of 200."
2025-06-12T10:31:11.201+05:30 host=ip-54-233-151-183 service=job_executor app=data_pipeline environment=prod job_id=5dfbde1f source=Airflow status=RUNNING message="DAG 'dag_pipeline_36' task 'extract_data' is now running."
2025-06-12T10:31:35.201+05:30 host=ip-168-13-45-166 service=job_executor app=data_pipeline environment=prod job_id=ec792d74 source=Glue status=RUNNING message="Glue ETL job 'job_name_7' started execution."
2025-06-12T10:31:45.201+05:30 host=ip-21-64-119-191 service=job_executor app=data_pipeline environment=prod job_id=b33b553d source=Spark status=RUNNING message="Spark job 'job_name_92' started. Processing partition 50 of 137."
2025-06-12T10:32:21.201+05:30 host=ip-154-58-156-220 service=job_executor app=data_pipeline environment=prod job_id=fa780461 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_29' task 'cleanup_temp' completed successfully."
2025-06-12T10:32:24.201+05:30 host=ip-191-69-181-177 service=job_executor app=data_pipeline environment=prod job_id=58edfb31 source=Spark status=SUCCEEDED message="Spark job 'job_name_84' completed successfully. Results saved."
2025-06-12T10:32:37.201+05:30 host=ip-128-105-125-250 service=job_executor app=data_pipeline environment=prod job_id=05eacfaf source=Spark status=PENDING message="Spark job 'job_name_167' submitted to cluster, waiting for resources."
2025-06-12T10:33:42.201+05:30 host=ip-193-7-132-142 service=job_executor app=data_pipeline environment=prod job_id=72dba069 source=Glue status=PENDING message="Glue ETL job 'job_name_60' is queued for execution."
2025-06-12T10:33:42.201+05:30 host=ip-114-23-186-167 service=job_executor app=data_pipeline environment=prod job_id=05eacfaf source=Spark status=RUNNING message="Spark job 'job_name_167' started. Processing partition 98 of 152."
2025-06-12T10:34:59.201+05:30 host=ip-77-33-85-73 service=job_executor app=data_pipeline environment=prod job_id=6fb2af68 source=Airflow status=PENDING message="DAG 'dag_pipeline_30' task 'cleanup_temp' is pending execution."
2025-06-12T10:35:21.201+05:30 host=ip-95-191-158-219 service=job_executor app=data_pipeline environment=prod job_id=ffbababd source=Glue status=PENDING message="Glue ETL job 'job_name_241' is queued for execution."
2025-06-12T10:35:26.201+05:30 host=ip-185-115-138-249 service=job_executor app=data_pipeline environment=prod job_id=dc342e69 source=Spark status=PENDING message="Spark job 'job_name_198' submitted to cluster, waiting for resources."
2025-06-12T10:35:31.201+05:30 host=ip-160-52-72-202 service=job_executor app=data_pipeline environment=prod job_id=94a432a0 source=Spark status=RUNNING message="Spark job 'job_name_240' started. Processing partition 81 of 132."
2025-06-12T10:36:31.201+05:30 host=ip-170-225-56-50 service=job_executor app=data_pipeline environment=prod job_id=5dd14a93 source=Spark status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_224' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 5dd14a93)
2025-06-12T10:36:48.201+05:30 host=ip-167-61-54-24 service=job_executor app=data_pipeline environment=prod job_id=4ed1ad89 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_226' completed successfully. Output written to S3."
2025-06-12T10:38:03.201+05:30 host=ip-24-60-163-126 service=job_executor app=data_pipeline environment=prod job_id=6870f91e source=Airflow status=PENDING message="DAG 'dag_pipeline_46' task 'load_to_dw' is pending execution."
2025-06-12T10:38:24.201+05:30 host=ip-177-144-15-38 service=job_executor app=data_pipeline environment=prod job_id=bab80d51 source=Glue status=PENDING message="Glue ETL job 'job_name_39' is queued for execution."
2025-06-12T10:38:38.201+05:30 host=ip-68-214-206-40 service=job_executor app=data_pipeline environment=prod job_id=ea8d962d source=Glue status=PENDING message="Glue ETL job 'job_name_95' is queued for execution."
2025-06-12T10:38:42.201+05:30 host=ip-56-46-31-170 service=job_executor app=data_pipeline environment=prod job_id=f617803f source=Spark status=PENDING message="Spark job 'job_name_119' submitted to cluster, waiting for resources."
2025-06-12T10:39:12.201+05:30 host=ip-102-120-224-237 service=job_executor app=data_pipeline environment=prod job_id=72dba069 source=Glue status=RUNNING message="Glue ETL job 'job_name_60' started execution."
2025-06-12T10:39:40.201+05:30 host=ip-32-195-63-128 service=job_executor app=data_pipeline environment=prod job_id=00762ffb source=Airflow status=FAILED error_type="AttributeError" message="Job 'dag_pipeline_47' failed: AttributeError: 'NoneType' object has no attribute 'split'"
AttributeError: 'NoneType' object has no attribute 'split'
	at preprocess_data.py:33
	at main.py:78
	... 5 more
Caused by: AttributeError (for JobID: 00762ffb)
2025-06-12T10:40:02.201+05:30 host=ip-124-140-155-136 service=job_executor app=data_pipeline environment=prod job_id=6cf249b7 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_181' completed successfully. Output written to HDFS."
2025-06-12T10:40:13.201+05:30 host=ip-163-116-148-216 service=job_executor app=data_pipeline environment=prod job_id=fefd09d4 source=Spark status=PENDING message="Spark job 'job_name_257' submitted to cluster, waiting for resources."
2025-06-12T10:40:38.201+05:30 host=ip-88-62-248-81 service=job_executor app=data_pipeline environment=prod job_id=6870f91e source=Airflow status=RUNNING message="DAG 'dag_pipeline_46' task 'transform_data' is now running."
2025-06-12T10:41:12.201+05:30 host=ip-13-89-168-58 service=job_executor app=data_pipeline environment=prod job_id=989309df source=Spark status=RUNNING message="Spark job 'job_name_248' started. Processing partition 42 of 104."
2025-06-12T10:41:26.201+05:30 host=ip-13-75-171-30 service=job_executor app=data_pipeline environment=prod job_id=ffbababd source=Glue status=RUNNING message="Glue ETL job 'job_name_241' started execution."
2025-06-12T10:41:52.201+05:30 host=ip-96-97-138-57 service=job_executor app=data_pipeline environment=prod job_id=c34a04de source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_5' completed successfully. Output written to HDFS."
2025-06-12T10:42:41.201+05:30 host=ip-180-107-191-163 service=job_executor app=data_pipeline environment=prod job_id=71a0d6c9 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_34' task 'load_to_dw' completed successfully."
2025-06-12T10:42:47.201+05:30 host=ip-41-86-202-56 service=job_executor app=data_pipeline environment=prod job_id=183d1984 source=Glue status=PENDING message="Glue ETL job 'job_name_140' is queued for execution."
2025-06-12T10:43:19.201+05:30 host=ip-135-237-124-234 service=job_executor app=data_pipeline environment=prod job_id=e8c2e1d9 source=Hadoop status=PENDING message="MapReduce job 'job_name_132' submitted to YARN queue."
2025-06-12T10:44:01.201+05:30 host=ip-106-219-193-122 service=job_executor app=data_pipeline environment=prod job_id=6fb2af68 source=Airflow status=RUNNING message="DAG 'dag_pipeline_30' task 'load_to_dw' is now running."
2025-06-12T10:45:09.201+05:30 host=ip-186-161-128-123 service=job_executor app=data_pipeline environment=prod job_id=05eacfaf source=Spark status=SUCCEEDED message="Spark job 'job_name_167' completed successfully. Results saved."
2025-06-12T10:47:06.201+05:30 host=ip-94-71-207-178 service=job_executor app=data_pipeline environment=prod job_id=1fe0b6fa source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_32' task 'transform_data' completed successfully."
2025-06-12T10:47:27.201+05:30 host=ip-121-227-34-67 service=job_executor app=data_pipeline environment=prod job_id=002ea974 source=Hadoop status=PENDING message="MapReduce job 'job_name_202' submitted to YARN queue."
2025-06-12T10:47:30.201+05:30 host=ip-18-84-25-144 service=job_executor app=data_pipeline environment=prod job_id=8a5c3603 source=Spark status=PENDING message="Spark job 'job_name_145' submitted to cluster, waiting for resources."
2025-06-12T10:47:33.201+05:30 host=ip-36-237-252-216 service=job_executor app=data_pipeline environment=prod job_id=121f3b54 source=Airflow status=PENDING message="DAG 'dag_pipeline_3' task 'transform_data' is pending execution."
2025-06-12T10:47:46.201+05:30 host=ip-61-71-235-118 service=job_executor app=data_pipeline environment=prod job_id=ea8d962d source=Glue status=RUNNING message="Glue ETL job 'job_name_95' started execution."
2025-06-12T10:48:38.201+05:30 host=ip-85-63-30-80 service=job_executor app=data_pipeline environment=prod job_id=bab80d51 source=Glue status=RUNNING message="Glue ETL job 'job_name_39' started execution."
2025-06-12T10:49:08.201+05:30 host=ip-192-16-230-24 service=job_executor app=data_pipeline environment=prod job_id=002ea974 source=Hadoop status=RUNNING message="MapReduce job 'job_name_202' running. Map phase 16% complete."
2025-06-12T10:49:09.201+05:30 host=ip-37-206-236-216 service=job_executor app=data_pipeline environment=prod job_id=f71b393e source=Glue status=FAILED error_type="java.io.FileNotFoundException" message="Job 'job_name_9' failed: java.io.FileNotFoundException"
java.io.FileNotFoundException: /path/to/output/file (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	... 6 more
Caused by: java.io.FileNotFoundException (for JobID: f71b393e)
2025-06-12T10:49:13.201+05:30 host=ip-160-239-226-177 service=job_executor app=data_pipeline environment=prod job_id=dc342e69 source=Spark status=RUNNING message="Spark job 'job_name_198' started. Processing partition 88 of 167."
2025-06-12T10:49:19.201+05:30 host=ip-43-240-97-228 service=job_executor app=data_pipeline environment=prod job_id=f617803f source=Spark status=RUNNING message="Spark job 'job_name_119' started. Processing partition 21 of 145."
2025-06-12T10:49:22.201+05:30 host=ip-21-136-178-73 service=job_executor app=data_pipeline environment=prod job_id=f8bf1198 source=Spark status=PENDING message="Spark job 'job_name_42' submitted to cluster, waiting for resources."
2025-06-12T10:49:34.201+05:30 host=ip-46-249-175-18 service=job_executor app=data_pipeline environment=prod job_id=56463e1c source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_275' completed successfully. Output written to HDFS."
2025-06-12T10:49:43.201+05:30 host=ip-157-105-131-155 service=job_executor app=data_pipeline environment=prod job_id=a03ed356 source=Spark status=PENDING message="Spark job 'job_name_31' submitted to cluster, waiting for resources."
2025-06-12T10:50:18.201+05:30 host=ip-68-72-160-24 service=job_executor app=data_pipeline environment=prod job_id=da45c5a1 source=Spark status=PENDING message="Spark job 'job_name_238' submitted to cluster, waiting for resources."
2025-06-12T10:50:47.201+05:30 host=ip-20-98-165-30 service=job_executor app=data_pipeline environment=prod job_id=b33b553d source=Spark status=SUCCEEDED message="Spark job 'job_name_92' completed successfully. Results saved."
2025-06-12T10:51:05.201+05:30 host=ip-23-69-174-159 service=job_executor app=data_pipeline environment=prod job_id=8a5c3603 source=Spark status=RUNNING message="Spark job 'job_name_145' started. Processing partition 33 of 186."
2025-06-12T10:51:13.201+05:30 host=ip-48-94-97-220 service=job_executor app=data_pipeline environment=prod job_id=fefd09d4 source=Spark status=RUNNING message="Spark job 'job_name_257' started. Processing partition 10 of 136."
2025-06-12T10:52:34.201+05:30 host=ip-194-190-98-198 service=job_executor app=data_pipeline environment=prod job_id=a03ed356 source=Spark status=RUNNING message="Spark job 'job_name_31' started. Processing partition 94 of 188."
2025-06-12T10:52:57.201+05:30 host=ip-101-230-33-251 service=job_executor app=data_pipeline environment=prod job_id=002ea974 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_202' completed successfully. Output written to HDFS."
2025-06-12T10:53:45.201+05:30 host=ip-150-127-104-157 service=job_executor app=data_pipeline environment=prod job_id=989309df source=Spark status=SUCCEEDED message="Spark job 'job_name_248' completed successfully. Results saved."
2025-06-12T10:54:16.201+05:30 host=ip-39-65-98-81 service=job_executor app=data_pipeline environment=prod job_id=12082ea3 source=Spark status=PENDING message="Spark job 'job_name_110' submitted to cluster, waiting for resources."
2025-06-12T10:54:27.201+05:30 host=ip-124-38-121-210 service=job_executor app=data_pipeline environment=prod job_id=ea8d962d source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_95' completed successfully. Output written to S3."
2025-06-12T10:54:33.201+05:30 host=ip-126-65-30-198 service=job_executor app=data_pipeline environment=prod job_id=ddc07ab0 source=Glue status=PENDING message="Glue ETL job 'job_name_38' is queued for execution."
2025-06-12T10:54:38.201+05:30 host=ip-38-90-23-201 service=job_executor app=data_pipeline environment=prod job_id=5dfbde1f source=Airflow status=FAILED error_type="java.net.ConnectException" message="Job 'dag_pipeline_36' failed: java.net.ConnectException: Connection refused"
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	... 7 more
Caused by: java.net.ConnectException (for JobID: 5dfbde1f)
2025-06-12T10:54:42.201+05:30 host=ip-25-56-221-127 service=job_executor app=data_pipeline environment=prod job_id=2a753999 source=Glue status=PENDING message="Glue ETL job 'job_name_24' is queued for execution."
2025-06-12T10:54:43.201+05:30 host=ip-162-197-13-58 service=job_executor app=data_pipeline environment=prod job_id=29ba9a1c source=Spark status=PENDING message="Spark job 'job_name_62' submitted to cluster, waiting for resources."
2025-06-12T10:54:56.201+05:30 host=ip-91-37-216-105 service=job_executor app=data_pipeline environment=prod job_id=e8c2e1d9 source=Hadoop status=RUNNING message="MapReduce job 'job_name_132' running. Map phase 27% complete."
2025-06-12T10:55:19.201+05:30 host=ip-89-96-89-46 service=job_executor app=data_pipeline environment=prod job_id=da45c5a1 source=Spark status=RUNNING message="Spark job 'job_name_238' started. Processing partition 11 of 125."
2025-06-12T10:55:28.201+05:30 host=ip-151-5-136-190 service=job_executor app=data_pipeline environment=prod job_id=ec792d74 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_7' completed successfully. Output written to S3."
2025-06-12T10:55:38.201+05:30 host=ip-168-159-191-230 service=job_executor app=data_pipeline environment=prod job_id=a03ed356 source=Spark status=SUCCEEDED message="Spark job 'job_name_31' completed successfully. Results saved."
2025-06-12T10:56:03.201+05:30 host=ip-70-37-94-86 service=job_executor app=data_pipeline environment=prod job_id=121f3b54 source=Airflow status=RUNNING message="DAG 'dag_pipeline_3' task 'generate_report' is now running."
2025-06-12T10:56:08.201+05:30 host=ip-174-231-36-212 service=job_executor app=data_pipeline environment=prod job_id=58c6b206 source=Glue status=PENDING message="Glue ETL job 'job_name_144' is queued for execution."
2025-06-12T10:56:16.201+05:30 host=ip-37-16-42-55 service=job_executor app=data_pipeline environment=prod job_id=fefd09d4 source=Spark status=SUCCEEDED message="Spark job 'job_name_257' completed successfully. Results saved."
2025-06-12T10:56:40.201+05:30 host=ip-75-51-105-153 service=job_executor app=data_pipeline environment=prod job_id=8437b00c source=Spark status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_76' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 8437b00c)
2025-06-12T10:56:43.201+05:30 host=ip-24-191-13-208 service=job_executor app=data_pipeline environment=prod job_id=70256e96 source=Airflow status=PENDING message="DAG 'dag_pipeline_22' task 'transform_data' is pending execution."
2025-06-12T10:56:53.201+05:30 host=ip-59-241-182-105 service=job_executor app=data_pipeline environment=prod job_id=5021c1e8 source=Spark status=PENDING message="Spark job 'job_name_61' submitted to cluster, waiting for resources."
2025-06-12T10:56:54.201+05:30 host=ip-53-134-60-32 service=job_executor app=data_pipeline environment=prod job_id=183d1984 source=Glue status=RUNNING message="Glue ETL job 'job_name_140' started execution."
2025-06-12T10:57:49.201+05:30 host=ip-100-145-158-145 service=job_executor app=data_pipeline environment=prod job_id=70256e96 source=Airflow status=RUNNING message="DAG 'dag_pipeline_22' task 'extract_data' is now running."
2025-06-12T10:57:50.201+05:30 host=ip-154-226-149-25 service=job_executor app=data_pipeline environment=prod job_id=bab80d51 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_39' completed successfully. Output written to S3."
2025-06-12T10:58:03.201+05:30 host=ip-108-223-120-135 service=job_executor app=data_pipeline environment=prod job_id=f8bf1198 source=Spark status=RUNNING message="Spark job 'job_name_42' started. Processing partition 15 of 167."
2025-06-12T10:58:04.201+05:30 host=ip-178-60-3-100 service=job_executor app=data_pipeline environment=prod job_id=bb2dd002 source=Hadoop status=PENDING message="MapReduce job 'job_name_102' submitted to YARN queue."
2025-06-12T10:58:46.201+05:30 host=ip-143-11-20-249 service=job_executor app=data_pipeline environment=prod job_id=f617803f source=Spark status=FAILED error_type="ZeroDivisionError" message="Job 'job_name_119' failed: ZeroDivisionError: division by zero"
ZeroDivisionError: division by zero
	at calculate.py:87
	at main.py:54
	... 2 more
Caused by: ZeroDivisionError (for JobID: f617803f)
2025-06-12T10:58:51.201+05:30 host=ip-165-202-91-140 service=job_executor app=data_pipeline environment=prod job_id=bdea3d1f source=Hadoop status=PENDING message="MapReduce job 'job_name_187' submitted to YARN queue."
2025-06-12T10:59:07.201+05:30 host=ip-102-69-244-87 service=job_executor app=data_pipeline environment=prod job_id=58c6b206 source=Glue status=RUNNING message="Glue ETL job 'job_name_144' started execution."
2025-06-12T10:59:21.201+05:30 host=ip-128-196-103-242 service=job_executor app=data_pipeline environment=prod job_id=e5320976 source=Glue status=PENDING message="Glue ETL job 'job_name_43' is queued for execution."
2025-06-12T10:59:46.201+05:30 host=ip-106-91-93-120 service=job_executor app=data_pipeline environment=prod job_id=121f3b54 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_3' task 'generate_report' completed successfully."
2025-06-12T11:00:33.201+05:30 host=ip-89-151-189-11 service=job_executor app=data_pipeline environment=prod job_id=da45c5a1 source=Spark status=FAILED error_type="FileNotFoundError" message="Job 'job_name_238' failed: FileNotFoundError: [Errno 2] No such file or directory"
FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'
	at open_config.py:45
	at initialize.py:12
	... 3 more
Caused by: FileNotFoundError (for JobID: da45c5a1)
2025-06-12T11:00:45.201+05:30 host=ip-65-188-81-106 service=job_executor app=data_pipeline environment=prod job_id=2a753999 source=Glue status=RUNNING message="Glue ETL job 'job_name_24' started execution."
2025-06-12T11:01:38.201+05:30 host=ip-33-29-197-9 service=job_executor app=data_pipeline environment=prod job_id=f498a243 source=Hadoop status=PENDING message="MapReduce job 'job_name_214' submitted to YARN queue."
2025-06-12T11:02:20.201+05:30 host=ip-146-130-99-158 service=job_executor app=data_pipeline environment=prod job_id=8735b127 source=Glue status=PENDING message="Glue ETL job 'job_name_138' is queued for execution."
2025-06-12T11:02:44.201+05:30 host=ip-21-22-92-143 service=job_executor app=data_pipeline environment=prod job_id=94a432a0 source=Spark status=FAILED error_type="org.apache.spark.SparkException" message="Job 'job_name_240' failed: org.apache.spark.SparkException"
org.apache.spark.SparkException: Job aborted due to stage failure
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.Task.run(Task.scala:120)
	at org.apache.spark.executor.Executor.$anonfun$runJob$4(Executor.scala:180)
	... 5 more
Caused by: org.apache.spark.SparkException (for JobID: 94a432a0)
2025-06-12T11:03:05.201+05:30 host=ip-111-187-224-176 service=job_executor app=data_pipeline environment=prod job_id=481f842e source=Airflow status=PENDING message="DAG 'dag_pipeline_48' task 'transform_data' is pending execution."
2025-06-12T11:03:56.201+05:30 host=ip-124-91-243-43 service=job_executor app=data_pipeline environment=prod job_id=12082ea3 source=Spark status=RUNNING message="Spark job 'job_name_110' started. Processing partition 62 of 189."
2025-06-12T11:03:59.201+05:30 host=ip-125-83-179-143 service=job_executor app=data_pipeline environment=prod job_id=4fe6ac51 source=Glue status=PENDING message="Glue ETL job 'job_name_146' is queued for execution."
2025-06-12T11:04:10.201+05:30 host=ip-15-180-65-47 service=job_executor app=data_pipeline environment=prod job_id=70256e96 source=Airflow status=FAILED error_type="org.apache.kafka.common.errors.TimeoutException" message="Job 'dag_pipeline_22' failed: org.apache.kafka.common.errors.TimeoutException"
org.apache.kafka.common.errors.TimeoutException: Topic partition assignment timeout after 60000 ms
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:350)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1245)
	... 6 more
Caused by: org.apache.kafka.common.errors.TimeoutException (for JobID: 70256e96)
2025-06-12T11:04:28.201+05:30 host=ip-45-200-212-170 service=job_executor app=data_pipeline environment=prod job_id=bd76482e source=Glue status=PENDING message="Glue ETL job 'job_name_103' is queued for execution."
2025-06-12T11:04:44.201+05:30 host=ip-90-163-19-207 service=job_executor app=data_pipeline environment=prod job_id=5021c1e8 source=Spark status=RUNNING message="Spark job 'job_name_61' started. Processing partition 28 of 156."
2025-06-12T11:04:53.201+05:30 host=ip-29-176-27-221 service=job_executor app=data_pipeline environment=prod job_id=a2521766 source=Airflow status=PENDING message="DAG 'dag_pipeline_47' task 'cleanup_temp' is pending execution."
2025-06-12T11:05:04.201+05:30 host=ip-199-0-202-107 service=job_executor app=data_pipeline environment=prod job_id=6870f91e source=Airflow status=FAILED error_type="IOError" message="Job 'dag_pipeline_46' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: 6870f91e)
2025-06-12T11:05:07.201+05:30 host=ip-89-141-151-131 service=job_executor app=data_pipeline environment=prod job_id=29ba9a1c source=Spark status=RUNNING message="Spark job 'job_name_62' started. Processing partition 77 of 181."
2025-06-12T11:05:46.201+05:30 host=ip-88-32-239-81 service=job_executor app=data_pipeline environment=prod job_id=6fb2af68 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_30' task 'cleanup_temp' completed successfully."
2025-06-12T11:05:56.201+05:30 host=ip-163-202-232-145 service=job_executor app=data_pipeline environment=prod job_id=a2521766 source=Airflow status=RUNNING message="DAG 'dag_pipeline_47' task 'transform_data' is now running."
2025-06-12T11:06:04.201+05:30 host=ip-75-245-63-209 service=job_executor app=data_pipeline environment=prod job_id=ca2f000e source=Airflow status=PENDING message="DAG 'dag_pipeline_39' task 'load_to_dw' is pending execution."
2025-06-12T11:06:10.201+05:30 host=ip-128-41-9-129 service=job_executor app=data_pipeline environment=prod job_id=34099ebd source=Airflow status=PENDING message="DAG 'dag_pipeline_15' task 'transform_data' is pending execution."
2025-06-12T11:06:15.201+05:30 host=ip-181-179-1-244 service=job_executor app=data_pipeline environment=prod job_id=88559ff2 source=Spark status=PENDING message="Spark job 'job_name_191' submitted to cluster, waiting for resources."
2025-06-12T11:06:30.201+05:30 host=ip-111-31-9-235 service=job_executor app=data_pipeline environment=prod job_id=4fe6ac51 source=Glue status=RUNNING message="Glue ETL job 'job_name_146' started execution."
2025-06-12T11:06:34.201+05:30 host=ip-11-86-254-62 service=job_executor app=data_pipeline environment=prod job_id=bdea3d1f source=Hadoop status=RUNNING message="MapReduce job 'job_name_187' running. Map phase 12% complete."
2025-06-12T11:06:39.201+05:30 host=ip-146-4-161-22 service=job_executor app=data_pipeline environment=prod job_id=e5320976 source=Glue status=RUNNING message="Glue ETL job 'job_name_43' started execution."
2025-06-12T11:06:47.201+05:30 host=ip-24-20-246-255 service=job_executor app=data_pipeline environment=prod job_id=59ee1364 source=Hadoop status=PENDING message="MapReduce job 'job_name_66' submitted to YARN queue."
2025-06-12T11:06:50.201+05:30 host=ip-105-89-233-11 service=job_executor app=data_pipeline environment=prod job_id=a784b8b2 source=Glue status=PENDING message="Glue ETL job 'job_name_174' is queued for execution."
2025-06-12T11:07:17.201+05:30 host=ip-166-228-160-43 service=job_executor app=data_pipeline environment=prod job_id=72dba069 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_60' completed successfully. Output written to S3."
2025-06-12T11:07:39.201+05:30 host=ip-143-236-252-52 service=job_executor app=data_pipeline environment=prod job_id=2fdee99d source=Glue status=PENDING message="Glue ETL job 'job_name_111' is queued for execution."
2025-06-12T11:08:23.201+05:30 host=ip-181-164-32-4 service=job_executor app=data_pipeline environment=prod job_id=e69389f9 source=Hadoop status=PENDING message="MapReduce job 'job_name_195' submitted to YARN queue."
2025-06-12T11:08:36.201+05:30 host=ip-25-138-195-36 service=job_executor app=data_pipeline environment=prod job_id=2a753999 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_24' completed successfully. Output written to S3."
2025-06-12T11:08:45.201+05:30 host=ip-165-21-75-240 service=job_executor app=data_pipeline environment=prod job_id=5021c1e8 source=Spark status=SUCCEEDED message="Spark job 'job_name_61' completed successfully. Results saved."
2025-06-12T11:08:54.201+05:30 host=ip-62-195-102-187 service=job_executor app=data_pipeline environment=prod job_id=bb2dd002 source=Hadoop status=RUNNING message="MapReduce job 'job_name_102' running. Map phase 25% complete."
2025-06-12T11:09:00.201+05:30 host=ip-114-116-12-174 service=job_executor app=data_pipeline environment=prod job_id=ddc07ab0 source=Glue status=RUNNING message="Glue ETL job 'job_name_38' started execution."
2025-06-12T11:09:48.201+05:30 host=ip-29-167-123-40 service=job_executor app=data_pipeline environment=prod job_id=ffbababd source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_241' completed successfully. Output written to S3."
2025-06-12T11:10:05.201+05:30 host=ip-20-169-154-204 service=job_executor app=data_pipeline environment=prod job_id=ac83b1ed source=Spark status=PENDING message="Spark job 'job_name_3' submitted to cluster, waiting for resources."
2025-06-12T11:11:13.201+05:30 host=ip-67-153-136-71 service=job_executor app=data_pipeline environment=prod job_id=12082ea3 source=Spark status=SUCCEEDED message="Spark job 'job_name_110' completed successfully. Results saved."
2025-06-12T11:11:15.201+05:30 host=ip-116-119-223-154 service=job_executor app=data_pipeline environment=prod job_id=ea91f10c source=Airflow status=PENDING message="DAG 'dag_pipeline_32' task 'generate_report' is pending execution."
2025-06-12T11:11:28.201+05:30 host=ip-150-225-193-201 service=job_executor app=data_pipeline environment=prod job_id=2b078091 source=Airflow status=PENDING message="DAG 'dag_pipeline_49' task 'generate_report' is pending execution."
2025-06-12T11:11:45.201+05:30 host=ip-41-50-71-93 service=job_executor app=data_pipeline environment=prod job_id=dc342e69 source=Spark status=SUCCEEDED message="Spark job 'job_name_198' completed successfully. Results saved."
2025-06-12T11:11:48.201+05:30 host=ip-39-37-169-203 service=job_executor app=data_pipeline environment=prod job_id=59ee1364 source=Hadoop status=RUNNING message="MapReduce job 'job_name_66' running. Map phase 43% complete."
2025-06-12T11:12:24.201+05:30 host=ip-55-193-82-27 service=job_executor app=data_pipeline environment=prod job_id=a2521766 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_47' task 'cleanup_temp' completed successfully."
2025-06-12T11:12:25.201+05:30 host=ip-156-215-250-21 service=job_executor app=data_pipeline environment=prod job_id=bd76482e source=Glue status=RUNNING message="Glue ETL job 'job_name_103' started execution."
2025-06-12T11:12:30.201+05:30 host=ip-74-195-53-179 service=job_executor app=data_pipeline environment=prod job_id=bdea3d1f source=Hadoop status=FAILED error_type="com.microsoft.sqlserver.jdbc.SQLServerException" message="Job 'job_name_187' failed: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed"
com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user 'etl_user'
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:500)
	at com.yourcompany.db.DBConnector.getConnection(DBConnector.java:80)
	... 9 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException (for JobID: bdea3d1f)
2025-06-12T11:13:40.201+05:30 host=ip-63-37-119-82 service=job_executor app=data_pipeline environment=prod job_id=9a0d3de3 source=Airflow status=PENDING message="DAG 'dag_pipeline_21' task 'generate_report' is pending execution."
2025-06-12T11:14:06.201+05:30 host=ip-92-199-187-164 service=job_executor app=data_pipeline environment=prod job_id=e69389f9 source=Hadoop status=RUNNING message="MapReduce job 'job_name_195' running. Map phase 35% complete."
2025-06-12T11:14:46.201+05:30 host=ip-154-216-175-191 service=job_executor app=data_pipeline environment=prod job_id=2fdee99d source=Glue status=RUNNING message="Glue ETL job 'job_name_111' started execution."
2025-06-12T11:14:47.201+05:30 host=ip-151-205-82-245 service=job_executor app=data_pipeline environment=prod job_id=34099ebd source=Airflow status=RUNNING message="DAG 'dag_pipeline_15' task 'cleanup_temp' is now running."
2025-06-12T11:15:03.201+05:30 host=ip-123-111-195-41 service=job_executor app=data_pipeline environment=prod job_id=183d1984 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_140' completed successfully. Output written to S3."
2025-06-12T11:15:07.201+05:30 host=ip-144-10-154-248 service=job_executor app=data_pipeline environment=prod job_id=88559ff2 source=Spark status=RUNNING message="Spark job 'job_name_191' started. Processing partition 2 of 155."
2025-06-12T11:15:22.201+05:30 host=ip-99-158-105-78 service=job_executor app=data_pipeline environment=prod job_id=ca2f000e source=Airflow status=RUNNING message="DAG 'dag_pipeline_39' task 'cleanup_temp' is now running."
2025-06-12T11:15:53.201+05:30 host=ip-198-203-78-164 service=job_executor app=data_pipeline environment=prod job_id=8735b127 source=Glue status=RUNNING message="Glue ETL job 'job_name_138' started execution."
2025-06-12T11:16:06.201+05:30 host=ip-102-40-136-137 service=job_executor app=data_pipeline environment=prod job_id=481f842e source=Airflow status=RUNNING message="DAG 'dag_pipeline_48' task 'extract_data' is now running."
2025-06-12T11:16:11.201+05:30 host=ip-81-165-94-166 service=job_executor app=data_pipeline environment=prod job_id=a784b8b2 source=Glue status=RUNNING message="Glue ETL job 'job_name_174' started execution."
2025-06-12T11:16:12.201+05:30 host=ip-61-19-29-128 service=job_executor app=data_pipeline environment=prod job_id=8a5c3603 source=Spark status=FAILED error_type="java.lang.NullPointerException" message="Job 'job_name_145' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: 8a5c3603)
2025-06-12T11:16:32.201+05:30 host=ip-144-213-233-96 service=job_executor app=data_pipeline environment=prod job_id=f498a243 source=Hadoop status=RUNNING message="MapReduce job 'job_name_214' running. Map phase 80% complete."
2025-06-12T11:16:59.201+05:30 host=ip-118-81-36-90 service=job_executor app=data_pipeline environment=prod job_id=f498a243 source=Hadoop status=FAILED error_type="java.lang.NullPointerException" message="Job 'job_name_214' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: f498a243)
2025-06-12T11:17:37.201+05:30 host=ip-92-74-63-19 service=job_executor app=data_pipeline environment=prod job_id=481f842e source=Airflow status=FAILED error_type="IOError" message="Job 'dag_pipeline_48' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: 481f842e)
2025-06-12T11:18:58.201+05:30 host=ip-52-102-37-21 service=job_executor app=data_pipeline environment=prod job_id=8735b127 source=Glue status=FAILED error_type="IOError" message="Job 'job_name_138' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: 8735b127)
2025-06-12T11:19:29.201+05:30 host=ip-73-0-241-149 service=job_executor app=data_pipeline environment=prod job_id=e8c2e1d9 source=Hadoop status=FAILED error_type="TypeError" message="Job 'job_name_132' failed: TypeError: cannot concatenate"
TypeError: cannot concatenate 'str' and 'int' objects
	at transform_data.py:56
	at main_workflow.py:112
	... 4 more
Caused by: TypeError (for JobID: e8c2e1d9)
2025-06-12T11:19:32.201+05:30 host=ip-124-189-16-234 service=job_executor app=data_pipeline environment=prod job_id=8399f439 source=Airflow status=PENDING message="DAG 'dag_pipeline_21' task 'generate_report' is pending execution."
2025-06-12T11:19:32.201+05:30 host=ip-190-53-29-111 service=job_executor app=data_pipeline environment=prod job_id=0f425668 source=Spark status=PENDING message="Spark job 'job_name_256' submitted to cluster, waiting for resources."
2025-06-12T11:19:38.201+05:30 host=ip-14-29-241-43 service=job_executor app=data_pipeline environment=prod job_id=58c6b206 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_144' completed successfully. Output written to S3."
2025-06-12T11:20:13.201+05:30 host=ip-106-192-7-205 service=job_executor app=data_pipeline environment=prod job_id=8d636da6 source=Glue status=PENDING message="Glue ETL job 'job_name_225' is queued for execution."
2025-06-12T11:20:17.201+05:30 host=ip-101-119-13-113 service=job_executor app=data_pipeline environment=prod job_id=59ee1364 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_66' completed successfully. Output written to HDFS."
2025-06-12T11:20:49.201+05:30 host=ip-185-228-213-225 service=job_executor app=data_pipeline environment=prod job_id=9d5d9051 source=Spark status=PENDING message="Spark job 'job_name_20' submitted to cluster, waiting for resources."
2025-06-12T11:21:01.201+05:30 host=ip-34-97-155-102 service=job_executor app=data_pipeline environment=prod job_id=38e5930e source=Spark status=PENDING message="Spark job 'job_name_182' submitted to cluster, waiting for resources."
2025-06-12T11:21:23.201+05:30 host=ip-160-167-246-1 service=job_executor app=data_pipeline environment=prod job_id=34099ebd source=Airflow status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'dag_pipeline_15' failed: com.amazonaws.services.glue.GlueException: Entity not found"
com.amazonaws.services.glue.GlueException: Entity not found: database 'sales_db'
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:350)
	at com.amazonaws.services.glue.AWSGlueClient.getTable(AWSGlueClient.java:115)
	... 4 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 34099ebd)
2025-06-12T11:22:08.201+05:30 host=ip-39-226-76-96 service=job_executor app=data_pipeline environment=prod job_id=4c226e22 source=Hadoop status=PENDING message="MapReduce job 'job_name_1' submitted to YARN queue."
2025-06-12T11:22:17.201+05:30 host=ip-153-107-130-38 service=job_executor app=data_pipeline environment=prod job_id=4fe6ac51 source=Glue status=FAILED error_type="java.net.ConnectException" message="Job 'job_name_146' failed: java.net.ConnectException: Connection refused"
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	... 7 more
Caused by: java.net.ConnectException (for JobID: 4fe6ac51)
2025-06-12T11:22:28.201+05:30 host=ip-197-11-182-182 service=job_executor app=data_pipeline environment=prod job_id=386bf406 source=Glue status=PENDING message="Glue ETL job 'job_name_269' is queued for execution."
2025-06-12T11:22:46.201+05:30 host=ip-40-138-143-169 service=job_executor app=data_pipeline environment=prod job_id=5449c628 source=Spark status=PENDING message="Spark job 'job_name_281' submitted to cluster, waiting for resources."
2025-06-12T11:23:24.201+05:30 host=ip-63-84-21-60 service=job_executor app=data_pipeline environment=prod job_id=f1978980 source=Glue status=PENDING message="Glue ETL job 'job_name_271' is queued for execution."
2025-06-12T11:23:46.201+05:30 host=ip-55-44-140-139 service=job_executor app=data_pipeline environment=prod job_id=2b078091 source=Airflow status=RUNNING message="DAG 'dag_pipeline_49' task 'extract_data' is now running."
2025-06-12T11:24:43.201+05:30 host=ip-150-54-190-174 service=job_executor app=data_pipeline environment=prod job_id=0c9eb7b1 source=Hadoop status=PENDING message="MapReduce job 'job_name_270' submitted to YARN queue."
2025-06-12T11:25:01.201+05:30 host=ip-186-244-153-90 service=job_executor app=data_pipeline environment=prod job_id=ea91f10c source=Airflow status=RUNNING message="DAG 'dag_pipeline_32' task 'load_to_dw' is now running."
2025-06-12T11:25:05.201+05:30 host=ip-147-181-79-78 service=job_executor app=data_pipeline environment=prod job_id=ac83b1ed source=Spark status=RUNNING message="Spark job 'job_name_3' started. Processing partition 43 of 149."
2025-06-12T11:25:24.201+05:30 host=ip-87-58-6-191 service=job_executor app=data_pipeline environment=prod job_id=9a0d3de3 source=Airflow status=RUNNING message="DAG 'dag_pipeline_21' task 'generate_report' is now running."
2025-06-12T11:27:14.201+05:30 host=ip-108-246-229-177 service=job_executor app=data_pipeline environment=prod job_id=f1978980 source=Glue status=RUNNING message="Glue ETL job 'job_name_271' started execution."
2025-06-12T11:27:34.201+05:30 host=ip-67-179-85-145 service=job_executor app=data_pipeline environment=prod job_id=f8bf1198 source=Spark status=SUCCEEDED message="Spark job 'job_name_42' completed successfully. Results saved."
2025-06-12T11:27:39.201+05:30 host=ip-149-174-64-87 service=job_executor app=data_pipeline environment=prod job_id=b3b1853a source=Hadoop status=PENDING message="MapReduce job 'job_name_100' submitted to YARN queue."
2025-06-12T11:28:18.201+05:30 host=ip-28-242-155-208 service=job_executor app=data_pipeline environment=prod job_id=9d5d9051 source=Spark status=RUNNING message="Spark job 'job_name_20' started. Processing partition 32 of 152."
2025-06-12T11:28:34.201+05:30 host=ip-20-83-14-9 service=job_executor app=data_pipeline environment=prod job_id=bb2dd002 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_102' completed successfully. Output written to HDFS."
2025-06-12T11:28:55.201+05:30 host=ip-185-217-34-3 service=job_executor app=data_pipeline environment=prod job_id=8d636da6 source=Glue status=RUNNING message="Glue ETL job 'job_name_225' started execution."
2025-06-12T11:29:56.201+05:30 host=ip-158-152-110-9 service=job_executor app=data_pipeline environment=prod job_id=3b184a13 source=Glue status=PENDING message="Glue ETL job 'job_name_73' is queued for execution."
2025-06-12T11:30:30.201+05:30 host=ip-156-63-249-232 service=job_executor app=data_pipeline environment=prod job_id=e69389f9 source=Hadoop status=FAILED error_type="java.net.ConnectException" message="Job 'job_name_195' failed: java.net.ConnectException: Connection refused"
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	... 7 more
Caused by: java.net.ConnectException (for JobID: e69389f9)
2025-06-12T11:30:34.201+05:30 host=ip-166-40-170-176 service=job_executor app=data_pipeline environment=prod job_id=a784b8b2 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_174' completed successfully. Output written to S3."
2025-06-12T11:30:45.201+05:30 host=ip-182-126-82-194 service=job_executor app=data_pipeline environment=prod job_id=38e5930e source=Spark status=RUNNING message="Spark job 'job_name_182' started. Processing partition 47 of 122."
2025-06-12T11:30:46.201+05:30 host=ip-181-161-2-224 service=job_executor app=data_pipeline environment=prod job_id=09fbace9 source=Spark status=PENDING message="Spark job 'job_name_107' submitted to cluster, waiting for resources."
2025-06-12T11:31:02.201+05:30 host=ip-191-136-152-97 service=job_executor app=data_pipeline environment=prod job_id=0c9eb7b1 source=Hadoop status=RUNNING message="MapReduce job 'job_name_270' running. Map phase 43% complete."
2025-06-12T11:31:03.201+05:30 host=ip-151-185-9-5 service=job_executor app=data_pipeline environment=prod job_id=5449c628 source=Spark status=RUNNING message="Spark job 'job_name_281' started. Processing partition 7 of 187."
2025-06-12T11:31:04.201+05:30 host=ip-48-43-212-246 service=job_executor app=data_pipeline environment=prod job_id=4c226e22 source=Hadoop status=RUNNING message="MapReduce job 'job_name_1' running. Map phase 89% complete."
2025-06-12T11:31:17.201+05:30 host=ip-11-85-91-168 service=job_executor app=data_pipeline environment=prod job_id=4c226e22 source=Hadoop status=FAILED error_type="com.microsoft.sqlserver.jdbc.SQLServerException" message="Job 'job_name_1' failed: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed"
com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user 'etl_user'
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:500)
	at com.yourcompany.db.DBConnector.getConnection(DBConnector.java:80)
	... 9 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException (for JobID: 4c226e22)
2025-06-12T11:31:32.201+05:30 host=ip-149-119-230-18 service=job_executor app=data_pipeline environment=prod job_id=29ba9a1c source=Spark status=SUCCEEDED message="Spark job 'job_name_62' completed successfully. Results saved."
2025-06-12T11:31:39.201+05:30 host=ip-153-203-200-228 service=job_executor app=data_pipeline environment=prod job_id=9a0d3de3 source=Airflow status=FAILED error_type="java.io.FileNotFoundException" message="Job 'dag_pipeline_21' failed: java.io.FileNotFoundException"
java.io.FileNotFoundException: /path/to/output/file (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	... 6 more
Caused by: java.io.FileNotFoundException (for JobID: 9a0d3de3)
2025-06-12T11:31:46.201+05:30 host=ip-153-132-35-127 service=job_executor app=data_pipeline environment=prod job_id=c9140e9b source=Hadoop status=PENDING message="MapReduce job 'job_name_179' submitted to YARN queue."
2025-06-12T11:32:17.201+05:30 host=ip-20-151-112-191 service=job_executor app=data_pipeline environment=prod job_id=ddc07ab0 source=Glue status=FAILED error_type="org.apache.hadoop.mapreduce.lib.input.FileInputFormat" message="Job 'job_name_38' failed: org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist"
org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist: file:/data/missing.csv
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:275)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJob(JobSubmitter.java:145)
	... 10 more
Caused by: org.apache.hadoop.mapreduce.lib.input.FileInputFormat (for JobID: ddc07ab0)
2025-06-12T11:32:21.201+05:30 host=ip-60-84-140-168 service=job_executor app=data_pipeline environment=prod job_id=09fbace9 source=Spark status=RUNNING message="Spark job 'job_name_107' started. Processing partition 100 of 136."
2025-06-12T11:32:22.201+05:30 host=ip-18-7-164-181 service=job_executor app=data_pipeline environment=prod job_id=0f425668 source=Spark status=RUNNING message="Spark job 'job_name_256' started. Processing partition 11 of 170."
2025-06-12T11:32:27.201+05:30 host=ip-171-215-37-228 service=job_executor app=data_pipeline environment=prod job_id=38e5930e source=Spark status=FAILED error_type="org.apache.kafka.common.errors.TimeoutException" message="Job 'job_name_182' failed: org.apache.kafka.common.errors.TimeoutException"
org.apache.kafka.common.errors.TimeoutException: Topic partition assignment timeout after 60000 ms
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:350)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1245)
	... 6 more
Caused by: org.apache.kafka.common.errors.TimeoutException (for JobID: 38e5930e)
2025-06-12T11:32:40.201+05:30 host=ip-143-59-73-75 service=job_executor app=data_pipeline environment=prod job_id=e1bfa441 source=Airflow status=PENDING message="DAG 'dag_pipeline_28' task 'cleanup_temp' is pending execution."
2025-06-12T11:32:41.201+05:30 host=ip-123-39-126-25 service=job_executor app=data_pipeline environment=prod job_id=3b184a13 source=Glue status=RUNNING message="Glue ETL job 'job_name_73' started execution."
2025-06-12T11:33:38.201+05:30 host=ip-38-107-140-158 service=job_executor app=data_pipeline environment=prod job_id=a3a48201 source=Airflow status=PENDING message="DAG 'dag_pipeline_48' task 'cleanup_temp' is pending execution."
2025-06-12T11:33:57.201+05:30 host=ip-83-111-39-65 service=job_executor app=data_pipeline environment=prod job_id=8399f439 source=Airflow status=RUNNING message="DAG 'dag_pipeline_21' task 'extract_data' is now running."
2025-06-12T11:34:37.201+05:30 host=ip-39-195-179-73 service=job_executor app=data_pipeline environment=prod job_id=09fbace9 source=Spark status=FAILED error_type="ZeroDivisionError" message="Job 'job_name_107' failed: ZeroDivisionError: division by zero"
ZeroDivisionError: division by zero
	at calculate.py:87
	at main.py:54
	... 2 more
Caused by: ZeroDivisionError (for JobID: 09fbace9)
2025-06-12T11:34:37.201+05:30 host=ip-154-212-156-2 service=job_executor app=data_pipeline environment=prod job_id=386bf406 source=Glue status=RUNNING message="Glue ETL job 'job_name_269' started execution."
2025-06-12T11:35:22.201+05:30 host=ip-66-106-76-22 service=job_executor app=data_pipeline environment=prod job_id=a5e394f4 source=Spark status=PENDING message="Spark job 'job_name_151' submitted to cluster, waiting for resources."
2025-06-12T11:35:47.201+05:30 host=ip-29-95-147-174 service=job_executor app=data_pipeline environment=prod job_id=b3b1853a source=Hadoop status=RUNNING message="MapReduce job 'job_name_100' running. Map phase 81% complete."
2025-06-12T11:35:59.201+05:30 host=ip-121-114-53-217 service=job_executor app=data_pipeline environment=prod job_id=e5320976 source=Glue status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_43' failed: com.amazonaws.services.glue.GlueException: Entity not found"
com.amazonaws.services.glue.GlueException: Entity not found: database 'sales_db'
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:350)
	at com.amazonaws.services.glue.AWSGlueClient.getTable(AWSGlueClient.java:115)
	... 4 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: e5320976)
2025-06-12T11:36:09.201+05:30 host=ip-160-140-254-76 service=job_executor app=data_pipeline environment=prod job_id=ea91f10c source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_32' task 'load_to_dw' completed successfully."
2025-06-12T11:36:27.201+05:30 host=ip-147-254-253-173 service=job_executor app=data_pipeline environment=prod job_id=9d5d9051 source=Spark status=FAILED error_type="org.apache.kafka.common.errors.TimeoutException" message="Job 'job_name_20' failed: org.apache.kafka.common.errors.TimeoutException"
org.apache.kafka.common.errors.TimeoutException: Topic partition assignment timeout after 60000 ms
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:350)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1245)
	... 6 more
Caused by: org.apache.kafka.common.errors.TimeoutException (for JobID: 9d5d9051)
2025-06-12T11:36:34.201+05:30 host=ip-178-103-49-93 service=job_executor app=data_pipeline environment=prod job_id=bd76482e source=Glue status=FAILED error_type="java.io.FileNotFoundException" message="Job 'job_name_103' failed: java.io.FileNotFoundException"
java.io.FileNotFoundException: /path/to/output/file (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	... 6 more
Caused by: java.io.FileNotFoundException (for JobID: bd76482e)
2025-06-12T11:37:35.201+05:30 host=ip-60-53-239-91 service=job_executor app=data_pipeline environment=prod job_id=e1bfa441 source=Airflow status=RUNNING message="DAG 'dag_pipeline_28' task 'transform_data' is now running."
2025-06-12T11:37:43.201+05:30 host=ip-96-185-179-153 service=job_executor app=data_pipeline environment=prod job_id=a3a48201 source=Airflow status=RUNNING message="DAG 'dag_pipeline_48' task 'generate_report' is now running."
2025-06-12T11:38:10.201+05:30 host=ip-170-17-233-77 service=job_executor app=data_pipeline environment=prod job_id=8dfc0787 source=Hadoop status=PENDING message="MapReduce job 'job_name_118' submitted to YARN queue."
2025-06-12T11:38:20.201+05:30 host=ip-19-43-162-20 service=job_executor app=data_pipeline environment=prod job_id=2007b618 source=Glue status=PENDING message="Glue ETL job 'job_name_247' is queued for execution."
2025-06-12T11:38:23.201+05:30 host=ip-133-140-49-68 service=job_executor app=data_pipeline environment=prod job_id=68029452 source=Glue status=PENDING message="Glue ETL job 'job_name_239' is queued for execution."
2025-06-12T11:38:58.201+05:30 host=ip-100-188-5-200 service=job_executor app=data_pipeline environment=prod job_id=ca2f000e source=Airflow status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'dag_pipeline_39' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: ca2f000e)
2025-06-12T11:39:26.201+05:30 host=ip-129-143-58-233 service=job_executor app=data_pipeline environment=prod job_id=2b078091 source=Airflow status=FAILED error_type="IOError" message="Job 'dag_pipeline_49' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: 2b078091)
2025-06-12T11:39:30.201+05:30 host=ip-71-91-75-23 service=job_executor app=data_pipeline environment=prod job_id=ac83b1ed source=Spark status=SUCCEEDED message="Spark job 'job_name_3' completed successfully. Results saved."
2025-06-12T11:40:51.201+05:30 host=ip-29-83-25-199 service=job_executor app=data_pipeline environment=prod job_id=e1bfa441 source=Airflow status=FAILED error_type="java.net.ConnectException" message="Job 'dag_pipeline_28' failed: java.net.ConnectException: Connection refused"
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	... 7 more
Caused by: java.net.ConnectException (for JobID: e1bfa441)
2025-06-12T11:41:21.201+05:30 host=ip-156-179-116-39 service=job_executor app=data_pipeline environment=prod job_id=2fdee99d source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_111' completed successfully. Output written to S3."
2025-06-12T11:42:02.201+05:30 host=ip-72-160-156-102 service=job_executor app=data_pipeline environment=prod job_id=c9140e9b source=Hadoop status=RUNNING message="MapReduce job 'job_name_179' running. Map phase 47% complete."
2025-06-12T11:42:16.201+05:30 host=ip-118-186-57-123 service=job_executor app=data_pipeline environment=prod job_id=8d636da6 source=Glue status=FAILED error_type="java.lang.OutOfMemoryError" message="Job 'job_name_225' failed: java.lang.OutOfMemoryError: Java heap space"
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
	... 15 more
Caused by: java.lang.OutOfMemoryError (for JobID: 8d636da6)
2025-06-12T11:42:18.201+05:30 host=ip-197-227-89-202 service=job_executor app=data_pipeline environment=prod job_id=0f425668 source=Spark status=FAILED error_type="IOError" message="Job 'job_name_256' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: 0f425668)
2025-06-12T11:42:55.201+05:30 host=ip-77-28-141-231 service=job_executor app=data_pipeline environment=prod job_id=b3b1853a source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_100' completed successfully. Output written to HDFS."
2025-06-12T11:43:04.201+05:30 host=ip-162-75-146-189 service=job_executor app=data_pipeline environment=prod job_id=88559ff2 source=Spark status=SUCCEEDED message="Spark job 'job_name_191' completed successfully. Results saved."
2025-06-12T11:43:06.201+05:30 host=ip-130-112-159-117 service=job_executor app=data_pipeline environment=prod job_id=2007b618 source=Glue status=RUNNING message="Glue ETL job 'job_name_247' started execution."
2025-06-12T11:43:34.201+05:30 host=ip-103-238-170-71 service=job_executor app=data_pipeline environment=prod job_id=910e91b5 source=Glue status=PENDING message="Glue ETL job 'job_name_272' is queued for execution."
2025-06-12T11:44:59.201+05:30 host=ip-79-177-162-49 service=job_executor app=data_pipeline environment=prod job_id=c13cfa22 source=Glue status=PENDING message="Glue ETL job 'job_name_36' is queued for execution."
2025-06-12T11:45:24.201+05:30 host=ip-49-137-41-18 service=job_executor app=data_pipeline environment=prod job_id=68029452 source=Glue status=RUNNING message="Glue ETL job 'job_name_239' started execution."
2025-06-12T11:46:38.201+05:30 host=ip-88-114-47-141 service=job_executor app=data_pipeline environment=prod job_id=e2dc8d33 source=Airflow status=PENDING message="DAG 'dag_pipeline_6' task 'load_to_dw' is pending execution."
2025-06-12T11:47:00.201+05:30 host=ip-35-201-6-48 service=job_executor app=data_pipeline environment=prod job_id=237bfde9 source=Spark status=PENDING message="Spark job 'job_name_26' submitted to cluster, waiting for resources."
2025-06-12T11:47:41.201+05:30 host=ip-167-76-208-179 service=job_executor app=data_pipeline environment=prod job_id=00370fba source=Spark status=PENDING message="Spark job 'job_name_69' submitted to cluster, waiting for resources."
2025-06-12T11:47:44.201+05:30 host=ip-164-107-209-255 service=job_executor app=data_pipeline environment=prod job_id=0c9eb7b1 source=Hadoop status=FAILED error_type="org.apache.hadoop.mapreduce.lib.input.FileInputFormat" message="Job 'job_name_270' failed: org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist"
org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist: file:/data/missing.csv
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:275)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJob(JobSubmitter.java:145)
	... 10 more
Caused by: org.apache.hadoop.mapreduce.lib.input.FileInputFormat (for JobID: 0c9eb7b1)
2025-06-12T11:48:18.201+05:30 host=ip-127-28-196-72 service=job_executor app=data_pipeline environment=prod job_id=19591d25 source=Hadoop status=PENDING message="MapReduce job 'job_name_276' submitted to YARN queue."
2025-06-12T11:48:20.201+05:30 host=ip-117-115-62-0 service=job_executor app=data_pipeline environment=prod job_id=da0e825b source=Glue status=PENDING message="Glue ETL job 'job_name_86' is queued for execution."
2025-06-12T11:48:59.201+05:30 host=ip-11-17-65-117 service=job_executor app=data_pipeline environment=prod job_id=5449c628 source=Spark status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_281' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 5449c628)
2025-06-12T11:49:36.201+05:30 host=ip-43-144-52-67 service=job_executor app=data_pipeline environment=prod job_id=ebf1c775 source=Spark status=PENDING message="Spark job 'job_name_228' submitted to cluster, waiting for resources."
2025-06-12T11:49:38.201+05:30 host=ip-68-29-103-141 service=job_executor app=data_pipeline environment=prod job_id=8600a8e8 source=Glue status=PENDING message="Glue ETL job 'job_name_52' is queued for execution."
2025-06-12T11:49:43.201+05:30 host=ip-102-207-95-229 service=job_executor app=data_pipeline environment=prod job_id=db6ec2a8 source=Hadoop status=PENDING message="MapReduce job 'job_name_120' submitted to YARN queue."
2025-06-12T11:49:51.201+05:30 host=ip-151-32-160-85 service=job_executor app=data_pipeline environment=prod job_id=a5e394f4 source=Spark status=RUNNING message="Spark job 'job_name_151' started. Processing partition 15 of 156."
2025-06-12T11:49:57.201+05:30 host=ip-172-81-191-41 service=job_executor app=data_pipeline environment=prod job_id=8dfc0787 source=Hadoop status=RUNNING message="MapReduce job 'job_name_118' running. Map phase 82% complete."
2025-06-12T11:50:44.201+05:30 host=ip-108-203-111-87 service=job_executor app=data_pipeline environment=prod job_id=65366a3d source=Airflow status=PENDING message="DAG 'dag_pipeline_44' task 'generate_report' is pending execution."
2025-06-12T11:50:48.201+05:30 host=ip-134-135-101-98 service=job_executor app=data_pipeline environment=prod job_id=da0e825b source=Glue status=RUNNING message="Glue ETL job 'job_name_86' started execution."
2025-06-12T11:50:54.201+05:30 host=ip-158-159-147-37 service=job_executor app=data_pipeline environment=prod job_id=237bfde9 source=Spark status=RUNNING message="Spark job 'job_name_26' started. Processing partition 7 of 157."
2025-06-12T11:51:16.201+05:30 host=ip-176-127-139-120 service=job_executor app=data_pipeline environment=prod job_id=68029452 source=Glue status=FAILED error_type="AttributeError" message="Job 'job_name_239' failed: AttributeError: 'NoneType' object has no attribute 'split'"
AttributeError: 'NoneType' object has no attribute 'split'
	at preprocess_data.py:33
	at main.py:78
	... 5 more
Caused by: AttributeError (for JobID: 68029452)
2025-06-12T11:51:38.201+05:30 host=ip-16-33-84-117 service=job_executor app=data_pipeline environment=prod job_id=8399f439 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_21' task 'transform_data' completed successfully."
2025-06-12T11:51:50.201+05:30 host=ip-152-5-180-149 service=job_executor app=data_pipeline environment=prod job_id=430aeb25 source=Hadoop status=PENDING message="MapReduce job 'job_name_285' submitted to YARN queue."
2025-06-12T11:52:10.201+05:30 host=ip-35-156-38-68 service=job_executor app=data_pipeline environment=prod job_id=19591d25 source=Hadoop status=RUNNING message="MapReduce job 'job_name_276' running. Map phase 75% complete."
2025-06-12T11:52:41.201+05:30 host=ip-22-205-72-97 service=job_executor app=data_pipeline environment=prod job_id=3b184a13 source=Glue status=FAILED error_type="ZeroDivisionError" message="Job 'job_name_73' failed: ZeroDivisionError: division by zero"
ZeroDivisionError: division by zero
	at calculate.py:87
	at main.py:54
	... 2 more
Caused by: ZeroDivisionError (for JobID: 3b184a13)
2025-06-12T11:53:02.201+05:30 host=ip-134-69-93-11 service=job_executor app=data_pipeline environment=prod job_id=00370fba source=Spark status=RUNNING message="Spark job 'job_name_69' started. Processing partition 62 of 148."
2025-06-12T11:53:19.201+05:30 host=ip-194-0-215-52 service=job_executor app=data_pipeline environment=prod job_id=c9140e9b source=Hadoop status=FAILED error_type="java.lang.NullPointerException" message="Job 'job_name_179' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: c9140e9b)
2025-06-12T11:53:52.201+05:30 host=ip-167-138-117-193 service=job_executor app=data_pipeline environment=prod job_id=119f9176 source=Glue status=PENDING message="Glue ETL job 'job_name_148' is queued for execution."
2025-06-12T11:54:01.201+05:30 host=ip-79-109-163-253 service=job_executor app=data_pipeline environment=prod job_id=f1978980 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_271' completed successfully. Output written to S3."
2025-06-12T11:55:05.201+05:30 host=ip-178-65-153-238 service=job_executor app=data_pipeline environment=prod job_id=65366a3d source=Airflow status=RUNNING message="DAG 'dag_pipeline_44' task 'cleanup_temp' is now running."
2025-06-12T11:56:01.201+05:30 host=ip-187-145-40-125 service=job_executor app=data_pipeline environment=prod job_id=430aeb25 source=Hadoop status=RUNNING message="MapReduce job 'job_name_285' running. Map phase 74% complete."
2025-06-12T11:56:49.201+05:30 host=ip-68-227-19-98 service=job_executor app=data_pipeline environment=prod job_id=f1667aa5 source=Glue status=PENDING message="Glue ETL job 'job_name_235' is queued for execution."
2025-06-12T11:57:08.201+05:30 host=ip-171-25-135-30 service=job_executor app=data_pipeline environment=prod job_id=2007b618 source=Glue status=FAILED error_type="java.lang.NullPointerException" message="Job 'job_name_247' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: 2007b618)
2025-06-12T11:57:08.201+05:30 host=ip-44-57-242-10 service=job_executor app=data_pipeline environment=prod job_id=910e91b5 source=Glue status=RUNNING message="Glue ETL job 'job_name_272' started execution."
2025-06-12T11:57:42.201+05:30 host=ip-68-136-109-221 service=job_executor app=data_pipeline environment=prod job_id=61593d37 source=Hadoop status=PENDING message="MapReduce job 'job_name_227' submitted to YARN queue."
2025-06-12T11:57:47.201+05:30 host=ip-122-92-197-60 service=job_executor app=data_pipeline environment=prod job_id=65366a3d source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_44' task 'load_to_dw' completed successfully."
2025-06-12T11:58:17.201+05:30 host=ip-95-117-135-109 service=job_executor app=data_pipeline environment=prod job_id=8600a8e8 source=Glue status=RUNNING message="Glue ETL job 'job_name_52' started execution."
2025-06-12T11:58:19.201+05:30 host=ip-36-110-122-83 service=job_executor app=data_pipeline environment=prod job_id=8dfc0787 source=Hadoop status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_118' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 8dfc0787)
2025-06-12T11:58:52.201+05:30 host=ip-150-230-155-212 service=job_executor app=data_pipeline environment=prod job_id=e2dc8d33 source=Airflow status=RUNNING message="DAG 'dag_pipeline_6' task 'transform_data' is now running."
2025-06-12T11:59:10.201+05:30 host=ip-195-107-205-105 service=job_executor app=data_pipeline environment=prod job_id=c13cfa22 source=Glue status=RUNNING message="Glue ETL job 'job_name_36' started execution."
2025-06-12T11:59:23.201+05:30 host=ip-136-249-148-138 service=job_executor app=data_pipeline environment=prod job_id=a5e394f4 source=Spark status=FAILED error_type="java.lang.NullPointerException" message="Job 'job_name_151' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: a5e394f4)
2025-06-12T11:59:52.201+05:30 host=ip-17-83-216-148 service=job_executor app=data_pipeline environment=prod job_id=280c4c4b source=Spark status=PENDING message="Spark job 'job_name_157' submitted to cluster, waiting for resources."
2025-06-12T12:00:08.201+05:30 host=ip-102-237-34-50 service=job_executor app=data_pipeline environment=prod job_id=b1147865 source=Hadoop status=PENDING message="MapReduce job 'job_name_169' submitted to YARN queue."
2025-06-12T12:00:12.201+05:30 host=ip-77-50-197-34 service=job_executor app=data_pipeline environment=prod job_id=3de1ca53 source=Airflow status=PENDING message="DAG 'dag_pipeline_47' task 'extract_data' is pending execution."
2025-06-12T12:00:29.201+05:30 host=ip-70-232-244-169 service=job_executor app=data_pipeline environment=prod job_id=386bf406 source=Glue status=FAILED error_type="org.apache.hadoop.mapreduce.lib.input.FileInputFormat" message="Job 'job_name_269' failed: org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist"
org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist: file:/data/missing.csv
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:275)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJob(JobSubmitter.java:145)
	... 10 more
Caused by: org.apache.hadoop.mapreduce.lib.input.FileInputFormat (for JobID: 386bf406)
2025-06-12T12:02:54.201+05:30 host=ip-172-244-206-64 service=job_executor app=data_pipeline environment=prod job_id=72f68cfb source=Hadoop status=PENDING message="MapReduce job 'job_name_265' submitted to YARN queue."
2025-06-12T12:03:03.201+05:30 host=ip-182-192-61-41 service=job_executor app=data_pipeline environment=prod job_id=119f9176 source=Glue status=RUNNING message="Glue ETL job 'job_name_148' started execution."
2025-06-12T12:03:04.201+05:30 host=ip-11-208-55-152 service=job_executor app=data_pipeline environment=prod job_id=b1147865 source=Hadoop status=RUNNING message="MapReduce job 'job_name_169' running. Map phase 76% complete."
2025-06-12T12:03:14.201+05:30 host=ip-174-109-24-18 service=job_executor app=data_pipeline environment=prod job_id=db6ec2a8 source=Hadoop status=RUNNING message="MapReduce job 'job_name_120' running. Map phase 83% complete."
2025-06-12T12:03:23.201+05:30 host=ip-27-53-195-230 service=job_executor app=data_pipeline environment=prod job_id=ebf1c775 source=Spark status=RUNNING message="Spark job 'job_name_228' started. Processing partition 72 of 155."
2025-06-12T12:03:32.201+05:30 host=ip-18-191-104-230 service=job_executor app=data_pipeline environment=prod job_id=237bfde9 source=Spark status=FAILED error_type="TypeError" message="Job 'job_name_26' failed: TypeError: cannot concatenate"
TypeError: cannot concatenate 'str' and 'int' objects
	at transform_data.py:56
	at main_workflow.py:112
	... 4 more
Caused by: TypeError (for JobID: 237bfde9)
2025-06-12T12:05:33.201+05:30 host=ip-78-195-231-133 service=job_executor app=data_pipeline environment=prod job_id=8600a8e8 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_52' completed successfully. Output written to S3."
2025-06-12T12:05:41.201+05:30 host=ip-136-247-212-210 service=job_executor app=data_pipeline environment=prod job_id=c7c50ddc source=Glue status=PENDING message="Glue ETL job 'job_name_211' is queued for execution."
2025-06-12T12:07:12.201+05:30 host=ip-121-26-117-225 service=job_executor app=data_pipeline environment=prod job_id=a3a48201 source=Airflow status=FAILED error_type="FileNotFoundError" message="Job 'dag_pipeline_48' failed: FileNotFoundError: [Errno 2] No such file or directory"
FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'
	at open_config.py:45
	at initialize.py:12
	... 3 more
Caused by: FileNotFoundError (for JobID: a3a48201)
2025-06-12T12:07:15.201+05:30 host=ip-92-167-90-22 service=job_executor app=data_pipeline environment=prod job_id=f1667aa5 source=Glue status=RUNNING message="Glue ETL job 'job_name_235' started execution."
2025-06-12T12:07:36.201+05:30 host=ip-128-171-246-30 service=job_executor app=data_pipeline environment=prod job_id=0ad0c4b7 source=Hadoop status=PENDING message="MapReduce job 'job_name_135' submitted to YARN queue."
2025-06-12T12:08:01.201+05:30 host=ip-35-42-33-246 service=job_executor app=data_pipeline environment=prod job_id=8b8d9d34 source=Airflow status=PENDING message="DAG 'dag_pipeline_41' task 'generate_report' is pending execution."
2025-06-12T12:09:38.201+05:30 host=ip-199-189-80-146 service=job_executor app=data_pipeline environment=prod job_id=0318db68 source=Airflow status=PENDING message="DAG 'dag_pipeline_38' task 'extract_data' is pending execution."
2025-06-12T12:09:53.201+05:30 host=ip-125-16-44-12 service=job_executor app=data_pipeline environment=prod job_id=6a4df293 source=Airflow status=PENDING message="DAG 'dag_pipeline_40' task 'extract_data' is pending execution."
2025-06-12T12:10:07.201+05:30 host=ip-13-23-213-201 service=job_executor app=data_pipeline environment=prod job_id=d3a9e616 source=Spark status=PENDING message="Spark job 'job_name_283' submitted to cluster, waiting for resources."
2025-06-12T12:10:16.201+05:30 host=ip-97-243-134-35 service=job_executor app=data_pipeline environment=prod job_id=19591d25 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_276' completed successfully. Output written to HDFS."
2025-06-12T12:10:32.201+05:30 host=ip-142-7-66-143 service=job_executor app=data_pipeline environment=prod job_id=21fb1e31 source=Glue status=PENDING message="Glue ETL job 'job_name_222' is queued for execution."
2025-06-12T12:10:34.201+05:30 host=ip-68-75-113-115 service=job_executor app=data_pipeline environment=prod job_id=280c4c4b source=Spark status=RUNNING message="Spark job 'job_name_157' started. Processing partition 35 of 182."
2025-06-12T12:10:52.201+05:30 host=ip-49-180-161-251 service=job_executor app=data_pipeline environment=prod job_id=93105df9 source=Hadoop status=PENDING message="MapReduce job 'job_name_236' submitted to YARN queue."
2025-06-12T12:10:55.201+05:30 host=ip-21-129-179-165 service=job_executor app=data_pipeline environment=prod job_id=6a4df293 source=Airflow status=RUNNING message="DAG 'dag_pipeline_40' task 'generate_report' is now running."
2025-06-12T12:11:37.201+05:30 host=ip-31-191-58-214 service=job_executor app=data_pipeline environment=prod job_id=da0e825b source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_86' completed successfully. Output written to S3."
2025-06-12T12:11:56.201+05:30 host=ip-25-254-61-44 service=job_executor app=data_pipeline environment=prod job_id=a813256c source=Glue status=PENDING message="Glue ETL job 'job_name_68' is queued for execution."
2025-06-12T12:12:04.201+05:30 host=ip-130-27-78-152 service=job_executor app=data_pipeline environment=prod job_id=3de1ca53 source=Airflow status=RUNNING message="DAG 'dag_pipeline_47' task 'extract_data' is now running."
2025-06-12T12:12:26.201+05:30 host=ip-129-92-133-174 service=job_executor app=data_pipeline environment=prod job_id=61593d37 source=Hadoop status=RUNNING message="MapReduce job 'job_name_227' running. Map phase 14% complete."
2025-06-12T12:12:36.201+05:30 host=ip-181-190-90-112 service=job_executor app=data_pipeline environment=prod job_id=21fb1e31 source=Glue status=RUNNING message="Glue ETL job 'job_name_222' started execution."
2025-06-12T12:12:49.201+05:30 host=ip-157-54-72-241 service=job_executor app=data_pipeline environment=prod job_id=430aeb25 source=Hadoop status=FAILED error_type="org.apache.hadoop.mapreduce.lib.input.FileInputFormat" message="Job 'job_name_285' failed: org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist"
org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist: file:/data/missing.csv
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:275)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJob(JobSubmitter.java:145)
	... 10 more
Caused by: org.apache.hadoop.mapreduce.lib.input.FileInputFormat (for JobID: 430aeb25)
2025-06-12T12:13:20.201+05:30 host=ip-53-141-194-199 service=job_executor app=data_pipeline environment=prod job_id=93105df9 source=Hadoop status=RUNNING message="MapReduce job 'job_name_236' running. Map phase 85% complete."
2025-06-12T12:13:29.201+05:30 host=ip-119-197-15-15 service=job_executor app=data_pipeline environment=prod job_id=ebf1c775 source=Spark status=SUCCEEDED message="Spark job 'job_name_228' completed successfully. Results saved."
2025-06-12T12:13:57.201+05:30 host=ip-156-189-213-108 service=job_executor app=data_pipeline environment=prod job_id=0318db68 source=Airflow status=RUNNING message="DAG 'dag_pipeline_38' task 'load_to_dw' is now running."
2025-06-12T12:13:57.201+05:30 host=ip-21-116-211-83 service=job_executor app=data_pipeline environment=prod job_id=8b095deb source=Airflow status=PENDING message="DAG 'dag_pipeline_9' task 'generate_report' is pending execution."
2025-06-12T12:14:00.201+05:30 host=ip-163-46-103-190 service=job_executor app=data_pipeline environment=prod job_id=72f68cfb source=Hadoop status=RUNNING message="MapReduce job 'job_name_265' running. Map phase 87% complete."
2025-06-12T12:15:11.201+05:30 host=ip-181-205-83-225 service=job_executor app=data_pipeline environment=prod job_id=72f68cfb source=Hadoop status=FAILED error_type="com.microsoft.sqlserver.jdbc.SQLServerException" message="Job 'job_name_265' failed: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed"
com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user 'etl_user'
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:500)
	at com.yourcompany.db.DBConnector.getConnection(DBConnector.java:80)
	... 9 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException (for JobID: 72f68cfb)
2025-06-12T12:15:33.201+05:30 host=ip-71-189-237-154 service=job_executor app=data_pipeline environment=prod job_id=1fbf8c23 source=Hadoop status=PENDING message="MapReduce job 'job_name_261' submitted to YARN queue."
2025-06-12T12:16:35.201+05:30 host=ip-119-210-31-92 service=job_executor app=data_pipeline environment=prod job_id=c7c50ddc source=Glue status=RUNNING message="Glue ETL job 'job_name_211' started execution."
2025-06-12T12:17:24.201+05:30 host=ip-106-27-171-180 service=job_executor app=data_pipeline environment=prod job_id=8b8d9d34 source=Airflow status=RUNNING message="DAG 'dag_pipeline_41' task 'generate_report' is now running."
2025-06-12T12:17:25.201+05:30 host=ip-143-169-118-0 service=job_executor app=data_pipeline environment=prod job_id=93105df9 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_236' completed successfully. Output written to HDFS."
2025-06-12T12:17:43.201+05:30 host=ip-75-231-114-95 service=job_executor app=data_pipeline environment=prod job_id=0ad0c4b7 source=Hadoop status=RUNNING message="MapReduce job 'job_name_135' running. Map phase 52% complete."
2025-06-12T12:18:24.201+05:30 host=ip-191-131-63-34 service=job_executor app=data_pipeline environment=prod job_id=3137035e source=Hadoop status=PENDING message="MapReduce job 'job_name_83' submitted to YARN queue."
2025-06-12T12:18:47.201+05:30 host=ip-183-96-216-114 service=job_executor app=data_pipeline environment=prod job_id=6a4df293 source=Airflow status=FAILED error_type="org.apache.hadoop.fs.FileNotFoundException" message="Job 'dag_pipeline_40' failed: org.apache.hadoop.fs.FileNotFoundException"
org.apache.hadoop.fs.FileNotFoundException: File /user/hadoop/input.txt does not exist
	at org.apache.hadoop.fs.LocalFileSystem.open(LocalFileSystem.java:123)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:67)
	... 8 more
Caused by: org.apache.hadoop.fs.FileNotFoundException (for JobID: 6a4df293)
2025-06-12T12:19:29.201+05:30 host=ip-129-141-60-197 service=job_executor app=data_pipeline environment=prod job_id=d3a9e616 source=Spark status=RUNNING message="Spark job 'job_name_283' started. Processing partition 69 of 148."
2025-06-12T12:19:44.201+05:30 host=ip-189-161-185-99 service=job_executor app=data_pipeline environment=prod job_id=9c2ffa63 source=Glue status=PENDING message="Glue ETL job 'job_name_22' is queued for execution."
2025-06-12T12:20:06.201+05:30 host=ip-31-28-56-109 service=job_executor app=data_pipeline environment=prod job_id=0d2b1249 source=Airflow status=PENDING message="DAG 'dag_pipeline_41' task 'extract_data' is pending execution."
2025-06-12T12:20:26.201+05:30 host=ip-162-174-68-60 service=job_executor app=data_pipeline environment=prod job_id=910e91b5 source=Glue status=FAILED error_type="java.net.ConnectException" message="Job 'job_name_272' failed: java.net.ConnectException: Connection refused"
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	... 7 more
Caused by: java.net.ConnectException (for JobID: 910e91b5)
2025-06-12T12:20:33.201+05:30 host=ip-33-144-225-81 service=job_executor app=data_pipeline environment=prod job_id=00370fba source=Spark status=FAILED error_type="IOError" message="Job 'job_name_69' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: 00370fba)
2025-06-12T12:21:16.201+05:30 host=ip-42-115-214-234 service=job_executor app=data_pipeline environment=prod job_id=560decf0 source=Airflow status=PENDING message="DAG 'dag_pipeline_9' task 'extract_data' is pending execution."
2025-06-12T12:21:39.201+05:30 host=ip-76-158-1-113 service=job_executor app=data_pipeline environment=prod job_id=0ace5612 source=Hadoop status=PENDING message="MapReduce job 'job_name_177' submitted to YARN queue."
2025-06-12T12:21:44.201+05:30 host=ip-34-213-18-236 service=job_executor app=data_pipeline environment=prod job_id=c13cfa22 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_36' completed successfully. Output written to S3."
2025-06-12T12:22:11.201+05:30 host=ip-102-12-137-231 service=job_executor app=data_pipeline environment=prod job_id=1fbf8c23 source=Hadoop status=RUNNING message="MapReduce job 'job_name_261' running. Map phase 84% complete."
2025-06-12T12:22:12.201+05:30 host=ip-129-44-201-146 service=job_executor app=data_pipeline environment=prod job_id=0d2b1249 source=Airflow status=RUNNING message="DAG 'dag_pipeline_41' task 'transform_data' is now running."
2025-06-12T12:22:37.201+05:30 host=ip-41-74-34-176 service=job_executor app=data_pipeline environment=prod job_id=d75ac79b source=Glue status=PENDING message="Glue ETL job 'job_name_207' is queued for execution."
2025-06-12T12:22:55.201+05:30 host=ip-149-148-143-185 service=job_executor app=data_pipeline environment=prod job_id=7b73d5ae source=Airflow status=PENDING message="DAG 'dag_pipeline_17' task 'generate_report' is pending execution."
2025-06-12T12:22:56.201+05:30 host=ip-129-221-220-6 service=job_executor app=data_pipeline environment=prod job_id=119f9176 source=Glue status=FAILED error_type="org.apache.spark.SparkException" message="Job 'job_name_148' failed: org.apache.spark.SparkException"
org.apache.spark.SparkException: Job aborted due to stage failure
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.Task.run(Task.scala:120)
	at org.apache.spark.executor.Executor.$anonfun$runJob$4(Executor.scala:180)
	... 5 more
Caused by: org.apache.spark.SparkException (for JobID: 119f9176)
2025-06-12T12:23:01.201+05:30 host=ip-193-10-233-143 service=job_executor app=data_pipeline environment=prod job_id=280c4c4b source=Spark status=SUCCEEDED message="Spark job 'job_name_157' completed successfully. Results saved."
2025-06-12T12:23:14.201+05:30 host=ip-85-176-94-202 service=job_executor app=data_pipeline environment=prod job_id=aa563819 source=Airflow status=PENDING message="DAG 'dag_pipeline_29' task 'load_to_dw' is pending execution."
2025-06-12T12:24:22.201+05:30 host=ip-190-10-191-120 service=job_executor app=data_pipeline environment=prod job_id=8b095deb source=Airflow status=RUNNING message="DAG 'dag_pipeline_9' task 'transform_data' is now running."
2025-06-12T12:24:27.201+05:30 host=ip-174-232-126-92 service=job_executor app=data_pipeline environment=prod job_id=0318db68 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_38' task 'extract_data' completed successfully."
2025-06-12T12:24:51.201+05:30 host=ip-186-53-53-55 service=job_executor app=data_pipeline environment=prod job_id=13d0efef source=Spark status=PENDING message="Spark job 'job_name_184' submitted to cluster, waiting for resources."
2025-06-12T12:24:58.201+05:30 host=ip-157-81-32-160 service=job_executor app=data_pipeline environment=prod job_id=df184484 source=Hadoop status=PENDING message="MapReduce job 'job_name_12' submitted to YARN queue."
2025-06-12T12:26:38.201+05:30 host=ip-163-47-96-59 service=job_executor app=data_pipeline environment=prod job_id=61593d37 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_227' completed successfully. Output written to HDFS."
2025-06-12T12:26:47.201+05:30 host=ip-101-100-82-234 service=job_executor app=data_pipeline environment=prod job_id=a813256c source=Glue status=RUNNING message="Glue ETL job 'job_name_68' started execution."
2025-06-12T12:26:53.201+05:30 host=ip-108-143-183-187 service=job_executor app=data_pipeline environment=prod job_id=f1667aa5 source=Glue status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_235' failed: com.amazonaws.services.glue.GlueException: Entity not found"
com.amazonaws.services.glue.GlueException: Entity not found: database 'sales_db'
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:350)
	at com.amazonaws.services.glue.AWSGlueClient.getTable(AWSGlueClient.java:115)
	... 4 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: f1667aa5)
2025-06-12T12:27:10.201+05:30 host=ip-148-151-114-4 service=job_executor app=data_pipeline environment=prod job_id=7b73d5ae source=Airflow status=RUNNING message="DAG 'dag_pipeline_17' task 'cleanup_temp' is now running."
2025-06-12T12:27:31.201+05:30 host=ip-92-79-74-77 service=job_executor app=data_pipeline environment=prod job_id=560decf0 source=Airflow status=RUNNING message="DAG 'dag_pipeline_9' task 'generate_report' is now running."
2025-06-12T12:28:19.201+05:30 host=ip-68-213-57-217 service=job_executor app=data_pipeline environment=prod job_id=9c2ffa63 source=Glue status=RUNNING message="Glue ETL job 'job_name_22' started execution."
2025-06-12T12:28:19.201+05:30 host=ip-194-214-18-19 service=job_executor app=data_pipeline environment=prod job_id=e2dc8d33 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_6' task 'extract_data' completed successfully."
2025-06-12T12:28:39.201+05:30 host=ip-73-103-253-74 service=job_executor app=data_pipeline environment=prod job_id=b1147865 source=Hadoop status=FAILED error_type="com.microsoft.sqlserver.jdbc.SQLServerException" message="Job 'job_name_169' failed: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed"
com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user 'etl_user'
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:500)
	at com.yourcompany.db.DBConnector.getConnection(DBConnector.java:80)
	... 9 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException (for JobID: b1147865)
2025-06-12T12:28:46.201+05:30 host=ip-129-100-3-105 service=job_executor app=data_pipeline environment=prod job_id=9c2ffa63 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_22' completed successfully. Output written to S3."
2025-06-12T12:28:55.201+05:30 host=ip-193-178-83-246 service=job_executor app=data_pipeline environment=prod job_id=d75ac79b source=Glue status=RUNNING message="Glue ETL job 'job_name_207' started execution."
2025-06-12T12:28:57.201+05:30 host=ip-86-126-94-39 service=job_executor app=data_pipeline environment=prod job_id=3137035e source=Hadoop status=RUNNING message="MapReduce job 'job_name_83' running. Map phase 64% complete."
2025-06-12T12:29:20.201+05:30 host=ip-142-213-172-53 service=job_executor app=data_pipeline environment=prod job_id=899d4a13 source=Airflow status=PENDING message="DAG 'dag_pipeline_6' task 'generate_report' is pending execution."
2025-06-12T12:29:40.201+05:30 host=ip-159-237-82-190 service=job_executor app=data_pipeline environment=prod job_id=18b599fc source=Hadoop status=PENDING message="MapReduce job 'job_name_160' submitted to YARN queue."
2025-06-12T12:29:54.201+05:30 host=ip-79-54-219-232 service=job_executor app=data_pipeline environment=prod job_id=3de1ca53 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_47' task 'cleanup_temp' completed successfully."
2025-06-12T12:30:05.201+05:30 host=ip-10-40-147-214 service=job_executor app=data_pipeline environment=prod job_id=c9838cd2 source=Airflow status=PENDING message="DAG 'dag_pipeline_13' task 'generate_report' is pending execution."
2025-06-12T12:30:14.201+05:30 host=ip-155-36-102-218 service=job_executor app=data_pipeline environment=prod job_id=c7c50ddc source=Glue status=FAILED error_type="ZeroDivisionError" message="Job 'job_name_211' failed: ZeroDivisionError: division by zero"
ZeroDivisionError: division by zero
	at calculate.py:87
	at main.py:54
	... 2 more
Caused by: ZeroDivisionError (for JobID: c7c50ddc)
2025-06-12T12:30:30.201+05:30 host=ip-47-161-225-65 service=job_executor app=data_pipeline environment=prod job_id=2e4efdd8 source=Hadoop status=PENDING message="MapReduce job 'job_name_47' submitted to YARN queue."
2025-06-12T12:30:33.201+05:30 host=ip-196-249-72-203 service=job_executor app=data_pipeline environment=prod job_id=02c0acc2 source=Glue status=PENDING message="Glue ETL job 'job_name_172' is queued for execution."
2025-06-12T12:31:08.201+05:30 host=ip-130-235-105-54 service=job_executor app=data_pipeline environment=prod job_id=0d2b1249 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_41' task 'generate_report' completed successfully."
2025-06-12T12:31:10.201+05:30 host=ip-56-247-42-191 service=job_executor app=data_pipeline environment=prod job_id=cf6a864c source=Airflow status=PENDING message="DAG 'dag_pipeline_37' task 'load_to_dw' is pending execution."
2025-06-12T12:31:50.201+05:30 host=ip-16-9-33-97 service=job_executor app=data_pipeline environment=prod job_id=0ace5612 source=Hadoop status=RUNNING message="MapReduce job 'job_name_177' running. Map phase 59% complete."
2025-06-12T12:31:52.201+05:30 host=ip-43-52-170-89 service=job_executor app=data_pipeline environment=prod job_id=0ad0c4b7 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_135' completed successfully. Output written to HDFS."
2025-06-12T12:33:10.201+05:30 host=ip-158-57-238-12 service=job_executor app=data_pipeline environment=prod job_id=db6ec2a8 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_120' completed successfully. Output written to HDFS."
2025-06-12T12:33:52.201+05:30 host=ip-141-177-88-140 service=job_executor app=data_pipeline environment=prod job_id=a813256c source=Glue status=FAILED error_type="IOError" message="Job 'job_name_68' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: a813256c)
2025-06-12T12:34:12.201+05:30 host=ip-184-110-169-124 service=job_executor app=data_pipeline environment=prod job_id=5c0be5bb source=Hadoop status=PENDING message="MapReduce job 'job_name_57' submitted to YARN queue."
2025-06-12T12:34:27.201+05:30 host=ip-123-81-120-21 service=job_executor app=data_pipeline environment=prod job_id=c232bd65 source=Glue status=PENDING message="Glue ETL job 'job_name_32' is queued for execution."
2025-06-12T12:34:58.201+05:30 host=ip-30-106-116-226 service=job_executor app=data_pipeline environment=prod job_id=6fcc100e source=Hadoop status=PENDING message="MapReduce job 'job_name_74' submitted to YARN queue."
2025-06-12T12:35:11.201+05:30 host=ip-194-104-137-90 service=job_executor app=data_pipeline environment=prod job_id=aa563819 source=Airflow status=RUNNING message="DAG 'dag_pipeline_29' task 'transform_data' is now running."
2025-06-12T12:35:40.201+05:30 host=ip-158-252-233-173 service=job_executor app=data_pipeline environment=prod job_id=c9838cd2 source=Airflow status=RUNNING message="DAG 'dag_pipeline_13' task 'generate_report' is now running."
2025-06-12T12:35:48.201+05:30 host=ip-54-140-25-94 service=job_executor app=data_pipeline environment=prod job_id=8b8d9d34 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_41' task 'extract_data' completed successfully."
2025-06-12T12:36:35.201+05:30 host=ip-160-8-243-51 service=job_executor app=data_pipeline environment=prod job_id=d3a9e616 source=Spark status=SUCCEEDED message="Spark job 'job_name_283' completed successfully. Results saved."
2025-06-12T12:37:16.201+05:30 host=ip-28-119-27-149 service=job_executor app=data_pipeline environment=prod job_id=d08e0588 source=Hadoop status=PENDING message="MapReduce job 'job_name_170' submitted to YARN queue."
2025-06-12T12:37:41.201+05:30 host=ip-52-39-119-189 service=job_executor app=data_pipeline environment=prod job_id=2e4efdd8 source=Hadoop status=RUNNING message="MapReduce job 'job_name_47' running. Map phase 44% complete."
2025-06-12T12:38:27.201+05:30 host=ip-152-166-188-159 service=job_executor app=data_pipeline environment=prod job_id=13d0efef source=Spark status=RUNNING message="Spark job 'job_name_184' started. Processing partition 52 of 116."
2025-06-12T12:38:29.201+05:30 host=ip-24-216-150-149 service=job_executor app=data_pipeline environment=prod job_id=7b73d5ae source=Airflow status=FAILED error_type="java.io.FileNotFoundException" message="Job 'dag_pipeline_17' failed: java.io.FileNotFoundException"
java.io.FileNotFoundException: /path/to/output/file (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	... 6 more
Caused by: java.io.FileNotFoundException (for JobID: 7b73d5ae)
2025-06-12T12:38:47.201+05:30 host=ip-83-114-214-59 service=job_executor app=data_pipeline environment=prod job_id=4aad2221 source=Hadoop status=PENDING message="MapReduce job 'job_name_229' submitted to YARN queue."
2025-06-12T12:39:07.201+05:30 host=ip-35-179-203-233 service=job_executor app=data_pipeline environment=prod job_id=18b599fc source=Hadoop status=RUNNING message="MapReduce job 'job_name_160' running. Map phase 45% complete."
2025-06-12T12:39:13.201+05:30 host=ip-42-14-20-207 service=job_executor app=data_pipeline environment=prod job_id=df184484 source=Hadoop status=RUNNING message="MapReduce job 'job_name_12' running. Map phase 45% complete."
2025-06-12T12:39:40.201+05:30 host=ip-160-72-137-99 service=job_executor app=data_pipeline environment=prod job_id=d08e0588 source=Hadoop status=RUNNING message="MapReduce job 'job_name_170' running. Map phase 75% complete."
2025-06-12T12:39:41.201+05:30 host=ip-168-243-19-167 service=job_executor app=data_pipeline environment=prod job_id=fcd3962d source=Glue status=PENDING message="Glue ETL job 'job_name_278' is queued for execution."
2025-06-12T12:40:32.201+05:30 host=ip-44-114-127-18 service=job_executor app=data_pipeline environment=prod job_id=ce97285c source=Glue status=PENDING message="Glue ETL job 'job_name_263' is queued for execution."
2025-06-12T12:41:11.201+05:30 host=ip-135-52-188-151 service=job_executor app=data_pipeline environment=prod job_id=c9838cd2 source=Airflow status=FAILED error_type="FileNotFoundError" message="Job 'dag_pipeline_13' failed: FileNotFoundError: [Errno 2] No such file or directory"
FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'
	at open_config.py:45
	at initialize.py:12
	... 3 more
Caused by: FileNotFoundError (for JobID: c9838cd2)
2025-06-12T12:41:21.201+05:30 host=ip-145-183-196-11 service=job_executor app=data_pipeline environment=prod job_id=899d4a13 source=Airflow status=RUNNING message="DAG 'dag_pipeline_6' task 'extract_data' is now running."
2025-06-12T12:41:28.201+05:30 host=ip-34-100-15-100 service=job_executor app=data_pipeline environment=prod job_id=21fb1e31 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_222' completed successfully. Output written to S3."
2025-06-12T12:41:33.201+05:30 host=ip-189-16-153-83 service=job_executor app=data_pipeline environment=prod job_id=ce97285c source=Glue status=RUNNING message="Glue ETL job 'job_name_263' started execution."
2025-06-12T12:41:52.201+05:30 host=ip-194-169-18-98 service=job_executor app=data_pipeline environment=prod job_id=cf6a864c source=Airflow status=RUNNING message="DAG 'dag_pipeline_37' task 'cleanup_temp' is now running."
2025-06-12T12:42:52.201+05:30 host=ip-196-60-102-14 service=job_executor app=data_pipeline environment=prod job_id=fcd3962d source=Glue status=RUNNING message="Glue ETL job 'job_name_278' started execution."
2025-06-12T12:43:04.201+05:30 host=ip-51-118-82-103 service=job_executor app=data_pipeline environment=prod job_id=5c0be5bb source=Hadoop status=RUNNING message="MapReduce job 'job_name_57' running. Map phase 56% complete."
2025-06-12T12:43:50.201+05:30 host=ip-191-215-159-118 service=job_executor app=data_pipeline environment=prod job_id=4aad2221 source=Hadoop status=RUNNING message="MapReduce job 'job_name_229' running. Map phase 41% complete."
2025-06-12T12:44:49.201+05:30 host=ip-22-208-96-58 service=job_executor app=data_pipeline environment=prod job_id=0ace5612 source=Hadoop status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_177' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 0ace5612)
2025-06-12T12:45:20.201+05:30 host=ip-42-163-141-116 service=job_executor app=data_pipeline environment=prod job_id=02c0acc2 source=Glue status=RUNNING message="Glue ETL job 'job_name_172' started execution."
2025-06-12T12:45:40.201+05:30 host=ip-115-47-232-90 service=job_executor app=data_pipeline environment=prod job_id=7f62a498 source=Hadoop status=PENDING message="MapReduce job 'job_name_186' submitted to YARN queue."
2025-06-12T12:46:14.201+05:30 host=ip-90-124-39-106 service=job_executor app=data_pipeline environment=prod job_id=18b599fc source=Hadoop status=FAILED error_type="org.apache.spark.SparkException" message="Job 'job_name_160' failed: org.apache.spark.SparkException"
org.apache.spark.SparkException: Job aborted due to stage failure
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.Task.run(Task.scala:120)
	at org.apache.spark.executor.Executor.$anonfun$runJob$4(Executor.scala:180)
	... 5 more
Caused by: org.apache.spark.SparkException (for JobID: 18b599fc)
2025-06-12T12:46:14.201+05:30 host=ip-79-170-188-68 service=job_executor app=data_pipeline environment=prod job_id=aa563819 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_29' task 'cleanup_temp' completed successfully."
2025-06-12T12:47:11.201+05:30 host=ip-43-210-45-164 service=job_executor app=data_pipeline environment=prod job_id=ddd8e160 source=Spark status=PENDING message="Spark job 'job_name_56' submitted to cluster, waiting for resources."
2025-06-12T12:47:36.201+05:30 host=ip-16-32-172-196 service=job_executor app=data_pipeline environment=prod job_id=df184484 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_12' completed successfully. Output written to HDFS."
2025-06-12T12:48:16.201+05:30 host=ip-158-243-0-107 service=job_executor app=data_pipeline environment=prod job_id=c232bd65 source=Glue status=RUNNING message="Glue ETL job 'job_name_32' started execution."
2025-06-12T12:48:27.201+05:30 host=ip-81-197-28-55 service=job_executor app=data_pipeline environment=prod job_id=e0d0db15 source=Spark status=PENDING message="Spark job 'job_name_125' submitted to cluster, waiting for resources."
2025-06-12T12:48:37.201+05:30 host=ip-74-104-69-28 service=job_executor app=data_pipeline environment=prod job_id=33f7f3ca source=Spark status=PENDING message="Spark job 'job_name_234' submitted to cluster, waiting for resources."
2025-06-12T12:48:39.201+05:30 host=ip-79-96-168-170 service=job_executor app=data_pipeline environment=prod job_id=4aad2221 source=Hadoop status=FAILED error_type="org.apache.hadoop.mapreduce.lib.input.FileInputFormat" message="Job 'job_name_229' failed: org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist"
org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist: file:/data/missing.csv
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:275)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJob(JobSubmitter.java:145)
	... 10 more
Caused by: org.apache.hadoop.mapreduce.lib.input.FileInputFormat (for JobID: 4aad2221)
2025-06-12T12:49:06.201+05:30 host=ip-10-248-143-139 service=job_executor app=data_pipeline environment=prod job_id=560decf0 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_9' task 'generate_report' completed successfully."
2025-06-12T12:49:11.201+05:30 host=ip-67-203-213-78 service=job_executor app=data_pipeline environment=prod job_id=6fcc100e source=Hadoop status=RUNNING message="MapReduce job 'job_name_74' running. Map phase 73% complete."
2025-06-12T12:49:26.201+05:30 host=ip-109-231-230-216 service=job_executor app=data_pipeline environment=prod job_id=ddd8e160 source=Spark status=RUNNING message="Spark job 'job_name_56' started. Processing partition 33 of 168."
2025-06-12T12:50:04.201+05:30 host=ip-172-47-112-149 service=job_executor app=data_pipeline environment=prod job_id=1fbf8c23 source=Hadoop status=FAILED error_type="org.apache.hadoop.mapreduce.lib.input.FileInputFormat" message="Job 'job_name_261' failed: org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist"
org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist: file:/data/missing.csv
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:275)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJob(JobSubmitter.java:145)
	... 10 more
Caused by: org.apache.hadoop.mapreduce.lib.input.FileInputFormat (for JobID: 1fbf8c23)
2025-06-12T12:50:23.201+05:30 host=ip-23-9-35-101 service=job_executor app=data_pipeline environment=prod job_id=8b095deb source=Airflow status=FAILED error_type="java.lang.NullPointerException" message="Job 'dag_pipeline_9' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: 8b095deb)
2025-06-12T12:50:28.201+05:30 host=ip-42-40-129-43 service=job_executor app=data_pipeline environment=prod job_id=3137035e source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_83' completed successfully. Output written to HDFS."
2025-06-12T12:50:48.201+05:30 host=ip-59-176-96-74 service=job_executor app=data_pipeline environment=prod job_id=e0d0db15 source=Spark status=RUNNING message="Spark job 'job_name_125' started. Processing partition 89 of 144."
2025-06-12T12:51:21.201+05:30 host=ip-36-126-23-35 service=job_executor app=data_pipeline environment=prod job_id=13d0efef source=Spark status=SUCCEEDED message="Spark job 'job_name_184' completed successfully. Results saved."
2025-06-12T12:51:39.201+05:30 host=ip-74-219-37-64 service=job_executor app=data_pipeline environment=prod job_id=7f62a498 source=Hadoop status=RUNNING message="MapReduce job 'job_name_186' running. Map phase 24% complete."
2025-06-12T12:51:47.201+05:30 host=ip-158-227-124-60 service=job_executor app=data_pipeline environment=prod job_id=2e4efdd8 source=Hadoop status=FAILED error_type="org.apache.hadoop.mapreduce.lib.input.FileInputFormat" message="Job 'job_name_47' failed: org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist"
org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist: file:/data/missing.csv
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:275)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJob(JobSubmitter.java:145)
	... 10 more
Caused by: org.apache.hadoop.mapreduce.lib.input.FileInputFormat (for JobID: 2e4efdd8)
2025-06-12T12:52:41.201+05:30 host=ip-126-152-80-149 service=job_executor app=data_pipeline environment=prod job_id=899d4a13 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_6' task 'load_to_dw' completed successfully."
2025-06-12T12:54:21.201+05:30 host=ip-174-172-228-209 service=job_executor app=data_pipeline environment=prod job_id=d75ac79b source=Glue status=FAILED error_type="org.apache.kafka.common.errors.TimeoutException" message="Job 'job_name_207' failed: org.apache.kafka.common.errors.TimeoutException"
org.apache.kafka.common.errors.TimeoutException: Topic partition assignment timeout after 60000 ms
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:350)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1245)
	... 6 more
Caused by: org.apache.kafka.common.errors.TimeoutException (for JobID: d75ac79b)
2025-06-12T12:55:02.201+05:30 host=ip-104-158-174-202 service=job_executor app=data_pipeline environment=prod job_id=6fcc100e source=Hadoop status=FAILED error_type="TypeError" message="Job 'job_name_74' failed: TypeError: cannot concatenate"
TypeError: cannot concatenate 'str' and 'int' objects
	at transform_data.py:56
	at main_workflow.py:112
	... 4 more
Caused by: TypeError (for JobID: 6fcc100e)
2025-06-12T12:56:10.201+05:30 host=ip-37-250-212-108 service=job_executor app=data_pipeline environment=prod job_id=33f7f3ca source=Spark status=RUNNING message="Spark job 'job_name_234' started. Processing partition 59 of 174."
2025-06-12T12:56:15.201+05:30 host=ip-155-240-78-160 service=job_executor app=data_pipeline environment=prod job_id=bf52ad58 source=Spark status=PENDING message="Spark job 'job_name_230' submitted to cluster, waiting for resources."
2025-06-12T12:56:43.201+05:30 host=ip-164-114-113-157 service=job_executor app=data_pipeline environment=prod job_id=0d614b48 source=Airflow status=PENDING message="DAG 'dag_pipeline_7' task 'cleanup_temp' is pending execution."
2025-06-12T12:57:23.201+05:30 host=ip-48-122-127-196 service=job_executor app=data_pipeline environment=prod job_id=ab25fbf7 source=Spark status=PENDING message="Spark job 'job_name_88' submitted to cluster, waiting for resources."
2025-06-12T12:57:35.201+05:30 host=ip-159-54-28-116 service=job_executor app=data_pipeline environment=prod job_id=ddd8e160 source=Spark status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_56' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: ddd8e160)
2025-06-12T12:58:15.201+05:30 host=ip-149-147-88-79 service=job_executor app=data_pipeline environment=prod job_id=cf6a864c source=Airflow status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'dag_pipeline_37' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: cf6a864c)
2025-06-12T12:58:44.201+05:30 host=ip-175-246-214-7 service=job_executor app=data_pipeline environment=prod job_id=d08e0588 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_170' completed successfully. Output written to HDFS."
2025-06-12T12:58:53.201+05:30 host=ip-106-12-28-254 service=job_executor app=data_pipeline environment=prod job_id=574b2643 source=Glue status=PENDING message="Glue ETL job 'job_name_262' is queued for execution."
2025-06-12T12:59:43.201+05:30 host=ip-104-179-80-158 service=job_executor app=data_pipeline environment=prod job_id=477fe0e2 source=Airflow status=PENDING message="DAG 'dag_pipeline_47' task 'generate_report' is pending execution."
2025-06-12T13:01:01.201+05:30 host=ip-40-20-78-96 service=job_executor app=data_pipeline environment=prod job_id=c232bd65 source=Glue status=FAILED error_type="IOError" message="Job 'job_name_32' failed: IOError: [Errno 13] Permission denied"
IOError: [Errno 13] Permission denied: '/var/log/app.log'
	at log_writer.py:59
	at main.py:20
	... 3 more
Caused by: IOError (for JobID: c232bd65)
2025-06-12T13:01:17.201+05:30 host=ip-119-62-219-180 service=job_executor app=data_pipeline environment=prod job_id=ab25fbf7 source=Spark status=RUNNING message="Spark job 'job_name_88' started. Processing partition 45 of 149."
2025-06-12T13:01:39.201+05:30 host=ip-10-101-56-7 service=job_executor app=data_pipeline environment=prod job_id=a23d2827 source=Hadoop status=PENDING message="MapReduce job 'job_name_200' submitted to YARN queue."
2025-06-12T13:01:40.201+05:30 host=ip-40-241-188-72 service=job_executor app=data_pipeline environment=prod job_id=aaafe1c3 source=Spark status=PENDING message="Spark job 'job_name_194' submitted to cluster, waiting for resources."
2025-06-12T13:02:31.201+05:30 host=ip-116-248-85-3 service=job_executor app=data_pipeline environment=prod job_id=acd5f4ab source=Hadoop status=PENDING message="MapReduce job 'job_name_206' submitted to YARN queue."
2025-06-12T13:02:49.201+05:30 host=ip-152-172-232-4 service=job_executor app=data_pipeline environment=prod job_id=477fe0e2 source=Airflow status=RUNNING message="DAG 'dag_pipeline_47' task 'generate_report' is now running."
2025-06-12T13:02:57.201+05:30 host=ip-122-138-154-32 service=job_executor app=data_pipeline environment=prod job_id=ce97285c source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_263' completed successfully. Output written to S3."
2025-06-12T13:03:41.201+05:30 host=ip-60-81-218-115 service=job_executor app=data_pipeline environment=prod job_id=37f1f1b0 source=Spark status=PENDING message="Spark job 'job_name_77' submitted to cluster, waiting for resources."
2025-06-12T13:04:40.201+05:30 host=ip-55-161-34-241 service=job_executor app=data_pipeline environment=prod job_id=02c0acc2 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_172' completed successfully. Output written to S3."
2025-06-12T13:04:46.201+05:30 host=ip-27-241-114-174 service=job_executor app=data_pipeline environment=prod job_id=73b7de20 source=Spark status=PENDING message="Spark job 'job_name_208' submitted to cluster, waiting for resources."
2025-06-12T13:04:54.201+05:30 host=ip-191-65-208-178 service=job_executor app=data_pipeline environment=prod job_id=ab933e02 source=Hadoop status=PENDING message="MapReduce job 'job_name_133' submitted to YARN queue."
2025-06-12T13:05:20.201+05:30 host=ip-52-61-168-42 service=job_executor app=data_pipeline environment=prod job_id=fb2b382f source=Spark status=PENDING message="Spark job 'job_name_218' submitted to cluster, waiting for resources."
2025-06-12T13:05:49.201+05:30 host=ip-158-228-67-226 service=job_executor app=data_pipeline environment=prod job_id=5c0be5bb source=Hadoop status=FAILED error_type="ValueError" message="Job 'job_name_57' failed: ValueError: invalid literal"
ValueError: invalid literal for int() with base 10: 'abc'
	at pandas_read.py:127
	at data_processing.py:42
	... 3 more
Caused by: ValueError (for JobID: 5c0be5bb)
2025-06-12T13:06:16.201+05:30 host=ip-53-214-138-44 service=job_executor app=data_pipeline environment=prod job_id=00f96fd6 source=Glue status=PENDING message="Glue ETL job 'job_name_161' is queued for execution."
2025-06-12T13:06:23.201+05:30 host=ip-187-6-141-158 service=job_executor app=data_pipeline environment=prod job_id=0d614b48 source=Airflow status=RUNNING message="DAG 'dag_pipeline_7' task 'load_to_dw' is now running."
2025-06-12T13:06:36.201+05:30 host=ip-174-174-201-246 service=job_executor app=data_pipeline environment=prod job_id=3c3ff1b8 source=Glue status=PENDING message="Glue ETL job 'job_name_35' is queued for execution."
2025-06-12T13:07:17.201+05:30 host=ip-194-186-34-168 service=job_executor app=data_pipeline environment=prod job_id=d548dd21 source=Glue status=PENDING message="Glue ETL job 'job_name_8' is queued for execution."
2025-06-12T13:07:23.201+05:30 host=ip-22-197-103-205 service=job_executor app=data_pipeline environment=prod job_id=c6d40170 source=Spark status=PENDING message="Spark job 'job_name_162' submitted to cluster, waiting for resources."
2025-06-12T13:08:03.201+05:30 host=ip-164-206-167-106 service=job_executor app=data_pipeline environment=prod job_id=ab933e02 source=Hadoop status=RUNNING message="MapReduce job 'job_name_133' running. Map phase 81% complete."
2025-06-12T13:08:10.201+05:30 host=ip-155-60-110-244 service=job_executor app=data_pipeline environment=prod job_id=dfecb62d source=Glue status=PENDING message="Glue ETL job 'job_name_82' is queued for execution."
2025-06-12T13:08:23.201+05:30 host=ip-105-1-252-45 service=job_executor app=data_pipeline environment=prod job_id=00f96fd6 source=Glue status=RUNNING message="Glue ETL job 'job_name_161' started execution."
2025-06-12T13:08:48.201+05:30 host=ip-182-153-94-161 service=job_executor app=data_pipeline environment=prod job_id=00f96fd6 source=Glue status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_161' failed: com.amazonaws.services.glue.GlueException: Entity not found"
com.amazonaws.services.glue.GlueException: Entity not found: database 'sales_db'
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:350)
	at com.amazonaws.services.glue.AWSGlueClient.getTable(AWSGlueClient.java:115)
	... 4 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 00f96fd6)
2025-06-12T13:08:49.201+05:30 host=ip-120-50-62-159 service=job_executor app=data_pipeline environment=prod job_id=fcd3962d source=Glue status=FAILED error_type="org.apache.kafka.common.errors.TimeoutException" message="Job 'job_name_278' failed: org.apache.kafka.common.errors.TimeoutException"
org.apache.kafka.common.errors.TimeoutException: Topic partition assignment timeout after 60000 ms
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:350)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1245)
	... 6 more
Caused by: org.apache.kafka.common.errors.TimeoutException (for JobID: fcd3962d)
2025-06-12T13:08:51.201+05:30 host=ip-42-138-227-171 service=job_executor app=data_pipeline environment=prod job_id=aaafe1c3 source=Spark status=RUNNING message="Spark job 'job_name_194' started. Processing partition 80 of 134."
2025-06-12T13:08:54.201+05:30 host=ip-53-11-159-164 service=job_executor app=data_pipeline environment=prod job_id=8ece055c source=Airflow status=PENDING message="DAG 'dag_pipeline_6' task 'generate_report' is pending execution."
2025-06-12T13:08:57.201+05:30 host=ip-70-121-231-114 service=job_executor app=data_pipeline environment=prod job_id=37f1f1b0 source=Spark status=RUNNING message="Spark job 'job_name_77' started. Processing partition 12 of 131."
2025-06-12T13:09:12.201+05:30 host=ip-87-129-159-134 service=job_executor app=data_pipeline environment=prod job_id=bf52ad58 source=Spark status=RUNNING message="Spark job 'job_name_230' started. Processing partition 96 of 172."
2025-06-12T13:10:07.201+05:30 host=ip-184-24-169-107 service=job_executor app=data_pipeline environment=prod job_id=c2f44ec5 source=Airflow status=PENDING message="DAG 'dag_pipeline_17' task 'generate_report' is pending execution."
2025-06-12T13:10:36.201+05:30 host=ip-10-113-215-244 service=job_executor app=data_pipeline environment=prod job_id=af5887b6 source=Hadoop status=PENDING message="MapReduce job 'job_name_260' submitted to YARN queue."
2025-06-12T13:11:06.201+05:30 host=ip-133-42-116-124 service=job_executor app=data_pipeline environment=prod job_id=574b2643 source=Glue status=RUNNING message="Glue ETL job 'job_name_262' started execution."
2025-06-12T13:11:30.201+05:30 host=ip-49-132-46-234 service=job_executor app=data_pipeline environment=prod job_id=0041ec1b source=Spark status=PENDING message="Spark job 'job_name_80' submitted to cluster, waiting for resources."
2025-06-12T13:11:42.201+05:30 host=ip-176-43-65-11 service=job_executor app=data_pipeline environment=prod job_id=3441df49 source=Hadoop status=PENDING message="MapReduce job 'job_name_122' submitted to YARN queue."
2025-06-12T13:12:03.201+05:30 host=ip-19-150-179-212 service=job_executor app=data_pipeline environment=prod job_id=d548dd21 source=Glue status=RUNNING message="Glue ETL job 'job_name_8' started execution."
2025-06-12T13:12:07.201+05:30 host=ip-187-35-228-186 service=job_executor app=data_pipeline environment=prod job_id=ff2e0558 source=Glue status=PENDING message="Glue ETL job 'job_name_44' is queued for execution."
2025-06-12T13:12:17.201+05:30 host=ip-154-177-52-215 service=job_executor app=data_pipeline environment=prod job_id=8ece055c source=Airflow status=RUNNING message="DAG 'dag_pipeline_6' task 'load_to_dw' is now running."
2025-06-12T13:13:04.201+05:30 host=ip-23-89-160-134 service=job_executor app=data_pipeline environment=prod job_id=e0d0db15 source=Spark status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_125' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: e0d0db15)
2025-06-12T13:13:07.201+05:30 host=ip-75-188-122-82 service=job_executor app=data_pipeline environment=prod job_id=7f62a498 source=Hadoop status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_186' failed: com.amazonaws.services.glue.GlueException: Failed to connect"
com.amazonaws.services.glue.GlueException: Failed to connect to Glue metadata store
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:235)
	at com.amazonaws.services.glue.AWSGlueClient.getDatabase(AWSGlueClient.java:97)
	... 3 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 7f62a498)
2025-06-12T13:13:11.201+05:30 host=ip-175-130-252-63 service=job_executor app=data_pipeline environment=prod job_id=a86b4ef4 source=Airflow status=PENDING message="DAG 'dag_pipeline_8' task 'load_to_dw' is pending execution."
2025-06-12T13:14:22.201+05:30 host=ip-44-186-56-30 service=job_executor app=data_pipeline environment=prod job_id=bf52ad58 source=Spark status=SUCCEEDED message="Spark job 'job_name_230' completed successfully. Results saved."
2025-06-12T13:14:41.201+05:30 host=ip-58-124-63-87 service=job_executor app=data_pipeline environment=prod job_id=a1915f22 source=Glue status=PENDING message="Glue ETL job 'job_name_221' is queued for execution."
2025-06-12T13:14:42.201+05:30 host=ip-12-93-172-196 service=job_executor app=data_pipeline environment=prod job_id=42b035be source=Spark status=PENDING message="Spark job 'job_name_50' submitted to cluster, waiting for resources."
2025-06-12T13:14:44.201+05:30 host=ip-187-16-2-66 service=job_executor app=data_pipeline environment=prod job_id=331b154d source=Glue status=PENDING message="Glue ETL job 'job_name_137' is queued for execution."
2025-06-12T13:14:45.201+05:30 host=ip-104-142-55-240 service=job_executor app=data_pipeline environment=prod job_id=c6d40170 source=Spark status=RUNNING message="Spark job 'job_name_162' started. Processing partition 100 of 183."
2025-06-12T13:14:48.201+05:30 host=ip-33-126-164-135 service=job_executor app=data_pipeline environment=prod job_id=42aa1e08 source=Airflow status=PENDING message="DAG 'dag_pipeline_7' task 'cleanup_temp' is pending execution."
2025-06-12T13:15:17.201+05:30 host=ip-100-58-148-53 service=job_executor app=data_pipeline environment=prod job_id=acd5f4ab source=Hadoop status=RUNNING message="MapReduce job 'job_name_206' running. Map phase 77% complete."
2025-06-12T13:15:19.201+05:30 host=ip-28-135-109-36 service=job_executor app=data_pipeline environment=prod job_id=a23d2827 source=Hadoop status=RUNNING message="MapReduce job 'job_name_200' running. Map phase 31% complete."
2025-06-12T13:15:24.201+05:30 host=ip-133-108-68-163 service=job_executor app=data_pipeline environment=prod job_id=e7c42ebd source=Spark status=PENDING message="Spark job 'job_name_28' submitted to cluster, waiting for resources."
2025-06-12T13:15:30.201+05:30 host=ip-97-85-171-14 service=job_executor app=data_pipeline environment=prod job_id=acd5f4ab source=Hadoop status=FAILED error_type="java.net.ConnectException" message="Job 'job_name_206' failed: java.net.ConnectException: Connection refused"
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	... 7 more
Caused by: java.net.ConnectException (for JobID: acd5f4ab)
2025-06-12T13:15:31.201+05:30 host=ip-155-24-164-248 service=job_executor app=data_pipeline environment=prod job_id=43efa9ce source=Airflow status=PENDING message="DAG 'dag_pipeline_18' task 'extract_data' is pending execution."
2025-06-12T13:15:52.201+05:30 host=ip-127-229-132-82 service=job_executor app=data_pipeline environment=prod job_id=73b7de20 source=Spark status=RUNNING message="Spark job 'job_name_208' started. Processing partition 28 of 168."
2025-06-12T13:15:56.201+05:30 host=ip-180-146-103-192 service=job_executor app=data_pipeline environment=prod job_id=dfecb62d source=Glue status=RUNNING message="Glue ETL job 'job_name_82' started execution."
2025-06-12T13:16:00.201+05:30 host=ip-36-51-66-95 service=job_executor app=data_pipeline environment=prod job_id=900a5829 source=Airflow status=PENDING message="DAG 'dag_pipeline_11' task 'cleanup_temp' is pending execution."
2025-06-12T13:16:07.201+05:30 host=ip-122-99-249-15 service=job_executor app=data_pipeline environment=prod job_id=aaafe1c3 source=Spark status=SUCCEEDED message="Spark job 'job_name_194' completed successfully. Results saved."
2025-06-12T13:16:09.201+05:30 host=ip-184-104-226-20 service=job_executor app=data_pipeline environment=prod job_id=3c3ff1b8 source=Glue status=RUNNING message="Glue ETL job 'job_name_35' started execution."
2025-06-12T13:16:43.201+05:30 host=ip-104-160-88-224 service=job_executor app=data_pipeline environment=prod job_id=011aa9bc source=Airflow status=PENDING message="DAG 'dag_pipeline_27' task 'load_to_dw' is pending execution."
2025-06-12T13:16:46.201+05:30 host=ip-185-151-109-54 service=job_executor app=data_pipeline environment=prod job_id=e7c42ebd source=Spark status=RUNNING message="Spark job 'job_name_28' started. Processing partition 75 of 193."
2025-06-12T13:17:23.201+05:30 host=ip-178-81-183-21 service=job_executor app=data_pipeline environment=prod job_id=ab933e02 source=Hadoop status=FAILED error_type="com.microsoft.sqlserver.jdbc.SQLServerException" message="Job 'job_name_133' failed: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed"
com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user 'etl_user'
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:500)
	at com.yourcompany.db.DBConnector.getConnection(DBConnector.java:80)
	... 9 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException (for JobID: ab933e02)
2025-06-12T13:17:36.201+05:30 host=ip-85-76-111-44 service=job_executor app=data_pipeline environment=prod job_id=13e9fede source=Spark status=PENDING message="Spark job 'job_name_93' submitted to cluster, waiting for resources."
2025-06-12T13:17:52.201+05:30 host=ip-159-255-23-51 service=job_executor app=data_pipeline environment=prod job_id=c2f44ec5 source=Airflow status=RUNNING message="DAG 'dag_pipeline_17' task 'generate_report' is now running."
2025-06-12T13:18:07.201+05:30 host=ip-71-175-50-211 service=job_executor app=data_pipeline environment=prod job_id=fb2b382f source=Spark status=RUNNING message="Spark job 'job_name_218' started. Processing partition 26 of 114."
2025-06-12T13:18:13.201+05:30 host=ip-156-249-50-81 service=job_executor app=data_pipeline environment=prod job_id=c2f44ec5 source=Airflow status=FAILED error_type="java.lang.NullPointerException" message="Job 'dag_pipeline_17' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: c2f44ec5)
2025-06-12T13:18:29.201+05:30 host=ip-99-3-151-85 service=job_executor app=data_pipeline environment=prod job_id=e801cf9a source=Spark status=PENDING message="Spark job 'job_name_37' submitted to cluster, waiting for resources."
2025-06-12T13:18:35.201+05:30 host=ip-85-33-223-251 service=job_executor app=data_pipeline environment=prod job_id=56148f8a source=Spark status=PENDING message="Spark job 'job_name_30' submitted to cluster, waiting for resources."
2025-06-12T13:19:04.201+05:30 host=ip-199-102-31-213 service=job_executor app=data_pipeline environment=prod job_id=706abb15 source=Spark status=PENDING message="Spark job 'job_name_63' submitted to cluster, waiting for resources."
2025-06-12T13:19:18.201+05:30 host=ip-182-201-112-200 service=job_executor app=data_pipeline environment=prod job_id=af5887b6 source=Hadoop status=RUNNING message="MapReduce job 'job_name_260' running. Map phase 47% complete."
2025-06-12T13:19:42.201+05:30 host=ip-92-230-127-39 service=job_executor app=data_pipeline environment=prod job_id=ff2e0558 source=Glue status=RUNNING message="Glue ETL job 'job_name_44' started execution."
2025-06-12T13:19:56.201+05:30 host=ip-180-149-197-160 service=job_executor app=data_pipeline environment=prod job_id=c6d40170 source=Spark status=FAILED error_type="ValueError" message="Job 'job_name_162' failed: ValueError: invalid literal"
ValueError: invalid literal for int() with base 10: 'abc'
	at pandas_read.py:127
	at data_processing.py:42
	... 3 more
Caused by: ValueError (for JobID: c6d40170)
2025-06-12T13:20:25.201+05:30 host=ip-153-109-62-112 service=job_executor app=data_pipeline environment=prod job_id=49529a8e source=Airflow status=PENDING message="DAG 'dag_pipeline_40' task 'generate_report' is pending execution."
2025-06-12T13:20:37.201+05:30 host=ip-164-187-178-234 service=job_executor app=data_pipeline environment=prod job_id=f5cea618 source=Hadoop status=PENDING message="MapReduce job 'job_name_223' submitted to YARN queue."
2025-06-12T13:20:44.201+05:30 host=ip-69-125-73-4 service=job_executor app=data_pipeline environment=prod job_id=900a5829 source=Airflow status=RUNNING message="DAG 'dag_pipeline_11' task 'load_to_dw' is now running."
2025-06-12T13:21:30.201+05:30 host=ip-77-158-189-245 service=job_executor app=data_pipeline environment=prod job_id=d548dd21 source=Glue status=FAILED error_type="java.lang.IllegalArgumentException" message="Job 'job_name_8' failed: java.lang.IllegalArgumentException: Port number"
java.lang.IllegalArgumentException: Port number out of range: 70000
	at java.net.ServerSocket.bind(ServerSocket.java:223)
	at com.example.NetworkService.start(NetworkService.java:89)
	... 2 more
Caused by: java.lang.IllegalArgumentException (for JobID: d548dd21)
2025-06-12T13:21:37.201+05:30 host=ip-82-7-2-234 service=job_executor app=data_pipeline environment=prod job_id=3441df49 source=Hadoop status=RUNNING message="MapReduce job 'job_name_122' running. Map phase 75% complete."
2025-06-12T13:21:42.201+05:30 host=ip-173-125-150-62 service=job_executor app=data_pipeline environment=prod job_id=ca10016c source=Glue status=PENDING message="Glue ETL job 'job_name_259' is queued for execution."
2025-06-12T13:21:51.201+05:30 host=ip-86-164-229-79 service=job_executor app=data_pipeline environment=prod job_id=3c6c1ae2 source=Glue status=PENDING message="Glue ETL job 'job_name_141' is queued for execution."
2025-06-12T13:22:25.201+05:30 host=ip-100-67-222-157 service=job_executor app=data_pipeline environment=prod job_id=a23d2827 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_200' completed successfully. Output written to HDFS."
2025-06-12T13:22:33.201+05:30 host=ip-181-24-13-255 service=job_executor app=data_pipeline environment=prod job_id=42b035be source=Spark status=RUNNING message="Spark job 'job_name_50' started. Processing partition 5 of 137."
2025-06-12T13:22:33.201+05:30 host=ip-65-19-122-60 service=job_executor app=data_pipeline environment=prod job_id=073c57ab source=Spark status=PENDING message="Spark job 'job_name_244' submitted to cluster, waiting for resources."
2025-06-12T13:23:35.201+05:30 host=ip-14-254-197-115 service=job_executor app=data_pipeline environment=prod job_id=56148f8a source=Spark status=RUNNING message="Spark job 'job_name_30' started. Processing partition 64 of 145."
2025-06-12T13:23:50.201+05:30 host=ip-32-235-88-243 service=job_executor app=data_pipeline environment=prod job_id=0041ec1b source=Spark status=RUNNING message="Spark job 'job_name_80' started. Processing partition 64 of 127."
2025-06-12T13:23:55.201+05:30 host=ip-34-218-159-65 service=job_executor app=data_pipeline environment=prod job_id=bd3a93fe source=Glue status=PENDING message="Glue ETL job 'job_name_117' is queued for execution."
2025-06-12T13:24:00.201+05:30 host=ip-82-141-106-20 service=job_executor app=data_pipeline environment=prod job_id=13e9fede source=Spark status=RUNNING message="Spark job 'job_name_93' started. Processing partition 67 of 129."
2025-06-12T13:24:28.201+05:30 host=ip-170-74-236-6 service=job_executor app=data_pipeline environment=prod job_id=f3c46b9f source=Spark status=PENDING message="Spark job 'job_name_251' submitted to cluster, waiting for resources."
2025-06-12T13:24:36.201+05:30 host=ip-68-90-184-17 service=job_executor app=data_pipeline environment=prod job_id=f5cea618 source=Hadoop status=RUNNING message="MapReduce job 'job_name_223' running. Map phase 72% complete."
2025-06-12T13:24:50.201+05:30 host=ip-22-74-61-217 service=job_executor app=data_pipeline environment=prod job_id=b53ac53c source=Spark status=PENDING message="Spark job 'job_name_156' submitted to cluster, waiting for resources."
2025-06-12T13:25:00.201+05:30 host=ip-73-50-79-144 service=job_executor app=data_pipeline environment=prod job_id=33f7f3ca source=Spark status=SUCCEEDED message="Spark job 'job_name_234' completed successfully. Results saved."
2025-06-12T13:25:12.201+05:30 host=ip-195-98-53-183 service=job_executor app=data_pipeline environment=prod job_id=d0f5a240 source=Hadoop status=PENDING message="MapReduce job 'job_name_197' submitted to YARN queue."
2025-06-12T13:25:25.201+05:30 host=ip-149-147-37-193 service=job_executor app=data_pipeline environment=prod job_id=ab25fbf7 source=Spark status=SUCCEEDED message="Spark job 'job_name_88' completed successfully. Results saved."
2025-06-12T13:25:27.201+05:30 host=ip-73-6-94-144 service=job_executor app=data_pipeline environment=prod job_id=477fe0e2 source=Airflow status=FAILED error_type="org.apache.hadoop.fs.FileNotFoundException" message="Job 'dag_pipeline_47' failed: org.apache.hadoop.fs.FileNotFoundException"
org.apache.hadoop.fs.FileNotFoundException: File /user/hadoop/input.txt does not exist
	at org.apache.hadoop.fs.LocalFileSystem.open(LocalFileSystem.java:123)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:67)
	... 8 more
Caused by: org.apache.hadoop.fs.FileNotFoundException (for JobID: 477fe0e2)
2025-06-12T13:25:28.201+05:30 host=ip-53-65-246-189 service=job_executor app=data_pipeline environment=prod job_id=ca10016c source=Glue status=RUNNING message="Glue ETL job 'job_name_259' started execution."
2025-06-12T13:25:30.201+05:30 host=ip-35-14-254-123 service=job_executor app=data_pipeline environment=prod job_id=51d6e632 source=Hadoop status=PENDING message="MapReduce job 'job_name_81' submitted to YARN queue."
2025-06-12T13:25:56.201+05:30 host=ip-159-168-127-138 service=job_executor app=data_pipeline environment=prod job_id=42b035be source=Spark status=SUCCEEDED message="Spark job 'job_name_50' completed successfully. Results saved."
2025-06-12T13:26:20.201+05:30 host=ip-101-129-189-163 service=job_executor app=data_pipeline environment=prod job_id=a86b4ef4 source=Airflow status=RUNNING message="DAG 'dag_pipeline_8' task 'transform_data' is now running."
2025-06-12T13:26:30.201+05:30 host=ip-49-244-134-86 service=job_executor app=data_pipeline environment=prod job_id=a86b4ef4 source=Airflow status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'dag_pipeline_8' failed: com.amazonaws.services.glue.GlueException: Entity not found"
com.amazonaws.services.glue.GlueException: Entity not found: database 'sales_db'
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:350)
	at com.amazonaws.services.glue.AWSGlueClient.getTable(AWSGlueClient.java:115)
	... 4 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: a86b4ef4)
2025-06-12T13:26:35.201+05:30 host=ip-14-177-138-154 service=job_executor app=data_pipeline environment=prod job_id=9797a539 source=Glue status=PENDING message="Glue ETL job 'job_name_158' is queued for execution."
2025-06-12T13:26:37.201+05:30 host=ip-47-199-146-230 service=job_executor app=data_pipeline environment=prod job_id=0d614b48 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_7' task 'transform_data' completed successfully."
2025-06-12T13:26:40.201+05:30 host=ip-106-56-171-237 service=job_executor app=data_pipeline environment=prod job_id=bd3a93fe source=Glue status=RUNNING message="Glue ETL job 'job_name_117' started execution."
2025-06-12T13:27:02.201+05:30 host=ip-51-41-205-247 service=job_executor app=data_pipeline environment=prod job_id=89791710 source=Airflow status=PENDING message="DAG 'dag_pipeline_22' task 'generate_report' is pending execution."
2025-06-12T13:27:22.201+05:30 host=ip-77-8-250-180 service=job_executor app=data_pipeline environment=prod job_id=13202d5e source=Airflow status=PENDING message="DAG 'dag_pipeline_41' task 'extract_data' is pending execution."
2025-06-12T13:27:25.201+05:30 host=ip-139-39-127-249 service=job_executor app=data_pipeline environment=prod job_id=ca10016c source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_259' completed successfully. Output written to S3."
2025-06-12T13:27:27.201+05:30 host=ip-131-35-219-171 service=job_executor app=data_pipeline environment=prod job_id=d0f5a240 source=Hadoop status=RUNNING message="MapReduce job 'job_name_197' running. Map phase 60% complete."
2025-06-12T13:27:39.201+05:30 host=ip-183-18-184-239 service=job_executor app=data_pipeline environment=prod job_id=331b154d source=Glue status=RUNNING message="Glue ETL job 'job_name_137' started execution."
2025-06-12T13:27:43.201+05:30 host=ip-193-90-251-169 service=job_executor app=data_pipeline environment=prod job_id=706abb15 source=Spark status=RUNNING message="Spark job 'job_name_63' started. Processing partition 57 of 139."
2025-06-12T13:27:48.201+05:30 host=ip-19-73-20-71 service=job_executor app=data_pipeline environment=prod job_id=073c57ab source=Spark status=RUNNING message="Spark job 'job_name_244' started. Processing partition 22 of 107."
2025-06-12T13:28:27.201+05:30 host=ip-176-55-13-196 service=job_executor app=data_pipeline environment=prod job_id=a1915f22 source=Glue status=RUNNING message="Glue ETL job 'job_name_221' started execution."
2025-06-12T13:28:43.201+05:30 host=ip-131-48-7-115 service=job_executor app=data_pipeline environment=prod job_id=654016f5 source=Spark status=PENDING message="Spark job 'job_name_51' submitted to cluster, waiting for resources."
2025-06-12T13:29:07.201+05:30 host=ip-182-1-97-90 service=job_executor app=data_pipeline environment=prod job_id=aa64ff53 source=Glue status=PENDING message="Glue ETL job 'job_name_173' is queued for execution."
2025-06-12T13:29:09.201+05:30 host=ip-134-54-1-139 service=job_executor app=data_pipeline environment=prod job_id=fb2b382f source=Spark status=SUCCEEDED message="Spark job 'job_name_218' completed successfully. Results saved."
2025-06-12T13:29:19.201+05:30 host=ip-70-98-66-39 service=job_executor app=data_pipeline environment=prod job_id=51d6e632 source=Hadoop status=RUNNING message="MapReduce job 'job_name_81' running. Map phase 88% complete."
2025-06-12T13:29:19.201+05:30 host=ip-29-195-254-95 service=job_executor app=data_pipeline environment=prod job_id=3c6c1ae2 source=Glue status=RUNNING message="Glue ETL job 'job_name_141' started execution."
2025-06-12T13:29:34.201+05:30 host=ip-130-215-16-75 service=job_executor app=data_pipeline environment=prod job_id=42aa1e08 source=Airflow status=RUNNING message="DAG 'dag_pipeline_7' task 'cleanup_temp' is now running."
2025-06-12T13:29:45.201+05:30 host=ip-101-119-130-163 service=job_executor app=data_pipeline environment=prod job_id=011aa9bc source=Airflow status=RUNNING message="DAG 'dag_pipeline_27' task 'transform_data' is now running."
2025-06-12T13:29:59.201+05:30 host=ip-22-83-100-36 service=job_executor app=data_pipeline environment=prod job_id=43efa9ce source=Airflow status=RUNNING message="DAG 'dag_pipeline_18' task 'generate_report' is now running."
2025-06-12T13:30:12.201+05:30 host=ip-182-231-169-100 service=job_executor app=data_pipeline environment=prod job_id=e801cf9a source=Spark status=RUNNING message="Spark job 'job_name_37' started. Processing partition 52 of 138."
2025-06-12T13:30:18.201+05:30 host=ip-187-64-248-237 service=job_executor app=data_pipeline environment=prod job_id=dc75e0f9 source=Hadoop status=PENDING message="MapReduce job 'job_name_165' submitted to YARN queue."
2025-06-12T13:30:18.201+05:30 host=ip-193-130-142-125 service=job_executor app=data_pipeline environment=prod job_id=d0f5a240 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_197' completed successfully. Output written to HDFS."
2025-06-12T13:30:21.201+05:30 host=ip-25-78-244-129 service=job_executor app=data_pipeline environment=prod job_id=6d2a53d4 source=Airflow status=PENDING message="DAG 'dag_pipeline_31' task 'extract_data' is pending execution."
2025-06-12T13:31:39.201+05:30 host=ip-27-84-60-80 service=job_executor app=data_pipeline environment=prod job_id=89791710 source=Airflow status=RUNNING message="DAG 'dag_pipeline_22' task 'cleanup_temp' is now running."
2025-06-12T13:32:04.201+05:30 host=ip-180-235-97-16 service=job_executor app=data_pipeline environment=prod job_id=49529a8e source=Airflow status=RUNNING message="DAG 'dag_pipeline_40' task 'extract_data' is now running."
2025-06-12T13:32:17.201+05:30 host=ip-11-37-42-112 service=job_executor app=data_pipeline environment=prod job_id=a474cf33 source=Hadoop status=PENDING message="MapReduce job 'job_name_204' submitted to YARN queue."
2025-06-12T13:32:24.201+05:30 host=ip-140-27-253-89 service=job_executor app=data_pipeline environment=prod job_id=aa64ff53 source=Glue status=RUNNING message="Glue ETL job 'job_name_173' started execution."
2025-06-12T13:32:45.201+05:30 host=ip-134-215-114-128 service=job_executor app=data_pipeline environment=prod job_id=19e775d9 source=Spark status=PENDING message="Spark job 'job_name_91' submitted to cluster, waiting for resources."
2025-06-12T13:32:53.201+05:30 host=ip-137-186-50-112 service=job_executor app=data_pipeline environment=prod job_id=331b154d source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_137' completed successfully. Output written to S3."
2025-06-12T13:32:57.201+05:30 host=ip-158-240-166-254 service=job_executor app=data_pipeline environment=prod job_id=2e55b26f source=Glue status=PENDING message="Glue ETL job 'job_name_205' is queued for execution."
2025-06-12T13:33:01.201+05:30 host=ip-104-45-205-142 service=job_executor app=data_pipeline environment=prod job_id=574b2643 source=Glue status=FAILED error_type="java.io.FileNotFoundException" message="Job 'job_name_262' failed: java.io.FileNotFoundException"
java.io.FileNotFoundException: /path/to/output/file (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	... 6 more
Caused by: java.io.FileNotFoundException (for JobID: 574b2643)
2025-06-12T13:33:09.201+05:30 host=ip-10-138-204-37 service=job_executor app=data_pipeline environment=prod job_id=6d2a53d4 source=Airflow status=RUNNING message="DAG 'dag_pipeline_31' task 'transform_data' is now running."
2025-06-12T13:33:30.201+05:30 host=ip-27-58-190-201 service=job_executor app=data_pipeline environment=prod job_id=56148f8a source=Spark status=SUCCEEDED message="Spark job 'job_name_30' completed successfully. Results saved."
2025-06-12T13:33:36.201+05:30 host=ip-79-140-60-193 service=job_executor app=data_pipeline environment=prod job_id=603acd63 source=Hadoop status=PENDING message="MapReduce job 'job_name_121' submitted to YARN queue."
2025-06-12T13:33:59.201+05:30 host=ip-163-167-25-41 service=job_executor app=data_pipeline environment=prod job_id=73b7de20 source=Spark status=FAILED error_type="org.apache.spark.SparkException" message="Job 'job_name_208' failed: org.apache.spark.SparkException"
org.apache.spark.SparkException: Job aborted due to stage failure
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
	at org.apache.spark.scheduler.Task.run(Task.scala:120)
	at org.apache.spark.executor.Executor.$anonfun$runJob$4(Executor.scala:180)
	... 5 more
Caused by: org.apache.spark.SparkException (for JobID: 73b7de20)
2025-06-12T13:34:07.201+05:30 host=ip-158-212-240-203 service=job_executor app=data_pipeline environment=prod job_id=3c6c1ae2 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_141' completed successfully. Output written to S3."
2025-06-12T13:34:49.201+05:30 host=ip-99-91-119-196 service=job_executor app=data_pipeline environment=prod job_id=e7c42ebd source=Spark status=SUCCEEDED message="Spark job 'job_name_28' completed successfully. Results saved."
2025-06-12T13:35:06.201+05:30 host=ip-193-157-89-247 service=job_executor app=data_pipeline environment=prod job_id=654016f5 source=Spark status=RUNNING message="Spark job 'job_name_51' started. Processing partition 84 of 106."
2025-06-12T13:35:21.201+05:30 host=ip-16-60-82-253 service=job_executor app=data_pipeline environment=prod job_id=5dabc600 source=Airflow status=PENDING message="DAG 'dag_pipeline_10' task 'generate_report' is pending execution."
2025-06-12T13:35:58.201+05:30 host=ip-165-58-211-80 service=job_executor app=data_pipeline environment=prod job_id=89791710 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_22' task 'cleanup_temp' completed successfully."
2025-06-12T13:36:22.201+05:30 host=ip-70-202-106-112 service=job_executor app=data_pipeline environment=prod job_id=f3c46b9f source=Spark status=RUNNING message="Spark job 'job_name_251' started. Processing partition 5 of 149."
2025-06-12T13:36:40.201+05:30 host=ip-171-191-161-217 service=job_executor app=data_pipeline environment=prod job_id=5cbc70d1 source=Glue status=PENDING message="Glue ETL job 'job_name_243' is queued for execution."
2025-06-12T13:36:43.201+05:30 host=ip-79-57-64-116 service=job_executor app=data_pipeline environment=prod job_id=af5887b6 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_260' completed successfully. Output written to HDFS."
2025-06-12T13:36:54.201+05:30 host=ip-187-38-220-113 service=job_executor app=data_pipeline environment=prod job_id=9797a539 source=Glue status=RUNNING message="Glue ETL job 'job_name_158' started execution."
2025-06-12T13:37:00.201+05:30 host=ip-19-64-34-19 service=job_executor app=data_pipeline environment=prod job_id=13e9fede source=Spark status=SUCCEEDED message="Spark job 'job_name_93' completed successfully. Results saved."
2025-06-12T13:37:36.201+05:30 host=ip-167-72-106-129 service=job_executor app=data_pipeline environment=prod job_id=3c3ff1b8 source=Glue status=FAILED error_type="org.apache.kafka.common.errors.TimeoutException" message="Job 'job_name_35' failed: org.apache.kafka.common.errors.TimeoutException"
org.apache.kafka.common.errors.TimeoutException: Topic partition assignment timeout after 60000 ms
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:350)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1245)
	... 6 more
Caused by: org.apache.kafka.common.errors.TimeoutException (for JobID: 3c3ff1b8)
2025-06-12T13:38:00.201+05:30 host=ip-148-202-209-216 service=job_executor app=data_pipeline environment=prod job_id=8ece055c source=Airflow status=FAILED error_type="ValueError" message="Job 'dag_pipeline_6' failed: ValueError: invalid literal"
ValueError: invalid literal for int() with base 10: 'abc'
	at pandas_read.py:127
	at data_processing.py:42
	... 3 more
Caused by: ValueError (for JobID: 8ece055c)
2025-06-12T13:38:00.201+05:30 host=ip-90-110-239-111 service=job_executor app=data_pipeline environment=prod job_id=bd281c73 source=Glue status=PENDING message="Glue ETL job 'job_name_209' is queued for execution."
2025-06-12T13:38:18.201+05:30 host=ip-21-16-184-84 service=job_executor app=data_pipeline environment=prod job_id=b53ac53c source=Spark status=RUNNING message="Spark job 'job_name_156' started. Processing partition 69 of 196."
2025-06-12T13:38:20.201+05:30 host=ip-126-150-72-11 service=job_executor app=data_pipeline environment=prod job_id=51f3010a source=Airflow status=PENDING message="DAG 'dag_pipeline_28' task 'extract_data' is pending execution."
2025-06-12T13:38:24.201+05:30 host=ip-150-251-132-191 service=job_executor app=data_pipeline environment=prod job_id=37f1f1b0 source=Spark status=SUCCEEDED message="Spark job 'job_name_77' completed successfully. Results saved."
2025-06-12T13:38:31.201+05:30 host=ip-192-221-96-113 service=job_executor app=data_pipeline environment=prod job_id=13202d5e source=Airflow status=RUNNING message="DAG 'dag_pipeline_41' task 'cleanup_temp' is now running."
2025-06-12T13:38:48.201+05:30 host=ip-60-210-255-137 service=job_executor app=data_pipeline environment=prod job_id=f3c46b9f source=Spark status=FAILED error_type="java.lang.IllegalArgumentException" message="Job 'job_name_251' failed: java.lang.IllegalArgumentException: Port number"
java.lang.IllegalArgumentException: Port number out of range: 70000
	at java.net.ServerSocket.bind(ServerSocket.java:223)
	at com.example.NetworkService.start(NetworkService.java:89)
	... 2 more
Caused by: java.lang.IllegalArgumentException (for JobID: f3c46b9f)
2025-06-12T13:39:11.201+05:30 host=ip-134-196-23-132 service=job_executor app=data_pipeline environment=prod job_id=ab3762ba source=Airflow status=PENDING message="DAG 'dag_pipeline_23' task 'generate_report' is pending execution."
2025-06-12T13:39:16.201+05:30 host=ip-124-77-242-233 service=job_executor app=data_pipeline environment=prod job_id=5dabc600 source=Airflow status=RUNNING message="DAG 'dag_pipeline_10' task 'generate_report' is now running."
2025-06-12T13:39:27.201+05:30 host=ip-128-239-39-223 service=job_executor app=data_pipeline environment=prod job_id=6d2a53d4 source=Airflow status=FAILED error_type="java.io.FileNotFoundException" message="Job 'dag_pipeline_31' failed: java.io.FileNotFoundException"
java.io.FileNotFoundException: /path/to/output/file (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	... 6 more
Caused by: java.io.FileNotFoundException (for JobID: 6d2a53d4)
2025-06-12T13:39:48.201+05:30 host=ip-190-16-240-4 service=job_executor app=data_pipeline environment=prod job_id=2e55b26f source=Glue status=RUNNING message="Glue ETL job 'job_name_205' started execution."
2025-06-12T13:40:22.201+05:30 host=ip-137-197-64-72 service=job_executor app=data_pipeline environment=prod job_id=3af8ec8c source=Glue status=PENDING message="Glue ETL job 'job_name_112' is queued for execution."
2025-06-12T13:40:34.201+05:30 host=ip-116-190-52-147 service=job_executor app=data_pipeline environment=prod job_id=dfecb62d source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_82' completed successfully. Output written to S3."
2025-06-12T13:40:45.201+05:30 host=ip-158-215-170-121 service=job_executor app=data_pipeline environment=prod job_id=60b7c8f5 source=Hadoop status=PENDING message="MapReduce job 'job_name_48' submitted to YARN queue."
2025-06-12T13:41:04.201+05:30 host=ip-71-165-105-76 service=job_executor app=data_pipeline environment=prod job_id=eb030f69 source=Airflow status=PENDING message="DAG 'dag_pipeline_44' task 'cleanup_temp' is pending execution."
2025-06-12T13:41:39.201+05:30 host=ip-194-139-218-53 service=job_executor app=data_pipeline environment=prod job_id=00ce23f6 source=Airflow status=PENDING message="DAG 'dag_pipeline_35' task 'extract_data' is pending execution."
2025-06-12T13:42:06.201+05:30 host=ip-177-161-164-69 service=job_executor app=data_pipeline environment=prod job_id=49c9e593 source=Hadoop status=PENDING message="MapReduce job 'job_name_129' submitted to YARN queue."
2025-06-12T13:43:09.201+05:30 host=ip-105-166-230-27 service=job_executor app=data_pipeline environment=prod job_id=706abb15 source=Spark status=FAILED error_type="AttributeError" message="Job 'job_name_63' failed: AttributeError: 'NoneType' object has no attribute 'split'"
AttributeError: 'NoneType' object has no attribute 'split'
	at preprocess_data.py:33
	at main.py:78
	... 5 more
Caused by: AttributeError (for JobID: 706abb15)
2025-06-12T13:43:13.201+05:30 host=ip-90-17-85-132 service=job_executor app=data_pipeline environment=prod job_id=eb030f69 source=Airflow status=RUNNING message="DAG 'dag_pipeline_44' task 'load_to_dw' is now running."
2025-06-12T13:43:23.201+05:30 host=ip-69-16-129-74 service=job_executor app=data_pipeline environment=prod job_id=ff2e0558 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_44' completed successfully. Output written to S3."
2025-06-12T13:43:42.201+05:30 host=ip-107-242-49-42 service=job_executor app=data_pipeline environment=prod job_id=00ce23f6 source=Airflow status=RUNNING message="DAG 'dag_pipeline_35' task 'generate_report' is now running."
2025-06-12T13:43:52.201+05:30 host=ip-37-183-109-100 service=job_executor app=data_pipeline environment=prod job_id=68e510b1 source=Hadoop status=PENDING message="MapReduce job 'job_name_96' submitted to YARN queue."
2025-06-12T13:43:55.201+05:30 host=ip-125-253-39-174 service=job_executor app=data_pipeline environment=prod job_id=51f3010a source=Airflow status=RUNNING message="DAG 'dag_pipeline_28' task 'generate_report' is now running."
2025-06-12T13:44:16.201+05:30 host=ip-102-208-184-170 service=job_executor app=data_pipeline environment=prod job_id=dc75e0f9 source=Hadoop status=RUNNING message="MapReduce job 'job_name_165' running. Map phase 40% complete."
2025-06-12T13:44:17.201+05:30 host=ip-61-97-109-59 service=job_executor app=data_pipeline environment=prod job_id=a474cf33 source=Hadoop status=RUNNING message="MapReduce job 'job_name_204' running. Map phase 73% complete."
2025-06-12T13:44:19.201+05:30 host=ip-15-87-107-79 service=job_executor app=data_pipeline environment=prod job_id=ab3762ba source=Airflow status=RUNNING message="DAG 'dag_pipeline_23' task 'transform_data' is now running."
2025-06-12T13:44:32.201+05:30 host=ip-178-1-218-163 service=job_executor app=data_pipeline environment=prod job_id=006fb5ff source=Spark status=PENDING message="Spark job 'job_name_71' submitted to cluster, waiting for resources."
2025-06-12T13:44:43.201+05:30 host=ip-186-94-176-219 service=job_executor app=data_pipeline environment=prod job_id=3441df49 source=Hadoop status=FAILED error_type="java.net.ConnectException" message="Job 'job_name_122' failed: java.net.ConnectException: Connection refused"
java.net.ConnectException: Connection refused (Connection refused)
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	... 7 more
Caused by: java.net.ConnectException (for JobID: 3441df49)
2025-06-12T13:44:46.201+05:30 host=ip-78-63-18-12 service=job_executor app=data_pipeline environment=prod job_id=603acd63 source=Hadoop status=RUNNING message="MapReduce job 'job_name_121' running. Map phase 81% complete."
2025-06-12T13:44:49.201+05:30 host=ip-115-109-92-56 service=job_executor app=data_pipeline environment=prod job_id=0041ec1b source=Spark status=SUCCEEDED message="Spark job 'job_name_80' completed successfully. Results saved."
2025-06-12T13:45:06.201+05:30 host=ip-21-150-183-139 service=job_executor app=data_pipeline environment=prod job_id=900a5829 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_11' task 'generate_report' completed successfully."
2025-06-12T13:45:12.201+05:30 host=ip-140-181-255-203 service=job_executor app=data_pipeline environment=prod job_id=19e775d9 source=Spark status=RUNNING message="Spark job 'job_name_91' started. Processing partition 34 of 125."
2025-06-12T13:45:29.201+05:30 host=ip-145-203-173-10 service=job_executor app=data_pipeline environment=prod job_id=aa64ff53 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_173' completed successfully. Output written to S3."
2025-06-12T13:45:30.201+05:30 host=ip-101-196-191-84 service=job_executor app=data_pipeline environment=prod job_id=13202d5e source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_41' task 'extract_data' completed successfully."
2025-06-12T13:45:34.201+05:30 host=ip-59-224-146-19 service=job_executor app=data_pipeline environment=prod job_id=bd3a93fe source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_117' completed successfully. Output written to S3."
2025-06-12T13:45:57.201+05:30 host=ip-17-89-243-151 service=job_executor app=data_pipeline environment=prod job_id=ab3762ba source=Airflow status=FAILED error_type="java.lang.NullPointerException" message="Job 'dag_pipeline_23' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: ab3762ba)
2025-06-12T13:46:44.201+05:30 host=ip-150-88-42-185 service=job_executor app=data_pipeline environment=prod job_id=dc75e0f9 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_165' completed successfully. Output written to HDFS."
2025-06-12T13:47:48.201+05:30 host=ip-191-239-91-181 service=job_executor app=data_pipeline environment=prod job_id=9797a539 source=Glue status=FAILED error_type="java.io.FileNotFoundException" message="Job 'job_name_158' failed: java.io.FileNotFoundException"
java.io.FileNotFoundException: /path/to/output/file (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	... 6 more
Caused by: java.io.FileNotFoundException (for JobID: 9797a539)
2025-06-12T13:48:14.201+05:30 host=ip-111-146-82-55 service=job_executor app=data_pipeline environment=prod job_id=2e55b26f source=Glue status=FAILED error_type="org.apache.hadoop.mapreduce.lib.input.FileInputFormat" message="Job 'job_name_205' failed: org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist"
org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Input path does not exist: file:/data/missing.csv
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:275)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJob(JobSubmitter.java:145)
	... 10 more
Caused by: org.apache.hadoop.mapreduce.lib.input.FileInputFormat (for JobID: 2e55b26f)
2025-06-12T13:48:16.201+05:30 host=ip-127-155-37-222 service=job_executor app=data_pipeline environment=prod job_id=f5cea618 source=Hadoop status=FAILED error_type="java.lang.IllegalArgumentException" message="Job 'job_name_223' failed: java.lang.IllegalArgumentException: Port number"
java.lang.IllegalArgumentException: Port number out of range: 70000
	at java.net.ServerSocket.bind(ServerSocket.java:223)
	at com.example.NetworkService.start(NetworkService.java:89)
	... 2 more
Caused by: java.lang.IllegalArgumentException (for JobID: f5cea618)
2025-06-12T13:48:17.201+05:30 host=ip-128-110-207-35 service=job_executor app=data_pipeline environment=prod job_id=77f4e545 source=Hadoop status=PENDING message="MapReduce job 'job_name_274' submitted to YARN queue."
2025-06-12T13:48:39.201+05:30 host=ip-191-19-130-195 service=job_executor app=data_pipeline environment=prod job_id=e801cf9a source=Spark status=SUCCEEDED message="Spark job 'job_name_37' completed successfully. Results saved."
2025-06-12T13:49:00.201+05:30 host=ip-78-133-130-243 service=job_executor app=data_pipeline environment=prod job_id=fb23f407 source=Hadoop status=PENDING message="MapReduce job 'job_name_17' submitted to YARN queue."
2025-06-12T13:49:01.201+05:30 host=ip-109-32-217-0 service=job_executor app=data_pipeline environment=prod job_id=49c9e593 source=Hadoop status=RUNNING message="MapReduce job 'job_name_129' running. Map phase 69% complete."
2025-06-12T13:49:01.201+05:30 host=ip-194-139-50-182 service=job_executor app=data_pipeline environment=prod job_id=a1915f22 source=Glue status=FAILED error_type="org.apache.hadoop.fs.FileNotFoundException" message="Job 'job_name_221' failed: org.apache.hadoop.fs.FileNotFoundException"
org.apache.hadoop.fs.FileNotFoundException: File /user/hadoop/input.txt does not exist
	at org.apache.hadoop.fs.LocalFileSystem.open(LocalFileSystem.java:123)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:67)
	... 8 more
Caused by: org.apache.hadoop.fs.FileNotFoundException (for JobID: a1915f22)
2025-06-12T13:49:18.201+05:30 host=ip-49-227-140-209 service=job_executor app=data_pipeline environment=prod job_id=60b7c8f5 source=Hadoop status=RUNNING message="MapReduce job 'job_name_48' running. Map phase 80% complete."
2025-06-12T13:49:52.201+05:30 host=ip-28-28-8-73 service=job_executor app=data_pipeline environment=prod job_id=68e510b1 source=Hadoop status=RUNNING message="MapReduce job 'job_name_96' running. Map phase 46% complete."
2025-06-12T13:50:02.201+05:30 host=ip-17-123-203-206 service=job_executor app=data_pipeline environment=prod job_id=51d6e632 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_81' completed successfully. Output written to HDFS."
2025-06-12T13:51:31.201+05:30 host=ip-32-22-124-14 service=job_executor app=data_pipeline environment=prod job_id=5cbc70d1 source=Glue status=RUNNING message="Glue ETL job 'job_name_243' started execution."
2025-06-12T13:51:42.201+05:30 host=ip-14-183-235-244 service=job_executor app=data_pipeline environment=prod job_id=3af8ec8c source=Glue status=RUNNING message="Glue ETL job 'job_name_112' started execution."
2025-06-12T13:51:46.201+05:30 host=ip-84-163-121-116 service=job_executor app=data_pipeline environment=prod job_id=e836688a source=Airflow status=PENDING message="DAG 'dag_pipeline_1' task 'extract_data' is pending execution."
2025-06-12T13:51:48.201+05:30 host=ip-98-241-20-147 service=job_executor app=data_pipeline environment=prod job_id=bd281c73 source=Glue status=RUNNING message="Glue ETL job 'job_name_209' started execution."
2025-06-12T13:52:00.201+05:30 host=ip-55-71-220-181 service=job_executor app=data_pipeline environment=prod job_id=77f4e545 source=Hadoop status=RUNNING message="MapReduce job 'job_name_274' running. Map phase 42% complete."
2025-06-12T13:52:01.201+05:30 host=ip-40-213-123-125 service=job_executor app=data_pipeline environment=prod job_id=1ce2908d source=Spark status=PENDING message="Spark job 'job_name_266' submitted to cluster, waiting for resources."
2025-06-12T13:52:07.201+05:30 host=ip-62-0-175-73 service=job_executor app=data_pipeline environment=prod job_id=49529a8e source=Airflow status=FAILED error_type="com.microsoft.sqlserver.jdbc.SQLServerException" message="Job 'dag_pipeline_40' failed: com.microsoft.sqlserver.jdbc.SQLServerException: Login failed"
com.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user 'etl_user'
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:500)
	at com.yourcompany.db.DBConnector.getConnection(DBConnector.java:80)
	... 9 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException (for JobID: 49529a8e)
2025-06-12T13:52:08.201+05:30 host=ip-148-252-69-202 service=job_executor app=data_pipeline environment=prod job_id=086f7756 source=Airflow status=PENDING message="DAG 'dag_pipeline_25' task 'cleanup_temp' is pending execution."
2025-06-12T13:52:52.201+05:30 host=ip-174-118-131-228 service=job_executor app=data_pipeline environment=prod job_id=5b2dd944 source=Glue status=PENDING message="Glue ETL job 'job_name_130' is queued for execution."
2025-06-12T13:53:16.201+05:30 host=ip-56-139-250-200 service=job_executor app=data_pipeline environment=prod job_id=745853e4 source=Hadoop status=PENDING message="MapReduce job 'job_name_136' submitted to YARN queue."
2025-06-12T13:53:40.201+05:30 host=ip-45-44-65-33 service=job_executor app=data_pipeline environment=prod job_id=006fb5ff source=Spark status=RUNNING message="Spark job 'job_name_71' started. Processing partition 24 of 117."
2025-06-12T13:53:41.201+05:30 host=ip-198-38-128-94 service=job_executor app=data_pipeline environment=prod job_id=17b5fbb0 source=Hadoop status=FAILED error_type="FileNotFoundError" message="Job 'job_name_16' failed: FileNotFoundError: [Errno 2] No such file or directory"
FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'
	at open_config.py:45
	at initialize.py:12
	... 3 more
Caused by: FileNotFoundError (for JobID: 17b5fbb0)
2025-06-12T13:53:43.201+05:30 host=ip-141-112-86-194 service=job_executor app=data_pipeline environment=prod job_id=50160afe source=Hadoop status=RUNNING message="MapReduce job 'job_name_155' running. Map phase 80% complete."
2025-06-12T13:53:44.201+05:30 host=ip-26-251-32-119 service=job_executor app=data_pipeline environment=prod job_id=49c9e593 source=Hadoop status=FAILED error_type="java.io.FileNotFoundException" message="Job 'job_name_129' failed: java.io.FileNotFoundException"
java.io.FileNotFoundException: /path/to/output/file (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	... 6 more
Caused by: java.io.FileNotFoundException (for JobID: 49c9e593)
2025-06-12T13:53:46.201+05:30 host=ip-55-227-20-73 service=job_executor app=data_pipeline environment=prod job_id=5b2dd944 source=Glue status=FAILED error_type="org.apache.kafka.common.errors.TimeoutException" message="Job 'job_name_130' failed: org.apache.kafka.common.errors.TimeoutException"
org.apache.kafka.common.errors.TimeoutException: Topic partition assignment timeout after 60000 ms
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:350)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1245)
	... 6 more
Caused by: org.apache.kafka.common.errors.TimeoutException (for JobID: 5b2dd944)
2025-06-12T13:53:46.201+05:30 host=ip-49-82-227-215 service=job_executor app=data_pipeline environment=prod job_id=50160afe source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_155' completed successfully. Output written to HDFS."
2025-06-12T13:53:48.201+05:30 host=ip-54-112-79-27 service=job_executor app=data_pipeline environment=prod job_id=684dda76 source=Airflow status=PENDING message="DAG 'dag_pipeline_26' task 'load_to_dw' is pending execution."
2025-06-12T13:53:50.201+05:30 host=ip-173-154-190-241 service=job_executor app=data_pipeline environment=prod job_id=19e775d9 source=Spark status=FAILED error_type="java.lang.NullPointerException" message="Job 'job_name_91' failed: java.lang.NullPointerException"
java.lang.NullPointerException: Cannot invoke "String.toString()" because "input" is null
	at com.example.DataProcessor.process(DataProcessor.java:42)
	at com.example.Main.run(Main.java:17)
	... 5 more
Caused by: java.lang.NullPointerException (for JobID: 19e775d9)
2025-06-12T13:53:51.201+05:30 host=ip-189-58-82-113 service=job_executor app=data_pipeline environment=prod job_id=68e510b1 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_96' completed successfully. Output written to HDFS."
2025-06-12T13:53:51.201+05:30 host=ip-174-199-58-87 service=job_executor app=data_pipeline environment=prod job_id=a474cf33 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_204' completed successfully. Output written to HDFS."
2025-06-12T13:53:53.201+05:30 host=ip-41-124-240-233 service=job_executor app=data_pipeline environment=prod job_id=654016f5 source=Spark status=SUCCEEDED message="Spark job 'job_name_51' completed successfully. Results saved."
2025-06-12T13:53:53.201+05:30 host=ip-37-38-132-166 service=job_executor app=data_pipeline environment=prod job_id=006fb5ff source=Spark status=FAILED error_type="TypeError" message="Job 'job_name_71' failed: TypeError: cannot concatenate"
TypeError: cannot concatenate 'str' and 'int' objects
	at transform_data.py:56
	at main_workflow.py:112
	... 4 more
Caused by: TypeError (for JobID: 006fb5ff)
2025-06-12T13:53:53.201+05:30 host=ip-122-80-76-225 service=job_executor app=data_pipeline environment=prod job_id=e2d9cb71 source=Airflow status=RUNNING message="DAG 'dag_pipeline_19' task 'load_to_dw' is now running."
2025-06-12T13:53:54.201+05:30 host=ip-120-62-97-145 service=job_executor app=data_pipeline environment=prod job_id=5294fdc1 source=Spark status=RUNNING message="Spark job 'job_name_180' started. Processing partition 44 of 180."
2025-06-12T13:53:55.201+05:30 host=ip-45-203-76-74 service=job_executor app=data_pipeline environment=prod job_id=104c2a01 source=Hadoop status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_55' failed: com.amazonaws.services.glue.GlueException: Entity not found"
com.amazonaws.services.glue.GlueException: Entity not found: database 'sales_db'
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:350)
	at com.amazonaws.services.glue.AWSGlueClient.getTable(AWSGlueClient.java:115)
	... 4 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 104c2a01)
2025-06-12T13:53:55.201+05:30 host=ip-68-209-89-195 service=job_executor app=data_pipeline environment=prod job_id=745853e4 source=Hadoop status=RUNNING message="MapReduce job 'job_name_136' running. Map phase 38% complete."
2025-06-12T13:53:55.201+05:30 host=ip-83-109-70-178 service=job_executor app=data_pipeline environment=prod job_id=50160afe source=Hadoop status=PENDING message="MapReduce job 'job_name_155' submitted to YARN queue."
2025-06-12T13:53:56.201+05:30 host=ip-83-69-19-46 service=job_executor app=data_pipeline environment=prod job_id=4921ebfa source=Spark status=RUNNING message="Spark job 'job_name_193' started. Processing partition 76 of 102."
2025-06-12T13:53:58.201+05:30 host=ip-149-78-76-65 service=job_executor app=data_pipeline environment=prod job_id=e836688a source=Airflow status=RUNNING message="DAG 'dag_pipeline_1' task 'transform_data' is now running."
2025-06-12T13:53:58.201+05:30 host=ip-187-189-186-245 service=job_executor app=data_pipeline environment=prod job_id=603acd63 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_121' completed successfully. Output written to HDFS."
2025-06-12T13:54:00.201+05:30 host=ip-23-60-31-220 service=job_executor app=data_pipeline environment=prod job_id=5cbc70d1 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_243' completed successfully. Output written to S3."
2025-06-12T13:54:01.201+05:30 host=ip-38-186-49-65 service=job_executor app=data_pipeline environment=prod job_id=42aa1e08 source=Airflow status=FAILED error_type="FileNotFoundError" message="Job 'dag_pipeline_7' failed: FileNotFoundError: [Errno 2] No such file or directory"
FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'
	at open_config.py:45
	at initialize.py:12
	... 3 more
Caused by: FileNotFoundError (for JobID: 42aa1e08)
2025-06-12T13:54:03.201+05:30 host=ip-97-141-35-215 service=job_executor app=data_pipeline environment=prod job_id=086f7756 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_25' task 'transform_data' completed successfully."
2025-06-12T13:54:06.201+05:30 host=ip-145-142-250-190 service=job_executor app=data_pipeline environment=prod job_id=fb23f407 source=Hadoop status=RUNNING message="MapReduce job 'job_name_17' running. Map phase 88% complete."
2025-06-12T13:54:09.201+05:30 host=ip-151-109-109-174 service=job_executor app=data_pipeline environment=prod job_id=104c2a01 source=Hadoop status=RUNNING message="MapReduce job 'job_name_55' running. Map phase 32% complete."
2025-06-12T13:54:09.201+05:30 host=ip-167-217-218-37 service=job_executor app=data_pipeline environment=prod job_id=2fc9c3b2 source=Airflow status=PENDING message="DAG 'dag_pipeline_41' task 'generate_report' is pending execution."
2025-06-12T13:54:10.201+05:30 host=ip-28-63-208-197 service=job_executor app=data_pipeline environment=prod job_id=684dda76 source=Airflow status=RUNNING message="DAG 'dag_pipeline_26' task 'load_to_dw' is now running."
2025-06-12T13:54:11.201+05:30 host=ip-125-198-129-34 service=job_executor app=data_pipeline environment=prod job_id=5dabc600 source=Airflow status=FAILED error_type="java.lang.IllegalArgumentException" message="Job 'dag_pipeline_10' failed: java.lang.IllegalArgumentException: Port number"
java.lang.IllegalArgumentException: Port number out of range: 70000
	at java.net.ServerSocket.bind(ServerSocket.java:223)
	at com.example.NetworkService.start(NetworkService.java:89)
	... 2 more
Caused by: java.lang.IllegalArgumentException (for JobID: 5dabc600)
2025-06-12T13:54:12.201+05:30 host=ip-87-47-78-182 service=job_executor app=data_pipeline environment=prod job_id=e2d9cb71 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_19' task 'generate_report' completed successfully."
2025-06-12T13:54:13.201+05:30 host=ip-34-181-89-112 service=job_executor app=data_pipeline environment=prod job_id=17b5fbb0 source=Hadoop status=RUNNING message="MapReduce job 'job_name_16' running. Map phase 63% complete."
2025-06-12T13:54:14.201+05:30 host=ip-57-224-9-218 service=job_executor app=data_pipeline environment=prod job_id=17b5fbb0 source=Hadoop status=PENDING message="MapReduce job 'job_name_16' submitted to YARN queue."
2025-06-12T13:54:14.201+05:30 host=ip-117-16-244-242 service=job_executor app=data_pipeline environment=prod job_id=745853e4 source=Hadoop status=FAILED error_type="org.apache.hadoop.fs.FileNotFoundException" message="Job 'job_name_136' failed: org.apache.hadoop.fs.FileNotFoundException"
org.apache.hadoop.fs.FileNotFoundException: File /user/hadoop/input.txt does not exist
	at org.apache.hadoop.fs.LocalFileSystem.open(LocalFileSystem.java:123)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:67)
	... 8 more
Caused by: org.apache.hadoop.fs.FileNotFoundException (for JobID: 745853e4)
2025-06-12T13:54:16.201+05:30 host=ip-107-127-140-239 service=job_executor app=data_pipeline environment=prod job_id=b53ac53c source=Spark status=FAILED error_type="ValueError" message="Job 'job_name_156' failed: ValueError: invalid literal"
ValueError: invalid literal for int() with base 10: 'abc'
	at pandas_read.py:127
	at data_processing.py:42
	... 3 more
Caused by: ValueError (for JobID: b53ac53c)
2025-06-12T13:54:18.201+05:30 host=ip-152-218-214-84 service=job_executor app=data_pipeline environment=prod job_id=5294fdc1 source=Spark status=FAILED error_type="com.amazonaws.services.glue.GlueException" message="Job 'job_name_180' failed: com.amazonaws.services.glue.GlueException: Entity not found"
com.amazonaws.services.glue.GlueException: Entity not found: database 'sales_db'
	at com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:350)
	at com.amazonaws.services.glue.AWSGlueClient.getTable(AWSGlueClient.java:115)
	... 4 more
Caused by: com.amazonaws.services.glue.GlueException (for JobID: 5294fdc1)
2025-06-12T13:54:18.201+05:30 host=ip-42-158-34-193 service=job_executor app=data_pipeline environment=prod job_id=4921ebfa source=Spark status=PENDING message="Spark job 'job_name_193' submitted to cluster, waiting for resources."
2025-06-12T13:54:19.201+05:30 host=ip-43-66-111-136 service=job_executor app=data_pipeline environment=prod job_id=e2d9cb71 source=Airflow status=PENDING message="DAG 'dag_pipeline_19' task 'generate_report' is pending execution."
2025-06-12T13:54:19.201+05:30 host=ip-179-110-115-29 service=job_executor app=data_pipeline environment=prod job_id=1ce2908d source=Spark status=RUNNING message="Spark job 'job_name_266' started. Processing partition 76 of 195."
2025-06-12T13:54:20.201+05:30 host=ip-41-165-156-13 service=job_executor app=data_pipeline environment=prod job_id=bd281c73 source=Glue status=SUCCEEDED message="Glue ETL job 'job_name_209' completed successfully. Output written to S3."
2025-06-12T13:54:20.201+05:30 host=ip-48-84-47-159 service=job_executor app=data_pipeline environment=prod job_id=1ce2908d source=Spark status=FAILED error_type="ValueError" message="Job 'job_name_266' failed: ValueError: invalid literal"
ValueError: invalid literal for int() with base 10: 'abc'
	at pandas_read.py:127
	at data_processing.py:42
	... 3 more
Caused by: ValueError (for JobID: 1ce2908d)
2025-06-12T13:54:23.201+05:30 host=ip-185-48-230-210 service=job_executor app=data_pipeline environment=prod job_id=2fc9c3b2 source=Airflow status=RUNNING message="DAG 'dag_pipeline_41' task 'extract_data' is now running."
2025-06-12T13:54:25.201+05:30 host=ip-90-10-184-75 service=job_executor app=data_pipeline environment=prod job_id=011aa9bc source=Airflow status=FAILED error_type="ValueError" message="Job 'dag_pipeline_27' failed: ValueError: invalid literal"
ValueError: invalid literal for int() with base 10: 'abc'
	at pandas_read.py:127
	at data_processing.py:42
	... 3 more
Caused by: ValueError (for JobID: 011aa9bc)
2025-06-12T13:54:26.201+05:30 host=ip-165-254-128-249 service=job_executor app=data_pipeline environment=prod job_id=77f4e545 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_274' completed successfully. Output written to HDFS."
2025-06-12T13:54:27.201+05:30 host=ip-58-223-144-235 service=job_executor app=data_pipeline environment=prod job_id=e836688a source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_1' task 'load_to_dw' completed successfully."
2025-06-12T13:54:27.201+05:30 host=ip-90-21-103-181 service=job_executor app=data_pipeline environment=prod job_id=5b2dd944 source=Glue status=RUNNING message="Glue ETL job 'job_name_130' started execution."
2025-06-12T13:54:28.201+05:30 host=ip-80-24-221-90 service=job_executor app=data_pipeline environment=prod job_id=2fc9c3b2 source=Airflow status=FAILED error_type="org.apache.hadoop.fs.FileNotFoundException" message="Job 'dag_pipeline_41' failed: org.apache.hadoop.fs.FileNotFoundException"
org.apache.hadoop.fs.FileNotFoundException: File /user/hadoop/input.txt does not exist
	at org.apache.hadoop.fs.LocalFileSystem.open(LocalFileSystem.java:123)
	at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:67)
	... 8 more
Caused by: org.apache.hadoop.fs.FileNotFoundException (for JobID: 2fc9c3b2)
2025-06-12T13:54:29.201+05:30 host=ip-54-67-54-39 service=job_executor app=data_pipeline environment=prod job_id=104c2a01 source=Hadoop status=PENDING message="MapReduce job 'job_name_55' submitted to YARN queue."
2025-06-12T13:54:30.201+05:30 host=ip-18-49-87-102 service=job_executor app=data_pipeline environment=prod job_id=eb030f69 source=Airflow status=FAILED error_type="ValueError" message="Job 'dag_pipeline_44' failed: ValueError: invalid literal"
ValueError: invalid literal for int() with base 10: 'abc'
	at pandas_read.py:127
	at data_processing.py:42
	... 3 more
Caused by: ValueError (for JobID: eb030f69)
2025-06-12T13:54:31.201+05:30 host=ip-104-21-15-156 service=job_executor app=data_pipeline environment=prod job_id=60b7c8f5 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_48' completed successfully. Output written to HDFS."
2025-06-12T13:54:31.201+05:30 host=ip-70-186-176-58 service=job_executor app=data_pipeline environment=prod job_id=5294fdc1 source=Spark status=PENDING message="Spark job 'job_name_180' submitted to cluster, waiting for resources."
2025-06-12T13:54:32.201+05:30 host=ip-50-19-142-104 service=job_executor app=data_pipeline environment=prod job_id=086f7756 source=Airflow status=RUNNING message="DAG 'dag_pipeline_25' task 'load_to_dw' is now running."
2025-06-12T13:54:33.201+05:30 host=ip-187-108-255-131 service=job_executor app=data_pipeline environment=prod job_id=fb23f407 source=Hadoop status=SUCCEEDED message="MapReduce job 'job_name_17' completed successfully. Output written to HDFS."
2025-06-12T13:54:34.201+05:30 host=ip-169-123-16-109 service=job_executor app=data_pipeline environment=prod job_id=51f3010a source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_28' task 'transform_data' completed successfully."
2025-06-12T13:54:34.201+05:30 host=ip-55-233-57-189 service=job_executor app=data_pipeline environment=prod job_id=073c57ab source=Spark status=SUCCEEDED message="Spark job 'job_name_244' completed successfully. Results saved."
2025-06-12T13:54:35.201+05:30 host=ip-30-122-223-156 service=job_executor app=data_pipeline environment=prod job_id=43efa9ce source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_18' task 'extract_data' completed successfully."
2025-06-12T13:54:35.201+05:30 host=ip-34-204-190-112 service=job_executor app=data_pipeline environment=prod job_id=3af8ec8c source=Glue status=FAILED error_type="org.apache.kafka.common.errors.TimeoutException" message="Job 'job_name_112' failed: org.apache.kafka.common.errors.TimeoutException"
org.apache.kafka.common.errors.TimeoutException: Topic partition assignment timeout after 60000 ms
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:350)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1245)
	... 6 more
Caused by: org.apache.kafka.common.errors.TimeoutException (for JobID: 3af8ec8c)
2025-06-12T13:54:36.201+05:30 host=ip-150-213-240-255 service=job_executor app=data_pipeline environment=prod job_id=4921ebfa source=Spark status=SUCCEEDED message="Spark job 'job_name_193' completed successfully. Results saved."
2025-06-12T13:54:38.201+05:30 host=ip-197-121-103-148 service=job_executor app=data_pipeline environment=prod job_id=684dda76 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_26' task 'extract_data' completed successfully."
2025-06-12T13:54:38.201+05:30 host=ip-78-156-211-182 service=job_executor app=data_pipeline environment=prod job_id=00ce23f6 source=Airflow status=SUCCEEDED message="DAG 'dag_pipeline_35' task 'load_to_dw' completed successfully."
